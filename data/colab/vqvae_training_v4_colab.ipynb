{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE v4 Training - Shape Preservation Focus (Google Colab)\n",
    "\n",
    "## Philosophy: SHAPE FIRST, DETAILS SECOND\n",
    "\n",
    "The key insight from Phase 2.5 validation: **buildings were disappearing** in reconstructions.\n",
    "The model was predicting AIR where there should be structure blocks.\n",
    "\n",
    "This version addresses that with:\n",
    "1. **Asymmetric Cross-Entropy**: structure->air errors penalized 10x more than air->structure\n",
    "2. **False Air Penalty**: explicit loss for predicting air where structure exists\n",
    "3. **Volume Preservation**: penalize if reconstruction has fewer blocks than original\n",
    "4. **Structure Recall**: new key metric tracking shape preservation\n",
    "\n",
    "## Key Improvements over v3\n",
    "\n",
    "| Change | v3 | v4 |\n",
    "|--------|-----|-----|\n",
    "| Latent grid | 4x4x4 (512:1) | **8x8x8 (64:1)** |\n",
    "| Embeddings | Frozen | **Trainable** |\n",
    "| Loss | CE only | **CE + embedding similarity + shape preservation** |\n",
    "| Metric | Exact-match | **Structure Recall (shape preservation)** |\n",
    "| Key focus | Accuracy | **Don't erase buildings!** |\n",
    "\n",
    "## New Metrics\n",
    "\n",
    "- **Structure Recall**: Of blocks that SHOULD be structure, how many are NOT erased? (Target: >90%)\n",
    "- **False Air Rate**: What % of structure was wrongly predicted as air? (Target: <10%)\n",
    "- **Volume Ratio**: predicted_volume / original_volume (Target: ~1.0)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Upload the following to your Google Drive under `minecraft_ai/`:\n",
    "- `splits/train/` - Training H5 files\n",
    "- `splits/val/` - Validation H5 files  \n",
    "- `tok2block.json` - Vocabulary file\n",
    "- `block_embeddings_v3.npy` - V3 compositional embeddings\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **Epochs**: 15 total (5 warmup + 10 full)\n",
    "- **Structure weight**: 50x (increased from 10x)\n",
    "- **False air weight**: 5x\n",
    "- **Structure->air weight**: 10x (asymmetric CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 0: Mount Google Drive & Prevent Idle Disconnect\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify mount\n",
    "import os\n",
    "print(\"Drive mounted. Checking for minecraft_ai folder...\")\n",
    "drive_path = \"/content/drive/MyDrive/minecraft_ai\"\n",
    "if os.path.exists(drive_path):\n",
    "    print(f\"Found: {drive_path}\")\n",
    "    print(f\"Contents: {os.listdir(drive_path)}\")\n",
    "else:\n",
    "    print(f\"WARNING: {drive_path} not found!\")\n",
    "    print(\"Please create this folder and upload your data.\")\n",
    "\n",
    "# --- Prevent Colab from disconnecting due to idle timeout ---\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "keep_alive_js = Javascript('''\n",
    "function KeepAlive() {\n",
    "    console.log(\"Keep-alive ping at \" + new Date().toLocaleTimeString());\n",
    "    var buttons = document.querySelectorAll(\"colab-connect-button, colab-toolbar-button#connect\");\n",
    "    buttons.forEach(function(btn) {\n",
    "        if (btn) btn.click();\n",
    "    });\n",
    "}\n",
    "setInterval(KeepAlive, 60000);\n",
    "console.log(\"Keep-alive script activated - will ping every 60 seconds\");\n",
    "''')\n",
    "\n",
    "display(keep_alive_js)\n",
    "print(\"\\nKeep-alive script activated to prevent idle disconnect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from collections import Counter\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Configuration\n",
    "# ============================================================\n",
    "\n",
    "# === Google Drive Paths ===\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/minecraft_ai\"\n",
    "\n",
    "DATA_DIR = f\"{DRIVE_BASE}/splits/train\"\n",
    "VAL_DIR = f\"{DRIVE_BASE}/splits/val\"\n",
    "VOCAB_PATH = f\"{DRIVE_BASE}/tok2block.json\"\n",
    "V3_EMBEDDINGS_PATH = f\"{DRIVE_BASE}/block_embeddings_v3.npy\"\n",
    "\n",
    "# Output to Drive so results persist after runtime ends\n",
    "OUTPUT_DIR = f\"{DRIVE_BASE}/output/vqvae_v4\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === V4 Model Architecture ===\n",
    "HIDDEN_DIMS = [96, 192]  # 2 stages for 32->8 (not 3 stages for 32->4)\n",
    "LATENT_DIM = 256\n",
    "NUM_CODEBOOK_ENTRIES = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# === VQ-VAE Settings ===\n",
    "COMMITMENT_COST = 0.5\n",
    "EMA_DECAY = 0.99\n",
    "STRUCTURE_WEIGHT = 50.0  # INCREASED from 10.0 for shape preservation\n",
    "\n",
    "# === V4 Specific Settings ===\n",
    "EMBEDDING_LOSS_ALPHA = 0.5  # Weight for embedding similarity loss\n",
    "STABILITY_WEIGHT = 0.01    # Embedding stability regularization\n",
    "DIVERSITY_WEIGHT = 0.001   # Embedding diversity regularization\n",
    "\n",
    "# === SHAPE PRESERVATION SETTINGS (NEW) ===\n",
    "# These prevent the \"buildings disappearing\" problem\n",
    "FALSE_AIR_WEIGHT = 5.0     # Heavily penalize predicting air where structure exists\n",
    "VOLUME_WEIGHT = 2.0        # Penalize losing structure volume\n",
    "STRUCTURE_TO_AIR_WEIGHT = 10.0  # Asymmetric CE: structure->air errors 10x worse\n",
    "USE_SHAPE_LOSS = True      # Enable shape preservation loss\n",
    "USE_ASYMMETRIC_LOSS = True # Enable asymmetric cross-entropy\n",
    "\n",
    "# === Training ===\n",
    "TOTAL_EPOCHS = 15\n",
    "WARMUP_EPOCHS = 5  # Freeze embeddings for first N epochs\n",
    "BATCH_SIZE = 4     # Reduced due to larger latent grid\n",
    "BASE_LR = 3e-4\n",
    "EMBEDDING_LR_SCALE = 0.1  # Embeddings train 10x slower\n",
    "USE_AMP = True\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"VQ-VAE v4 Configuration (Colab):\")\n",
    "print(f\"  Drive base: {DRIVE_BASE}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Latent grid: 8x8x8 (64:1 compression)\")\n",
    "print(f\"  Hidden dims: {HIDDEN_DIMS}\")\n",
    "print(f\"  Epochs: {TOTAL_EPOCHS} ({WARMUP_EPOCHS} warmup + {TOTAL_EPOCHS - WARMUP_EPOCHS} full)\")\n",
    "print(f\"  Embedding loss alpha: {EMBEDDING_LOSS_ALPHA}\")\n",
    "print(f\"  Structure weight: {STRUCTURE_WEIGHT}x\")\n",
    "print(f\"\\nShape Preservation (NEW):\")\n",
    "print(f\"  False air weight: {FALSE_AIR_WEIGHT}\")\n",
    "print(f\"  Volume weight: {VOLUME_WEIGHT}\")\n",
    "print(f\"  Structure->air weight: {STRUCTURE_TO_AIR_WEIGHT}\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"\\nPath verification:\")\n",
    "for path, name in [(DATA_DIR, \"train\"), (VAL_DIR, \"val\"), (VOCAB_PATH, \"vocab\"), \n",
    "                   (V3_EMBEDDINGS_PATH, \"V3 emb\")]:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  {name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load Vocabulary and Embeddings\n",
    "# ============================================================\n",
    "\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "block2tok = {v: k for k, v in tok2block.items()}\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Find air tokens\n",
    "AIR_TOKENS = set()\n",
    "for tok, block in tok2block.items():\n",
    "    if 'air' in block.lower() and 'stair' not in block.lower():\n",
    "        AIR_TOKENS.add(tok)\n",
    "        print(f\"  Air token: {tok} = {block}\")\n",
    "\n",
    "AIR_TOKENS_LIST = sorted(AIR_TOKENS)\n",
    "AIR_TOKENS_TENSOR = torch.tensor(AIR_TOKENS_LIST, dtype=torch.long)\n",
    "\n",
    "# Load V3 embeddings (compositional)\n",
    "v3_embeddings = np.load(V3_EMBEDDINGS_PATH).astype(np.float32)\n",
    "EMBEDDING_DIM = v3_embeddings.shape[1]\n",
    "print(f\"V3 embeddings: {v3_embeddings.shape} (dim={EMBEDDING_DIM})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Dataset\n",
    "# ============================================================\n",
    "\n",
    "class VQVAEDataset(Dataset):\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        if not self.h5_files:\n",
    "            raise ValueError(f\"No H5 files in {data_dir}\")\n",
    "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.h5_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_files[idx], 'r') as f:\n",
    "            key = list(f.keys())[0]\n",
    "            structure = f[key][:].astype(np.int64)\n",
    "        return torch.from_numpy(structure).long()\n",
    "\n",
    "train_dataset = VQVAEDataset(DATA_DIR)\n",
    "val_dataset = VQVAEDataset(VAL_DIR)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: VQ-VAE v4 Architecture with Shape Preservation\n",
    "# ============================================================\n",
    "\n",
    "class ResidualBlock3D(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(channels)\n",
    "        self.bn2 = nn.BatchNorm3d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    \"\"\"EMA-based vector quantizer with dead code reset.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes, latent_dim, commitment_cost=0.5, ema_decay=0.99):\n",
    "        super().__init__()\n",
    "        self.num_codes = num_codes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.ema_decay = ema_decay\n",
    "        \n",
    "        self.register_buffer('codebook', torch.randn(num_codes, latent_dim))\n",
    "        self.codebook.data.uniform_(-1/num_codes, 1/num_codes)\n",
    "        self.register_buffer('ema_cluster_size', torch.zeros(num_codes))\n",
    "        self.register_buffer('ema_embed_sum', torch.zeros(num_codes, latent_dim))\n",
    "        self.register_buffer('code_usage', torch.zeros(num_codes))\n",
    "    \n",
    "    def reset_epoch_stats(self):\n",
    "        self.code_usage.zero_()\n",
    "    \n",
    "    def get_usage_fraction(self):\n",
    "        return (self.code_usage > 0).float().mean().item()\n",
    "    \n",
    "    def get_perplexity(self):\n",
    "        if self.code_usage.sum() == 0:\n",
    "            return 0.0\n",
    "        probs = self.code_usage / self.code_usage.sum()\n",
    "        probs = probs[probs > 0]\n",
    "        entropy = -(probs * probs.log()).sum()\n",
    "        return entropy.exp().item()\n",
    "    \n",
    "    def forward(self, z_e):\n",
    "        z_e_perm = z_e.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        flat = z_e_perm.view(-1, self.latent_dim)\n",
    "        flat_f32 = flat.float()\n",
    "        codebook_f32 = self.codebook.float()\n",
    "        \n",
    "        d = (flat_f32.pow(2).sum(1, keepdim=True) \n",
    "             + codebook_f32.pow(2).sum(1) \n",
    "             - 2 * flat_f32 @ codebook_f32.t())\n",
    "        \n",
    "        indices = d.argmin(dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in indices.unique():\n",
    "                self.code_usage[idx] += (indices == idx).sum()\n",
    "        \n",
    "        z_q_flat = self.codebook[indices]\n",
    "        z_q_perm = z_q_flat.view(z_e_perm.shape)\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                encodings = F.one_hot(indices, self.num_codes).float()\n",
    "                batch_size = encodings.sum(0)\n",
    "                \n",
    "                self.ema_cluster_size = self.ema_decay * self.ema_cluster_size + (1 - self.ema_decay) * batch_size\n",
    "                batch_sum = encodings.t() @ flat_f32\n",
    "                self.ema_embed_sum = self.ema_decay * self.ema_embed_sum + (1 - self.ema_decay) * batch_sum\n",
    "                \n",
    "                n = self.ema_cluster_size.sum()\n",
    "                smoothed = (self.ema_cluster_size + 1e-5) / (n + self.num_codes * 1e-5) * n\n",
    "                self.codebook.data = self.ema_embed_sum / smoothed.unsqueeze(1)\n",
    "                \n",
    "                # Dead code reset\n",
    "                dead = batch_size < 2\n",
    "                if dead.any() and flat_f32.size(0) > 0:\n",
    "                    n_dead = dead.sum().item()\n",
    "                    rand_idx = torch.randint(0, flat_f32.size(0), (n_dead,), device=flat_f32.device)\n",
    "                    self.codebook.data[dead] = flat_f32[rand_idx]\n",
    "                    self.ema_cluster_size[dead] = 1\n",
    "                    self.ema_embed_sum[dead] = flat_f32[rand_idx]\n",
    "        \n",
    "        commitment_loss = F.mse_loss(z_e_perm, z_q_perm.detach())\n",
    "        vq_loss = self.commitment_cost * commitment_loss\n",
    "        \n",
    "        z_q_st = z_e_perm + (z_q_perm - z_e_perm).detach()\n",
    "        z_q = z_q_st.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        \n",
    "        return z_q, vq_loss, indices.view(z_e_perm.shape[:-1])\n",
    "\n",
    "\n",
    "class EncoderV4(nn.Module):\n",
    "    \"\"\"32x32x32 -> 8x8x8 encoder (2 stages instead of 3).\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, hidden_dims, latent_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current = in_channels\n",
    "        \n",
    "        for h in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Conv3d(current, h, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout3d(dropout),\n",
    "                ResidualBlock3D(h),\n",
    "            ])\n",
    "            current = h\n",
    "        \n",
    "        # Extra capacity at 8x8x8\n",
    "        layers.extend([\n",
    "            ResidualBlock3D(current),\n",
    "            ResidualBlock3D(current),\n",
    "            nn.Conv3d(current, latent_dim, 3, padding=1),\n",
    "        ])\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class DecoderV4(nn.Module):\n",
    "    \"\"\"8x8x8 -> 32x32x32 decoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims, num_blocks, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            ResidualBlock3D(latent_dim),\n",
    "            ResidualBlock3D(latent_dim),\n",
    "        ]\n",
    "        \n",
    "        current = latent_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([\n",
    "                ResidualBlock3D(current),\n",
    "                nn.ConvTranspose3d(current, h, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout3d(dropout),\n",
    "            ])\n",
    "            current = h\n",
    "        \n",
    "        layers.append(nn.Conv3d(current, num_blocks, 3, padding=1))\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z_q):\n",
    "        return self.decoder(z_q)\n",
    "\n",
    "\n",
    "def compute_similarity_matrix(embeddings):\n",
    "    \"\"\"Compute cosine similarity matrix scaled to [0,1].\"\"\"\n",
    "    with torch.no_grad():\n",
    "        normed = F.normalize(embeddings, dim=1)\n",
    "        sim = normed @ normed.t()\n",
    "        return (sim + 1) / 2\n",
    "\n",
    "\n",
    "class ShapePreservationLoss(nn.Module):\n",
    "    \"\"\"Loss functions to prevent buildings from disappearing.\n",
    "    \n",
    "    Philosophy: SHAPE FIRST, DETAILS SECOND.\n",
    "    It's better to predict the wrong block type than to predict air.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, false_air_weight=5.0, volume_weight=2.0):\n",
    "        super().__init__()\n",
    "        self.false_air_weight = false_air_weight\n",
    "        self.volume_weight = volume_weight\n",
    "    \n",
    "    def forward(self, logits, targets, air_tokens):\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        \n",
    "        is_struct_orig = ~torch.isin(targets, air_tokens)\n",
    "        is_air_pred = torch.isin(predictions, air_tokens)\n",
    "        is_struct_pred = ~is_air_pred\n",
    "        \n",
    "        # False air: predicted air where structure existed\n",
    "        false_air_mask = is_struct_orig & is_air_pred\n",
    "        if is_struct_orig.sum() > 0:\n",
    "            false_air_rate = false_air_mask.float().sum() / is_struct_orig.float().sum()\n",
    "        else:\n",
    "            false_air_rate = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # Volume preservation: penalize losing structure volume\n",
    "        orig_vol = is_struct_orig.float().sum()\n",
    "        pred_vol = is_struct_pred.float().sum()\n",
    "        if orig_vol > 0:\n",
    "            volume_loss = F.relu(orig_vol - pred_vol) / orig_vol\n",
    "        else:\n",
    "            volume_loss = torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        # Structure recall: fraction of original structure preserved\n",
    "        true_struct = is_struct_orig & is_struct_pred\n",
    "        if is_struct_orig.sum() > 0:\n",
    "            struct_recall = true_struct.float().sum() / is_struct_orig.float().sum()\n",
    "        else:\n",
    "            struct_recall = torch.tensor(1.0, device=predictions.device)\n",
    "        \n",
    "        total = self.false_air_weight * false_air_rate + self.volume_weight * volume_loss\n",
    "        \n",
    "        return {\n",
    "            'false_air_rate': false_air_rate,\n",
    "            'volume_loss': volume_loss,\n",
    "            'structure_recall': struct_recall,\n",
    "            'total': total,\n",
    "        }\n",
    "\n",
    "\n",
    "class AsymmetricStructureLoss(nn.Module):\n",
    "    \"\"\"Asymmetric CE that penalizes structure->air more than air->structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, structure_to_air_weight=10.0, air_to_structure_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.structure_to_air_weight = structure_to_air_weight\n",
    "        self.air_to_structure_weight = air_to_structure_weight\n",
    "    \n",
    "    def forward(self, logits, targets, air_tokens):\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        \n",
    "        is_air_tgt = torch.isin(targets, air_tokens)\n",
    "        is_air_pred = torch.isin(predictions, air_tokens)\n",
    "        is_struct_tgt = ~is_air_tgt\n",
    "        is_struct_pred = ~is_air_pred\n",
    "        \n",
    "        weights = torch.ones_like(targets, dtype=torch.float)\n",
    "        \n",
    "        # Structure->air: HEAVY penalty (erases building)\n",
    "        struct_to_air = is_struct_tgt & is_air_pred\n",
    "        weights[struct_to_air] = self.structure_to_air_weight\n",
    "        \n",
    "        # Air->structure: light penalty (adds extra blocks)\n",
    "        air_to_struct = is_air_tgt & is_struct_pred\n",
    "        weights[air_to_struct] = self.air_to_structure_weight\n",
    "        \n",
    "        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        return (ce_loss * weights).sum() / weights.sum()\n",
    "\n",
    "\n",
    "class VQVAEv4(nn.Module):\n",
    "    \"\"\"VQ-VAE v4 with trainable embeddings, embedding-aware loss, and SHAPE PRESERVATION.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dims, latent_dim, num_codes,\n",
    "                 pretrained_emb, embedding_loss_alpha=0.5, stability_weight=0.01,\n",
    "                 diversity_weight=0.001, false_air_weight=5.0, volume_weight=2.0,\n",
    "                 structure_to_air_weight=10.0, dropout=0.1, commitment_cost=0.5, \n",
    "                 ema_decay=0.99):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedding_loss_alpha = embedding_loss_alpha\n",
    "        self.stability_weight = stability_weight\n",
    "        self.diversity_weight = diversity_weight\n",
    "        self.false_air_weight = false_air_weight\n",
    "        self.volume_weight = volume_weight\n",
    "        self.train_embeddings = False  # Start frozen\n",
    "        \n",
    "        # Embeddings\n",
    "        self.block_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.block_emb.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
    "        self.block_emb.weight.requires_grad = False  # Start frozen\n",
    "        self.register_buffer('original_embeddings', torch.from_numpy(pretrained_emb.copy()))\n",
    "        \n",
    "        # Encoder 32->8\n",
    "        self.encoder = EncoderV4(emb_dim, hidden_dims, latent_dim, dropout)\n",
    "        \n",
    "        # Quantizer\n",
    "        self.quantizer = VectorQuantizerEMA(num_codes, latent_dim, commitment_cost, ema_decay)\n",
    "        \n",
    "        # Decoder 8->32\n",
    "        self.decoder = DecoderV4(latent_dim, list(reversed(hidden_dims)), vocab_size, dropout)\n",
    "        \n",
    "        # Shape preservation loss (NEW)\n",
    "        self.shape_loss = ShapePreservationLoss(false_air_weight, volume_weight)\n",
    "        \n",
    "        # Asymmetric loss (NEW)\n",
    "        self.asymmetric_loss = AsymmetricStructureLoss(structure_to_air_weight)\n",
    "        \n",
    "        # Similarity cache\n",
    "        self._sim_matrix = None\n",
    "        self._sim_valid = False\n",
    "    \n",
    "    def set_train_embeddings(self, train: bool):\n",
    "        \"\"\"Enable/disable embedding training for phased training.\"\"\"\n",
    "        self.train_embeddings = train\n",
    "        self.block_emb.weight.requires_grad = train\n",
    "        if train:\n",
    "            self._sim_valid = False\n",
    "    \n",
    "    def get_similarity_matrix(self):\n",
    "        if not self._sim_valid or self._sim_matrix is None:\n",
    "            self._sim_matrix = compute_similarity_matrix(self.block_emb.weight.detach())\n",
    "            self._sim_valid = True\n",
    "        return self._sim_matrix\n",
    "    \n",
    "    def forward(self, block_ids):\n",
    "        x = self.block_emb(block_ids).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, vq_loss, indices = self.quantizer(z_e)\n",
    "        logits = self.decoder(z_q)\n",
    "        return {'logits': logits, 'vq_loss': vq_loss, 'indices': indices}\n",
    "    \n",
    "    def compute_embedding_regularization(self):\n",
    "        current = self.block_emb.weight\n",
    "        original = self.original_embeddings\n",
    "        \n",
    "        stability = F.mse_loss(current, original)\n",
    "        \n",
    "        normed = F.normalize(current, dim=1)\n",
    "        sim = normed @ normed.t()\n",
    "        off_diag = 1 - torch.eye(current.size(0), device=current.device)\n",
    "        avg_sim = (sim * off_diag).sum() / off_diag.sum()\n",
    "        diversity = F.relu(avg_sim - 0.3)\n",
    "        \n",
    "        return {'stability': stability, 'diversity': diversity}\n",
    "    \n",
    "    def compute_loss(self, block_ids, air_tokens, structure_weight, \n",
    "                     use_emb_loss=True, use_shape_loss=True, use_asymmetric_loss=True):\n",
    "        out = self(block_ids)\n",
    "        logits = out['logits']\n",
    "        \n",
    "        logits_flat = logits.permute(0, 2, 3, 4, 1).reshape(-1, self.vocab_size)\n",
    "        targets_flat = block_ids.view(-1)\n",
    "        \n",
    "        # Air tokens for structure masking\n",
    "        air_dev = air_tokens.to(targets_flat.device)\n",
    "        is_air = torch.isin(targets_flat, air_dev)\n",
    "        is_struct = ~is_air\n",
    "        \n",
    "        # === PRIMARY LOSS: Asymmetric Cross-Entropy ===\n",
    "        if use_asymmetric_loss:\n",
    "            ce_loss = self.asymmetric_loss(logits_flat, targets_flat, air_dev)\n",
    "        else:\n",
    "            weights = torch.ones_like(targets_flat, dtype=torch.float)\n",
    "            weights[is_struct] = structure_weight\n",
    "            ce = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
    "            ce_loss = (weights * ce).sum() / weights.sum()\n",
    "        \n",
    "        # === SHAPE PRESERVATION LOSS ===\n",
    "        if use_shape_loss:\n",
    "            shape = self.shape_loss(logits_flat, targets_flat, air_dev)\n",
    "            shape_loss = shape['total']\n",
    "            false_air_rate = shape['false_air_rate']\n",
    "            struct_recall = shape['structure_recall']\n",
    "        else:\n",
    "            shape_loss = torch.tensor(0.0, device=block_ids.device)\n",
    "            false_air_rate = torch.tensor(0.0, device=block_ids.device)\n",
    "            struct_recall = torch.tensor(0.0, device=block_ids.device)\n",
    "        \n",
    "        # === EMBEDDING SIMILARITY LOSS ===\n",
    "        if use_emb_loss and self.embedding_loss_alpha > 0:\n",
    "            probs = F.softmax(logits_flat / 0.1, dim=1)\n",
    "            emb_normed = F.normalize(self.block_emb.weight, dim=1)\n",
    "            pred_emb = probs @ emb_normed\n",
    "            target_emb = emb_normed[targets_flat]\n",
    "            similarity = (pred_emb * target_emb).sum(dim=1)\n",
    "            emb_loss_raw = 1 - similarity\n",
    "            \n",
    "            weights = torch.ones_like(targets_flat, dtype=torch.float)\n",
    "            weights[is_struct] = structure_weight\n",
    "            emb_loss = (weights * emb_loss_raw).sum() / weights.sum()\n",
    "        else:\n",
    "            emb_loss = torch.tensor(0.0, device=block_ids.device)\n",
    "        \n",
    "        # === EMBEDDING REGULARIZATION ===\n",
    "        if self.train_embeddings:\n",
    "            reg = self.compute_embedding_regularization()\n",
    "            stab_loss = self.stability_weight * reg['stability']\n",
    "            div_loss = self.diversity_weight * reg['diversity']\n",
    "        else:\n",
    "            stab_loss = torch.tensor(0.0, device=block_ids.device)\n",
    "            div_loss = torch.tensor(0.0, device=block_ids.device)\n",
    "        \n",
    "        # === TOTAL LOSS ===\n",
    "        total = (ce_loss + shape_loss + \n",
    "                 self.embedding_loss_alpha * emb_loss + \n",
    "                 out['vq_loss'] + stab_loss + div_loss)\n",
    "        \n",
    "        # === COMPUTE METRICS ===\n",
    "        with torch.no_grad():\n",
    "            preds = logits_flat.argmax(dim=1)\n",
    "            correct = (preds == targets_flat).float()\n",
    "            \n",
    "            exact_acc = correct.mean()\n",
    "            struct_exact = correct[is_struct].mean() if is_struct.any() else torch.tensor(0.0)\n",
    "            air_exact = correct[is_air].mean() if is_air.any() else torch.tensor(0.0)\n",
    "            \n",
    "            # Similarity-weighted accuracy\n",
    "            sim_matrix = self.get_similarity_matrix().to(preds.device)\n",
    "            sim_scores = sim_matrix[preds, targets_flat]\n",
    "            sim_acc = sim_scores.mean()\n",
    "            struct_sim = sim_scores[is_struct].mean() if is_struct.any() else torch.tensor(0.0)\n",
    "            \n",
    "            # Volume ratio\n",
    "            is_air_pred = torch.isin(preds, air_dev)\n",
    "            orig_vol = is_struct.float().sum()\n",
    "            pred_vol = (~is_air_pred).float().sum()\n",
    "            vol_ratio = pred_vol / orig_vol if orig_vol > 0 else torch.tensor(1.0)\n",
    "        \n",
    "        return {\n",
    "            'loss': total,\n",
    "            'ce_loss': ce_loss,\n",
    "            'shape_loss': shape_loss,\n",
    "            'emb_loss': emb_loss,\n",
    "            'vq_loss': out['vq_loss'],\n",
    "            'stab_loss': stab_loss,\n",
    "            'div_loss': div_loss,\n",
    "            'exact_acc': exact_acc,\n",
    "            'struct_exact': struct_exact,\n",
    "            'air_exact': air_exact,\n",
    "            'sim_acc': sim_acc,\n",
    "            'struct_sim': struct_sim,\n",
    "            # Shape preservation metrics (NEW - KEY METRICS)\n",
    "            'false_air_rate': false_air_rate,  # Want LOW (<10%)\n",
    "            'struct_recall': struct_recall,     # Want HIGH (>90%)\n",
    "            'vol_ratio': vol_ratio,             # Want CLOSE TO 1.0\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"VQ-VAE v4 architecture with shape preservation defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Training Functions with Shape Preservation Metrics\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device, air_tokens, structure_weight,\n",
    "                use_emb_loss, use_shape_loss, use_asymmetric_loss):\n",
    "    model.train()\n",
    "    model.quantizer.reset_epoch_stats()\n",
    "    \n",
    "    metrics = {k: 0.0 for k in ['loss', 'ce', 'shape', 'emb', 'vq', 'stab', 'div',\n",
    "                                 'exact', 'struct_exact', 'air_exact', 'sim', 'struct_sim',\n",
    "                                 'false_air', 'struct_recall', 'vol_ratio']}\n",
    "    n = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss(batch, air_tokens, structure_weight, \n",
    "                                     use_emb_loss, use_shape_loss, use_asymmetric_loss)\n",
    "            loss = out['loss'] / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['ce'] += out['ce_loss'].item()\n",
    "        metrics['shape'] += out['shape_loss'].item()\n",
    "        metrics['emb'] += out['emb_loss'].item()\n",
    "        metrics['vq'] += out['vq_loss'].item()\n",
    "        metrics['stab'] += out['stab_loss'].item()\n",
    "        metrics['div'] += out['div_loss'].item()\n",
    "        metrics['exact'] += out['exact_acc'].item()\n",
    "        metrics['struct_exact'] += out['struct_exact'].item()\n",
    "        metrics['air_exact'] += out['air_exact'].item()\n",
    "        metrics['sim'] += out['sim_acc'].item()\n",
    "        metrics['struct_sim'] += out['struct_sim'].item()\n",
    "        # NEW shape preservation metrics\n",
    "        metrics['false_air'] += out['false_air_rate'].item()\n",
    "        metrics['struct_recall'] += out['struct_recall'].item()\n",
    "        metrics['vol_ratio'] += out['vol_ratio'].item()\n",
    "        n += 1\n",
    "    \n",
    "    metrics['cb_usage'] = model.quantizer.get_usage_fraction()\n",
    "    metrics['perplexity'] = model.quantizer.get_perplexity()\n",
    "    \n",
    "    return {k: v/n if k not in ['cb_usage', 'perplexity'] else v for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device, air_tokens, structure_weight, \n",
    "             use_emb_loss, use_shape_loss, use_asymmetric_loss):\n",
    "    model.eval()\n",
    "    model.quantizer.reset_epoch_stats()\n",
    "    \n",
    "    metrics = {k: 0.0 for k in ['loss', 'ce', 'shape', 'emb', 'vq',\n",
    "                                 'exact', 'struct_exact', 'air_exact', 'sim', 'struct_sim',\n",
    "                                 'false_air', 'struct_recall', 'vol_ratio']}\n",
    "    n = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss(batch, air_tokens, structure_weight,\n",
    "                                     use_emb_loss, use_shape_loss, use_asymmetric_loss)\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['ce'] += out['ce_loss'].item()\n",
    "        metrics['shape'] += out['shape_loss'].item()\n",
    "        metrics['emb'] += out['emb_loss'].item()\n",
    "        metrics['vq'] += out['vq_loss'].item()\n",
    "        metrics['exact'] += out['exact_acc'].item()\n",
    "        metrics['struct_exact'] += out['struct_exact'].item()\n",
    "        metrics['air_exact'] += out['air_exact'].item()\n",
    "        metrics['sim'] += out['sim_acc'].item()\n",
    "        metrics['struct_sim'] += out['struct_sim'].item()\n",
    "        # NEW shape preservation metrics\n",
    "        metrics['false_air'] += out['false_air_rate'].item()\n",
    "        metrics['struct_recall'] += out['struct_recall'].item()\n",
    "        metrics['vol_ratio'] += out['vol_ratio'].item()\n",
    "        n += 1\n",
    "    \n",
    "    metrics['cb_usage'] = model.quantizer.get_usage_fraction()\n",
    "    metrics['perplexity'] = model.quantizer.get_perplexity()\n",
    "    \n",
    "    return {k: v/n if k not in ['cb_usage', 'perplexity'] else v for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "print(\"Training functions with shape preservation metrics defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Create Model and Optimizer\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Data loaders\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Create model with shape preservation\n",
    "model = VQVAEv4(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_dim=EMBEDDING_DIM,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    num_codes=NUM_CODEBOOK_ENTRIES,\n",
    "    pretrained_emb=v3_embeddings,\n",
    "    embedding_loss_alpha=EMBEDDING_LOSS_ALPHA,\n",
    "    stability_weight=STABILITY_WEIGHT,\n",
    "    diversity_weight=DIVERSITY_WEIGHT,\n",
    "    false_air_weight=FALSE_AIR_WEIGHT,\n",
    "    volume_weight=VOLUME_WEIGHT,\n",
    "    structure_to_air_weight=STRUCTURE_TO_AIR_WEIGHT,\n",
    "    dropout=DROPOUT,\n",
    "    commitment_cost=COMMITMENT_COST,\n",
    "    ema_decay=EMA_DECAY,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable params: {trainable_params:,}\")\n",
    "print(f\"Embeddings trainable: {model.train_embeddings}\")\n",
    "\n",
    "# Optimizer with separate LR for embeddings\n",
    "emb_params = list(model.block_emb.parameters())\n",
    "other_params = [p for n, p in model.named_parameters() if 'block_emb' not in n and p.requires_grad]\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': other_params, 'lr': BASE_LR},\n",
    "    {'params': emb_params, 'lr': BASE_LR * EMBEDDING_LR_SCALE},\n",
    "], weight_decay=1e-5)\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW\")\n",
    "print(f\"  Base LR: {BASE_LR}\")\n",
    "print(f\"  Embedding LR: {BASE_LR * EMBEDDING_LR_SCALE}\")\n",
    "print(f\"\\nShape Preservation:\")\n",
    "print(f\"  False air weight: {FALSE_AIR_WEIGHT}\")\n",
    "print(f\"  Volume weight: {VOLUME_WEIGHT}\")\n",
    "print(f\"  Structure->air weight: {STRUCTURE_TO_AIR_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Training Loop with Shape Preservation\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VQ-VAE V4 TRAINING - SHAPE FIRST, DETAILS SECOND\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Phase 1: Warmup (epochs 1-{WARMUP_EPOCHS}) - Embeddings FROZEN\")\n",
    "print(f\"Phase 2: Full (epochs {WARMUP_EPOCHS+1}-{TOTAL_EPOCHS}) - Embeddings TRAINABLE\")\n",
    "print(f\"\\nShape Preservation Enabled:\")\n",
    "print(f\"  - Asymmetric CE: structure->air penalized {STRUCTURE_TO_AIR_WEIGHT}x\")\n",
    "print(f\"  - False air penalty: {FALSE_AIR_WEIGHT}x\")\n",
    "print(f\"  - Volume preservation: {VOLUME_WEIGHT}x\")\n",
    "print()\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_struct_exact': [], 'train_struct_sim': [],\n",
    "    'train_cb_usage': [], 'train_perplexity': [],\n",
    "    'train_false_air': [], 'train_struct_recall': [], 'train_vol_ratio': [],\n",
    "    'val_loss': [], 'val_struct_exact': [], 'val_struct_sim': [],\n",
    "    'val_cb_usage': [], 'val_perplexity': [],\n",
    "    'val_false_air': [], 'val_struct_recall': [], 'val_vol_ratio': [],\n",
    "    'phase': [],\n",
    "}\n",
    "\n",
    "best_struct_recall = 0  # NEW: track best shape preservation, not similarity\n",
    "best_epoch = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    # Phased training\n",
    "    if epoch < WARMUP_EPOCHS:\n",
    "        phase = \"warmup\"\n",
    "        model.set_train_embeddings(False)\n",
    "        use_emb_loss = False\n",
    "    else:\n",
    "        phase = \"full\"\n",
    "        model.set_train_embeddings(True)\n",
    "        use_emb_loss = True\n",
    "    \n",
    "    # Train\n",
    "    train_m = train_epoch(model, train_loader, optimizer, scaler, device,\n",
    "                          AIR_TOKENS_TENSOR, STRUCTURE_WEIGHT, use_emb_loss,\n",
    "                          USE_SHAPE_LOSS, USE_ASYMMETRIC_LOSS)\n",
    "    \n",
    "    # Validate\n",
    "    val_m = validate(model, val_loader, device, AIR_TOKENS_TENSOR, STRUCTURE_WEIGHT,\n",
    "                     use_emb_loss, USE_SHAPE_LOSS, USE_ASYMMETRIC_LOSS)\n",
    "    \n",
    "    # Record\n",
    "    history['train_loss'].append(train_m['loss'])\n",
    "    history['train_struct_exact'].append(train_m['struct_exact'])\n",
    "    history['train_struct_sim'].append(train_m['struct_sim'])\n",
    "    history['train_cb_usage'].append(train_m['cb_usage'])\n",
    "    history['train_perplexity'].append(train_m['perplexity'])\n",
    "    history['train_false_air'].append(train_m['false_air'])\n",
    "    history['train_struct_recall'].append(train_m['struct_recall'])\n",
    "    history['train_vol_ratio'].append(train_m['vol_ratio'])\n",
    "    \n",
    "    history['val_loss'].append(val_m['loss'])\n",
    "    history['val_struct_exact'].append(val_m['struct_exact'])\n",
    "    history['val_struct_sim'].append(val_m['struct_sim'])\n",
    "    history['val_cb_usage'].append(val_m['cb_usage'])\n",
    "    history['val_perplexity'].append(val_m['perplexity'])\n",
    "    history['val_false_air'].append(val_m['false_air'])\n",
    "    history['val_struct_recall'].append(val_m['struct_recall'])\n",
    "    history['val_vol_ratio'].append(val_m['vol_ratio'])\n",
    "    history['phase'].append(phase)\n",
    "    \n",
    "    # Best model - now track STRUCTURE RECALL (shape preservation) as the key metric\n",
    "    if val_m['struct_recall'] > best_struct_recall:\n",
    "        best_struct_recall = val_m['struct_recall']\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/vqvae_v4_best.pt\")\n",
    "    \n",
    "    # Log - now with shape preservation metrics\n",
    "    emb_status = \"FROZEN\" if phase == \"warmup\" else \"TRAIN\"\n",
    "    print(f\"Epoch {epoch+1:2d} [{phase:6s}] | \"\n",
    "          f\"Recall: {train_m['struct_recall']:.1%}/{val_m['struct_recall']:.1%} | \"\n",
    "          f\"FalseAir: {train_m['false_air']:.1%}/{val_m['false_air']:.1%} | \"\n",
    "          f\"Vol: {val_m['vol_ratio']:.2f} | \"\n",
    "          f\"Exact: {val_m['struct_exact']:.1%} | \"\n",
    "          f\"CB: {val_m['cb_usage']:.0%}\")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"\\nTraining complete in {train_time/60:.1f} minutes\")\n",
    "print(f\"Best val struct_recall: {best_struct_recall:.1%} at epoch {best_epoch}\")\n",
    "print(f\"\\n*** KEY METRIC: Structure Recall = {best_struct_recall:.1%} ***\")\n",
    "print(\"(This measures: of all blocks that SHOULD be structure, how many were NOT erased)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Plot Training Curves - Shape Preservation Focus\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "\n",
    "epochs = range(1, TOTAL_EPOCHS + 1)\n",
    "\n",
    "# Plot 1: STRUCTURE RECALL (THE KEY METRIC)\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs, history['train_struct_recall'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs, history['val_struct_recall'], 'r--', label='Val', linewidth=2)\n",
    "ax.axvline(x=WARMUP_EPOCHS, color='gray', linestyle=':', label='Warmup end')\n",
    "ax.axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='Target (90%)')\n",
    "ax.set_title('Structure Recall (KEY METRIC)', fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: FALSE AIR RATE (lower is better)\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs, history['train_false_air'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_false_air'], 'r--', label='Val')\n",
    "ax.axvline(x=WARMUP_EPOCHS, color='gray', linestyle=':')\n",
    "ax.axhline(y=0.1, color='green', linestyle='--', alpha=0.5, label='Target (<10%)')\n",
    "ax.set_title('False Air Rate (lower=better)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Rate')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Plot 3: Volume Ratio (want close to 1.0)\n",
    "ax = axes[0, 2]\n",
    "ax.plot(epochs, history['train_vol_ratio'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_vol_ratio'], 'r--', label='Val')\n",
    "ax.axvline(x=WARMUP_EPOCHS, color='gray', linestyle=':')\n",
    "ax.axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Target (1.0)')\n",
    "ax.set_title('Volume Ratio (1.0=perfect)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Ratio')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Structure accuracy (exact vs similarity)\n",
    "ax = axes[0, 3]\n",
    "ax.plot(epochs, history['train_struct_exact'], 'b-', label='Train Exact')\n",
    "ax.plot(epochs, history['val_struct_exact'], 'b--', label='Val Exact')\n",
    "ax.plot(epochs, history['train_struct_sim'], 'g-', label='Train Sim')\n",
    "ax.plot(epochs, history['val_struct_sim'], 'g--', label='Val Sim')\n",
    "ax.axvline(x=WARMUP_EPOCHS, color='gray', linestyle=':')\n",
    "ax.set_title('Structure Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Loss\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs, history['train_loss'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_loss'], 'r--', label='Val')\n",
    "ax.axvline(x=WARMUP_EPOCHS, color='gray', linestyle=':')\n",
    "ax.set_title('Total Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Codebook usage\n",
    "ax = axes[1, 1]\n",
    "ax.plot(epochs, history['train_cb_usage'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_cb_usage'], 'r--', label='Val')\n",
    "ax.axhline(y=0.3, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=WARMUP_EPOCHS, color='gray', linestyle=':')\n",
    "ax.set_title('Codebook Usage')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Perplexity\n",
    "ax = axes[1, 2]\n",
    "ax.plot(epochs, history['train_perplexity'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_perplexity'], 'r--', label='Val')\n",
    "ax.axvline(x=WARMUP_EPOCHS, color='gray', linestyle=':')\n",
    "ax.set_title('Codebook Perplexity')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 8: Final comparison bar - SHAPE PRESERVATION FOCUS\n",
    "ax = axes[1, 3]\n",
    "final_metrics = {\n",
    "    'Struct\\nRecall': history['val_struct_recall'][-1],\n",
    "    '1-False\\nAir': 1 - history['val_false_air'][-1],\n",
    "    'Exact\\nAcc': history['val_struct_exact'][-1],\n",
    "    'Sim\\nAcc': history['val_struct_sim'][-1],\n",
    "}\n",
    "colors = ['green', 'orange', 'blue', 'purple']\n",
    "bars = ax.bar(final_metrics.keys(), final_metrics.values(), color=colors)\n",
    "ax.set_title('Final Metrics (Shape Focus)')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, val in zip(bars, final_metrics.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{val:.1%}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/vqvae_v4_training.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHAPE PRESERVATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Structure Recall: {history['val_struct_recall'][-1]:.1%} (target: >90%)\")\n",
    "print(f\"False Air Rate:   {history['val_false_air'][-1]:.1%} (target: <10%)\")\n",
    "print(f\"Volume Ratio:     {history['val_vol_ratio'][-1]:.2f} (target: ~1.0)\")\n",
    "print(f\"Exact Accuracy:   {history['val_struct_exact'][-1]:.1%}\")\n",
    "print(f\"Sim Accuracy:     {history['val_struct_sim'][-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Save Results and Checkpoint\n",
    "# ============================================================\n",
    "\n",
    "results = {\n",
    "    'config': {\n",
    "        'hidden_dims': HIDDEN_DIMS,\n",
    "        'latent_dim': LATENT_DIM,\n",
    "        'num_codes': NUM_CODEBOOK_ENTRIES,\n",
    "        'total_epochs': TOTAL_EPOCHS,\n",
    "        'warmup_epochs': WARMUP_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'base_lr': BASE_LR,\n",
    "        'embedding_lr_scale': EMBEDDING_LR_SCALE,\n",
    "        'embedding_loss_alpha': EMBEDDING_LOSS_ALPHA,\n",
    "        'stability_weight': STABILITY_WEIGHT,\n",
    "        'diversity_weight': DIVERSITY_WEIGHT,\n",
    "        'structure_weight': STRUCTURE_WEIGHT,\n",
    "        # NEW shape preservation config\n",
    "        'false_air_weight': FALSE_AIR_WEIGHT,\n",
    "        'volume_weight': VOLUME_WEIGHT,\n",
    "        'structure_to_air_weight': STRUCTURE_TO_AIR_WEIGHT,\n",
    "        'use_shape_loss': USE_SHAPE_LOSS,\n",
    "        'use_asymmetric_loss': USE_ASYMMETRIC_LOSS,\n",
    "        'seed': SEED,\n",
    "    },\n",
    "    'results': {\n",
    "        # Shape preservation metrics (THE KEY METRICS)\n",
    "        'best_struct_recall': float(best_struct_recall),\n",
    "        'best_epoch': best_epoch,\n",
    "        'final_struct_recall': float(history['val_struct_recall'][-1]),\n",
    "        'final_false_air_rate': float(history['val_false_air'][-1]),\n",
    "        'final_vol_ratio': float(history['val_vol_ratio'][-1]),\n",
    "        # Original metrics\n",
    "        'final_struct_exact': float(history['val_struct_exact'][-1]),\n",
    "        'final_struct_sim': float(history['val_struct_sim'][-1]),\n",
    "        'final_cb_usage': float(history['val_cb_usage'][-1]),\n",
    "        'final_perplexity': float(history['val_perplexity'][-1]),\n",
    "        'training_time_min': float(train_time / 60),\n",
    "    },\n",
    "    'history': {k: [float(x) if isinstance(x, (int, float)) else x for x in v] \n",
    "                for k, v in history.items()},\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/vqvae_v4_results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Save complete checkpoint with all metadata for visualization script\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'emb_dim': EMBEDDING_DIM,\n",
    "        'hidden_dims': HIDDEN_DIMS,\n",
    "        'latent_dim': LATENT_DIM,\n",
    "        'num_codes': NUM_CODEBOOK_ENTRIES,\n",
    "        'embedding_loss_alpha': EMBEDDING_LOSS_ALPHA,\n",
    "        'stability_weight': STABILITY_WEIGHT,\n",
    "        'diversity_weight': DIVERSITY_WEIGHT,\n",
    "        'false_air_weight': FALSE_AIR_WEIGHT,\n",
    "        'volume_weight': VOLUME_WEIGHT,\n",
    "        'structure_to_air_weight': STRUCTURE_TO_AIR_WEIGHT,\n",
    "        'dropout': DROPOUT,\n",
    "        'commitment_cost': COMMITMENT_COST,\n",
    "        'ema_decay': EMA_DECAY,\n",
    "    },\n",
    "    'air_tokens': AIR_TOKENS_LIST,\n",
    "    'best_struct_recall': float(best_struct_recall),\n",
    "    'best_epoch': best_epoch,\n",
    "    'training_time_min': float(train_time / 60),\n",
    "}\n",
    "\n",
    "# Save best and final checkpoints\n",
    "torch.save(checkpoint, f\"{OUTPUT_DIR}/vqvae_v4_best_checkpoint.pt\")\n",
    "\n",
    "checkpoint['model_state_dict'] = model.state_dict()  # Update to final state\n",
    "torch.save(checkpoint, f\"{OUTPUT_DIR}/vqvae_v4_final_checkpoint.pt\")\n",
    "\n",
    "# Also save just the state dict for backwards compatibility\n",
    "torch.save(model.state_dict(), f\"{OUTPUT_DIR}/vqvae_v4_final.pt\")\n",
    "\n",
    "print(\"\\nResults saved to Google Drive:\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v4_results.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v4_best_checkpoint.pt (full checkpoint)\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v4_final_checkpoint.pt (full checkpoint)\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v4_final.pt (state dict only)\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v4_training.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS - SHAPE PRESERVATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best structure recall:     {best_struct_recall:.1%} (epoch {best_epoch})\")\n",
    "print(f\"Final structure recall:    {history['val_struct_recall'][-1]:.1%}\")\n",
    "print(f\"Final false air rate:      {history['val_false_air'][-1]:.1%}\")\n",
    "print(f\"Final volume ratio:        {history['val_vol_ratio'][-1]:.2f}\")\n",
    "print(f\"Final structure exact acc: {history['val_struct_exact'][-1]:.1%}\")\n",
    "print(f\"Final codebook usage:      {history['val_cb_usage'][-1]:.1%}\")\n",
    "print(f\"Training time:             {train_time/60:.1f} minutes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"Structure Recall: % of original building blocks NOT erased\")\n",
    "print(\"  - Target: >90% (buildings should be preserved)\")\n",
    "print(\"  - If low: model is still erasing buildings -> increase shape loss weights\")\n",
    "print()\n",
    "print(\"False Air Rate: % of structure blocks wrongly predicted as air\")\n",
    "print(\"  - Target: <10% (minimal building erasure)\")\n",
    "print(\"  - This is 1 - Structure Recall\")\n",
    "print()\n",
    "print(\"Volume Ratio: predicted_volume / original_volume\")\n",
    "print(\"  - Target: ~1.0 (same amount of blocks)\")\n",
    "print(\"  - <1.0 means buildings shrunk, >1.0 means extra blocks added\")\n",
    "\n",
    "print(\"\\n--- DOWNLOAD INSTRUCTIONS ---\")\n",
    "print(\"The checkpoint files are saved to your Google Drive at:\")\n",
    "print(f\"  {OUTPUT_DIR}/\")\n",
    "print(\"\\nYou can download vqvae_v4_best_checkpoint.pt and use with visualization script:\")\n",
    "print(\"  python scripts/visualize_reconstruction_mcp.py \\\\\")\n",
    "print(\"      --checkpoint vqvae_v4_best_checkpoint.pt \\\\\")\n",
    "print(\"      --h5-file path/to/build.h5 \\\\\")\n",
    "print(\"      --output commands.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 11: Reconstruction Visualizations - DIAGNOSTIC PLOTS\n# ============================================================\n# These visualizations help diagnose issues that metrics alone can miss,\n# like the building deletion problem we discovered in Phase 2.5.\n\nprint(\"=\"*70)\nprint(\"DIAGNOSTIC VISUALIZATIONS\")\nprint(\"=\"*70)\nprint(\"These plots help catch issues that metrics alone might miss:\")\nprint(\"  1. Reconstruction slices - see if buildings are visually preserved\")\nprint(\"  2. Block substitution matrix - what's replacing what\")\nprint(\"  3. Error heatmaps - where are mistakes concentrated\")\nprint(\"  4. Category accuracy - which block types fail completely\")\nprint()\n\n# Load best model for visualization\nmodel.load_state_dict(torch.load(f\"{OUTPUT_DIR}/vqvae_v4_best.pt\"))\nmodel.eval()\n\n# Get a batch for visualization\nvis_samples = []\nfor i, sample in enumerate(val_dataset):\n    if i >= 8:  # Get 8 samples for visualization\n        break\n    vis_samples.append(sample)\n\nvis_batch = torch.stack(vis_samples).to(device)\n\n# Get reconstructions\nwith torch.no_grad():\n    out = model(vis_batch)\n    logits = out['logits']\n    preds = logits.argmax(dim=1)\n\nprint(f\"Loaded {len(vis_samples)} samples for visualization\")\nprint(f\"Original shape: {vis_batch.shape}\")\nprint(f\"Predictions shape: {preds.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 12: Slice-by-Slice Reconstruction Comparison\n# ============================================================\n# This is the MOST IMPORTANT diagnostic - visually compare original vs reconstruction\n# If buildings are being erased, you'll SEE it here even if metrics look okay\n\ndef plot_reconstruction_slices(original, reconstructed, air_tokens, sample_idx=0, \n                               slices=[8, 12, 16, 20, 24], save_path=None):\n    \"\"\"\n    Plot horizontal slices showing:\n    - Original structure (colored by block)\n    - Reconstructed structure\n    - Difference (red=erased, blue=added)\n    \"\"\"\n    orig = original.cpu().numpy()\n    recon = reconstructed.cpu().numpy()\n    \n    air_set = set(air_tokens.tolist()) if hasattr(air_tokens, 'tolist') else set(air_tokens)\n    \n    # Create structure masks\n    orig_struct = ~np.isin(orig, list(air_set))\n    recon_struct = ~np.isin(recon, list(air_set))\n    \n    fig, axes = plt.subplots(len(slices), 4, figsize=(16, 4*len(slices)))\n    \n    for i, y in enumerate(slices):\n        if y >= orig.shape[0]:\n            continue\n            \n        # Original structure slice\n        ax = axes[i, 0]\n        orig_slice = orig[y]\n        ax.imshow(orig_slice, cmap='tab20', interpolation='nearest')\n        ax.set_title(f'Original (y={y})')\n        ax.set_ylabel(f'Slice {y}')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n        # Reconstructed slice\n        ax = axes[i, 1]\n        recon_slice = recon[y]\n        ax.imshow(recon_slice, cmap='tab20', interpolation='nearest')\n        ax.set_title(f'Reconstructed (y={y})')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n        # Structure mask comparison (binary)\n        ax = axes[i, 2]\n        orig_mask = orig_struct[y].astype(float)\n        recon_mask = recon_struct[y].astype(float)\n        # Stack: original in green, recon in blue\n        comparison = np.stack([np.zeros_like(orig_mask), orig_mask, recon_mask], axis=-1)\n        ax.imshow(comparison)\n        ax.set_title('Structure: Green=Orig, Blue=Recon')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n        # Difference map\n        ax = axes[i, 3]\n        diff = np.zeros((*orig_mask.shape, 3))\n        # Red: erased (was structure, now air) - THIS IS THE PROBLEM WE'RE LOOKING FOR\n        erased = orig_struct[y] & ~recon_struct[y]\n        diff[erased] = [1, 0, 0]  # Red\n        # Blue: added (was air, now structure)\n        added = ~orig_struct[y] & recon_struct[y]\n        diff[added] = [0, 0, 1]  # Blue\n        # Green: correct structure\n        correct = orig_struct[y] & recon_struct[y]\n        diff[correct] = [0, 0.5, 0]  # Dark green\n        ax.imshow(diff)\n        erased_count = erased.sum()\n        added_count = added.sum()\n        ax.set_title(f'Diff: RED=erased({erased_count}), BLUE=added({added_count})')\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    plt.suptitle(f'Sample {sample_idx}: Reconstruction Analysis\\n'\n                 f'RED cells = BUILDING DELETED (structure->air) - This is what we want to minimize!', \n                 fontsize=12, fontweight='bold')\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # Summary stats\n    total_erased = (orig_struct & ~recon_struct).sum()\n    total_added = (~orig_struct & recon_struct).sum()\n    total_orig = orig_struct.sum()\n    total_recon = recon_struct.sum()\n    \n    print(f\"\\nSample {sample_idx} Structure Analysis:\")\n    print(f\"  Original structure blocks: {total_orig}\")\n    print(f\"  Reconstructed structure blocks: {total_recon}\")\n    print(f\"  ERASED (structure->air): {total_erased} ({100*total_erased/max(total_orig,1):.1f}%)\")\n    print(f\"  Added (air->structure): {total_added}\")\n    print(f\"  Volume ratio: {total_recon/max(total_orig,1):.2f}\")\n\n# Plot first 4 samples\nfor idx in range(min(4, len(vis_samples))):\n    plot_reconstruction_slices(\n        vis_batch[idx], preds[idx], AIR_TOKENS_TENSOR,\n        sample_idx=idx,\n        slices=[4, 8, 12, 16, 20, 24, 28],\n        save_path=f\"{OUTPUT_DIR}/reconstruction_sample_{idx}.png\"\n    )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 13: Block Substitution Analysis\n# ============================================================\n# This shows WHAT blocks are being replaced by WHAT\n# Key insight: If everything is being replaced by air, that's the erasure problem\n# If oak_planks -> spruce_planks, that's less concerning (similar blocks)\n\ndef analyze_block_substitutions(original, reconstructed, tok2block, air_tokens, top_k=20):\n    \"\"\"\n    Analyze what blocks are being substituted for what.\n    Returns the most common (original -> predicted) pairs.\n    \"\"\"\n    orig = original.cpu().numpy().flatten()\n    recon = reconstructed.cpu().numpy().flatten()\n    \n    air_set = set(air_tokens.tolist()) if hasattr(air_tokens, 'tolist') else set(air_tokens)\n    \n    # Count substitutions\n    substitutions = Counter()\n    struct_to_air = Counter()  # THE BAD ONES\n    air_to_struct = Counter()\n    struct_to_struct = Counter()\n    \n    for o, r in zip(orig, recon):\n        if o != r:  # Only count errors\n            orig_is_struct = o not in air_set\n            recon_is_air = r in air_set\n            recon_is_struct = not recon_is_air\n            \n            orig_name = tok2block.get(o, f\"tok_{o}\")[:25]\n            recon_name = tok2block.get(r, f\"tok_{r}\")[:25]\n            \n            if orig_is_struct and recon_is_air:\n                # STRUCTURE ERASED - this is the problem we're tracking\n                struct_to_air[(orig_name, recon_name)] += 1\n            elif not orig_is_struct and recon_is_struct:\n                # Air -> structure (adding blocks)\n                air_to_struct[(orig_name, recon_name)] += 1\n            elif orig_is_struct and recon_is_struct:\n                # Structure -> different structure (material confusion)\n                struct_to_struct[(orig_name, recon_name)] += 1\n                \n            substitutions[(orig_name, recon_name)] += 1\n    \n    return {\n        'all': substitutions.most_common(top_k),\n        'struct_to_air': struct_to_air.most_common(top_k),\n        'air_to_struct': air_to_struct.most_common(top_k),\n        'struct_to_struct': struct_to_struct.most_common(top_k),\n    }\n\n# Analyze all validation samples\nall_subs = {'all': Counter(), 'struct_to_air': Counter(), \n            'air_to_struct': Counter(), 'struct_to_struct': Counter()}\n\nprint(\"Analyzing block substitutions across validation set...\")\nfor sample in tqdm(val_dataset, desc=\"Analyzing\"):\n    sample = sample.unsqueeze(0).to(device)\n    with torch.no_grad():\n        out = model(sample)\n        pred = out['logits'].argmax(dim=1)\n    \n    subs = analyze_block_substitutions(sample[0], pred[0], tok2block, AIR_TOKENS_TENSOR, top_k=100)\n    for key in all_subs:\n        all_subs[key].update(dict(subs[key]))\n\n# Plot top substitutions\nfig, axes = plt.subplots(2, 2, figsize=(18, 14))\n\n# 1. STRUCTURE -> AIR (THE PROBLEM WE'RE WATCHING FOR)\nax = axes[0, 0]\ns2a = all_subs['struct_to_air'].most_common(15)\nif s2a:\n    labels = [f\"{o}->{r}\" for (o,r), _ in s2a]\n    counts = [c for _, c in s2a]\n    bars = ax.barh(range(len(labels)), counts, color='red', alpha=0.7)\n    ax.set_yticks(range(len(labels)))\n    ax.set_yticklabels(labels, fontsize=8)\n    ax.invert_yaxis()\n    ax.set_xlabel('Count')\n    ax.set_title('STRUCTURE -> AIR (Buildings Erased)\\nTHIS IS THE PROBLEM - should be minimal!', \n                 fontweight='bold', color='red')\nelse:\n    ax.text(0.5, 0.5, 'No structure->air errors!\\nGreat!', ha='center', va='center', fontsize=14)\n    ax.set_title('STRUCTURE -> AIR', fontweight='bold', color='green')\n\n# 2. AIR -> STRUCTURE\nax = axes[0, 1]\na2s = all_subs['air_to_struct'].most_common(15)\nif a2s:\n    labels = [f\"{o}->{r}\" for (o,r), _ in a2s]\n    counts = [c for _, c in a2s]\n    ax.barh(range(len(labels)), counts, color='blue', alpha=0.7)\n    ax.set_yticks(range(len(labels)))\n    ax.set_yticklabels(labels, fontsize=8)\n    ax.invert_yaxis()\n    ax.set_xlabel('Count')\n    ax.set_title('AIR -> STRUCTURE (Extra Blocks Added)\\nLess concerning - adds blocks instead of removing', \n                 color='blue')\nelse:\n    ax.text(0.5, 0.5, 'No air->structure errors!', ha='center', va='center', fontsize=14)\n    ax.set_title('AIR -> STRUCTURE', color='blue')\n\n# 3. STRUCTURE -> STRUCTURE (material confusion)\nax = axes[1, 0]\ns2s = all_subs['struct_to_struct'].most_common(15)\nif s2s:\n    labels = [f\"{o}->{r}\" for (o,r), _ in s2s]\n    counts = [c for _, c in s2s]\n    ax.barh(range(len(labels)), counts, color='orange', alpha=0.7)\n    ax.set_yticks(range(len(labels)))\n    ax.set_yticklabels(labels, fontsize=8)\n    ax.invert_yaxis()\n    ax.set_xlabel('Count')\n    ax.set_title('STRUCTURE -> STRUCTURE (Material Confusion)\\nAcceptable if similar blocks (oak->spruce)', \n                 color='orange')\nelse:\n    ax.text(0.5, 0.5, 'No struct->struct errors!', ha='center', va='center', fontsize=14)\n    ax.set_title('STRUCTURE -> STRUCTURE', color='orange')\n\n# 4. Summary pie chart\nax = axes[1, 1]\ntotal_s2a = sum(all_subs['struct_to_air'].values())\ntotal_a2s = sum(all_subs['air_to_struct'].values())\ntotal_s2s = sum(all_subs['struct_to_struct'].values())\ntotal_correct = sum((vis_batch == preds).float().sum().item() for _ in range(1))  # Placeholder\n\nsizes = [total_s2a, total_a2s, total_s2s]\nlabels = [f'Struct->Air\\n(ERASURE)\\n{total_s2a:,}', \n          f'Air->Struct\\n{total_a2s:,}', \n          f'Struct->Struct\\n{total_s2s:,}']\ncolors = ['red', 'blue', 'orange']\nexplode = (0.1, 0, 0)  # Emphasize the bad one\n\nif sum(sizes) > 0:\n    ax.pie(sizes, labels=labels, colors=colors, explode=explode, \n           autopct='%1.1f%%', startangle=90)\n    ax.set_title('Error Type Distribution')\nelse:\n    ax.text(0.5, 0.5, 'No errors!', ha='center', va='center', fontsize=14)\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/block_substitutions.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"BLOCK SUBSTITUTION SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Structure -> Air (ERASURE): {total_s2a:,} errors\")\nprint(f\"Air -> Structure (adding):  {total_a2s:,} errors\")\nprint(f\"Struct -> Struct (confusion): {total_s2s:,} errors\")\nprint()\nif total_s2a > total_s2s:\n    print(\"WARNING: More erasure than material confusion!\")\n    print(\"The model is DELETING buildings more than getting materials wrong.\")\nelse:\n    print(\"Good: Material confusion > erasure. Shape is being preserved.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 14: Per-Category Accuracy Breakdown\n# ============================================================\n# In Phase 2.5, we saw 9/16 categories at 0% accuracy!\n# This helps identify which block types the model can't predict at all\n\n# Define block categories based on naming patterns\nBLOCK_CATEGORIES = {\n    'wood_planks': lambda b: 'planks' in b and 'slab' not in b and 'stair' not in b,\n    'logs': lambda b: 'log' in b or 'wood' in b and 'planks' not in b,\n    'stone': lambda b: any(s in b for s in ['stone', 'cobblestone', 'granite', 'diorite', 'andesite']) \n                       and 'slab' not in b and 'stair' not in b and 'wall' not in b,\n    'stairs': lambda b: 'stair' in b,\n    'slabs': lambda b: 'slab' in b,\n    'walls': lambda b: 'wall' in b and 'banner' not in b,\n    'fences': lambda b: 'fence' in b,\n    'doors': lambda b: 'door' in b,\n    'glass': lambda b: 'glass' in b,\n    'wool': lambda b: 'wool' in b,\n    'concrete': lambda b: 'concrete' in b,\n    'terracotta': lambda b: 'terracotta' in b,\n    'bricks': lambda b: 'brick' in b and 'slab' not in b and 'stair' not in b and 'wall' not in b,\n    'ores': lambda b: 'ore' in b,\n    'leaves': lambda b: 'leaves' in b or 'leaf' in b,\n    'flowers': lambda b: any(f in b for f in ['dandelion', 'poppy', 'tulip', 'daisy', 'orchid', 'allium', 'rose']),\n    'crops': lambda b: any(c in b for c in ['wheat', 'carrot', 'potato', 'beetroot', 'melon', 'pumpkin']),\n    'redstone': lambda b: any(r in b for r in ['redstone', 'repeater', 'comparator', 'piston', 'observer', 'dispenser']),\n    'lighting': lambda b: any(l in b for l in ['torch', 'lantern', 'lamp', 'glowstone', 'sea_lantern', 'shroomlight']),\n    'water': lambda b: 'water' in b,\n    'lava': lambda b: 'lava' in b,\n}\n\ndef categorize_block(block_name):\n    \"\"\"Assign a block to a category.\"\"\"\n    block_lower = block_name.lower()\n    for cat, check_fn in BLOCK_CATEGORIES.items():\n        if check_fn(block_lower):\n            return cat\n    return 'other'\n\n# Build token -> category mapping\ntok2cat = {}\nfor tok, block in tok2block.items():\n    tok2cat[tok] = categorize_block(block)\n\n# Compute per-category accuracy\ncat_correct = Counter()\ncat_total = Counter()\ncat_recall = Counter()  # Structure recall per category\ncat_struct_total = Counter()\n\nprint(\"Computing per-category accuracy...\")\nfor sample in tqdm(val_dataset, desc=\"Categories\"):\n    sample_t = sample.unsqueeze(0).to(device)\n    with torch.no_grad():\n        out = model(sample_t)\n        pred = out['logits'].argmax(dim=1)[0]\n    \n    orig = sample.cpu().numpy().flatten()\n    recon = pred.cpu().numpy().flatten()\n    air_set = set(AIR_TOKENS_TENSOR.tolist())\n    \n    for o, r in zip(orig, recon):\n        cat = tok2cat.get(o, 'other')\n        is_struct = o not in air_set\n        \n        if is_struct:\n            cat_struct_total[cat] += 1\n            if r not in air_set:  # Predicted as structure (not erased)\n                cat_recall[cat] += 1\n        \n        cat_total[cat] += 1\n        if o == r:\n            cat_correct[cat] += 1\n\n# Compute accuracies\ncategories = sorted(set(cat_total.keys()) - {'other', 'air'})\nexact_accs = []\nrecalls = []\n\nfor cat in categories:\n    total = cat_total.get(cat, 0)\n    correct = cat_correct.get(cat, 0)\n    struct_total = cat_struct_total.get(cat, 0)\n    recall = cat_recall.get(cat, 0)\n    \n    exact_acc = correct / total if total > 0 else 0\n    struct_recall = recall / struct_total if struct_total > 0 else 0\n    \n    exact_accs.append(exact_acc)\n    recalls.append(struct_recall)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Exact accuracy per category\nax = axes[0]\ncolors = ['red' if acc == 0 else 'orange' if acc < 0.3 else 'green' for acc in exact_accs]\nbars = ax.barh(categories, exact_accs, color=colors, alpha=0.7)\nax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='50%')\nax.set_xlabel('Exact Accuracy')\nax.set_title('Per-Category Exact Accuracy\\nRED = 0% (never predicted correctly)')\nax.set_xlim(0, 1)\nfor bar, acc in zip(bars, exact_accs):\n    ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2, \n            f'{acc:.1%}', va='center', fontsize=8)\n\n# Structure recall per category (shape preservation)\nax = axes[1]\ncolors = ['red' if r < 0.5 else 'orange' if r < 0.8 else 'green' for r in recalls]\nbars = ax.barh(categories, recalls, color=colors, alpha=0.7)\nax.axvline(x=0.9, color='gray', linestyle='--', alpha=0.5, label='90% target')\nax.set_xlabel('Structure Recall (not erased)')\nax.set_title('Per-Category Structure Recall\\nRED = blocks being erased')\nax.set_xlim(0, 1)\nfor bar, r in zip(bars, recalls):\n    ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2, \n            f'{r:.1%}', va='center', fontsize=8)\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/category_accuracy.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\n# Print summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"PER-CATEGORY BREAKDOWN\")\nprint(\"=\"*60)\nprint(f\"{'Category':<15} {'Count':>8} {'Exact':>8} {'Recall':>8}\")\nprint(\"-\"*45)\n\nzero_acc_cats = []\nlow_recall_cats = []\n\nfor cat, acc, rec in zip(categories, exact_accs, recalls):\n    count = cat_total.get(cat, 0)\n    print(f\"{cat:<15} {count:>8,} {acc:>7.1%} {rec:>7.1%}\")\n    if acc == 0:\n        zero_acc_cats.append(cat)\n    if rec < 0.5:\n        low_recall_cats.append(cat)\n\nprint(\"-\"*45)\nprint(f\"\\nCategories with 0% exact accuracy: {len(zero_acc_cats)}\")\nif zero_acc_cats:\n    print(f\"  {', '.join(zero_acc_cats)}\")\nprint(f\"\\nCategories with <50% recall (being erased): {len(low_recall_cats)}\")\nif low_recall_cats:\n    print(f\"  {', '.join(low_recall_cats)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 15: Error Location Heatmaps\n# ============================================================\n# Where in the structure are errors concentrated?\n# - Center vs edges?\n# - Top vs bottom?\n# - Are corners being erased?\n\ndef compute_error_heatmaps(original, reconstructed, air_tokens):\n    \"\"\"Compute spatial heatmaps of different error types.\"\"\"\n    orig = original.cpu().numpy()\n    recon = reconstructed.cpu().numpy()\n    air_set = set(air_tokens.tolist()) if hasattr(air_tokens, 'tolist') else set(air_tokens)\n    \n    orig_struct = ~np.isin(orig, list(air_set))\n    recon_struct = ~np.isin(recon, list(air_set))\n    \n    # Error masks\n    erasure = orig_struct & ~recon_struct  # Structure deleted\n    addition = ~orig_struct & recon_struct  # Extra blocks\n    confusion = orig_struct & recon_struct & (orig != recon)  # Wrong material\n    correct = orig == recon\n    \n    return {\n        'erasure': erasure.astype(float),\n        'addition': addition.astype(float),\n        'confusion': confusion.astype(float),\n        'correct': correct.astype(float),\n        'orig_struct': orig_struct.astype(float),\n    }\n\n# Aggregate error heatmaps across all validation samples\nprint(\"Computing spatial error distribution...\")\nagg_erasure = np.zeros((32, 32, 32))\nagg_addition = np.zeros((32, 32, 32))\nagg_confusion = np.zeros((32, 32, 32))\nagg_struct = np.zeros((32, 32, 32))\nn_samples = 0\n\nfor sample in tqdm(val_dataset, desc=\"Error locations\"):\n    sample_t = sample.unsqueeze(0).to(device)\n    with torch.no_grad():\n        out = model(sample_t)\n        pred = out['logits'].argmax(dim=1)[0]\n    \n    heatmaps = compute_error_heatmaps(sample, pred, AIR_TOKENS_TENSOR)\n    agg_erasure += heatmaps['erasure']\n    agg_addition += heatmaps['addition']\n    agg_confusion += heatmaps['confusion']\n    agg_struct += heatmaps['orig_struct']\n    n_samples += 1\n\n# Normalize by number of samples\nagg_erasure /= n_samples\nagg_addition /= n_samples\nagg_confusion /= n_samples\nagg_struct /= n_samples\n\n# Plot XZ projection (top-down view) and XY projection (side view)\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\n\n# Top-down views (sum over Y axis)\nfor idx, (name, data, cmap, title) in enumerate([\n    ('erasure', agg_erasure, 'Reds', 'ERASURE (structure->air)'),\n    ('addition', agg_addition, 'Blues', 'Addition (air->structure)'),\n    ('confusion', agg_confusion, 'Oranges', 'Material Confusion'),\n    ('struct', agg_struct, 'Greens', 'Original Structure Density'),\n]):\n    ax = axes[0, idx]\n    proj = data.sum(axis=0)  # Sum over Y (height)\n    im = ax.imshow(proj, cmap=cmap, interpolation='nearest')\n    ax.set_title(f'{title}\\n(Top-down projection)')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Z')\n    plt.colorbar(im, ax=ax, fraction=0.046)\n\n# Side views (sum over Z axis)\nfor idx, (name, data, cmap, title) in enumerate([\n    ('erasure', agg_erasure, 'Reds', 'ERASURE'),\n    ('addition', agg_addition, 'Blues', 'Addition'),\n    ('confusion', agg_confusion, 'Oranges', 'Confusion'),\n    ('struct', agg_struct, 'Greens', 'Structure'),\n]):\n    ax = axes[1, idx]\n    proj = data.sum(axis=2)  # Sum over Z\n    im = ax.imshow(proj.T, cmap=cmap, origin='lower', interpolation='nearest')\n    ax.set_title(f'{title}\\n(Side projection)')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y (height)')\n    plt.colorbar(im, ax=ax, fraction=0.046)\n\nplt.suptitle('Spatial Error Distribution\\nRed areas = where buildings are being erased', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/error_heatmaps.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\n# Analyze error distribution by region\nprint(\"\\n\" + \"=\"*60)\nprint(\"SPATIAL ERROR ANALYSIS\")\nprint(\"=\"*60)\n\n# Compute errors in different regions\ncenter = slice(10, 22)\nedge_x = [slice(0, 8), slice(24, 32)]\nedge_z = [slice(0, 8), slice(24, 32)]\n\ncenter_erasure = agg_erasure[:, center, center].mean()\nedge_erasure = np.mean([agg_erasure[:, e, :].mean() for e in edge_x] + \n                        [agg_erasure[:, :, e].mean() for e in edge_z])\n\ncenter_struct = agg_struct[:, center, center].mean()\nedge_struct = np.mean([agg_struct[:, e, :].mean() for e in edge_x] + \n                       [agg_struct[:, :, e].mean() for e in edge_z])\n\nprint(f\"Erasure rate in CENTER: {center_erasure:.4f}\")\nprint(f\"Erasure rate at EDGES:  {edge_erasure:.4f}\")\nprint(f\"Structure density in CENTER: {center_struct:.4f}\")\nprint(f\"Structure density at EDGES:  {edge_struct:.4f}\")\n\nif center_erasure > edge_erasure * 1.5:\n    print(\"\\nWARNING: Center is being erased more than edges!\")\n    print(\"This might indicate the latent grid is too coarse for interior details.\")\nelif edge_erasure > center_erasure * 1.5:\n    print(\"\\nWARNING: Edges are being erased more than center!\")\n    print(\"The model might be having trouble with boundary regions.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 16: Worst Reconstructions Analysis\n# ============================================================\n# Examine the failure cases to understand what the model struggles with\n# Sorted by: lowest structure recall (most building deleted)\n\nprint(\"Finding worst reconstructions (most building erased)...\")\n\nsample_stats = []\nfor idx, sample in enumerate(tqdm(val_dataset, desc=\"Evaluating\")):\n    sample_t = sample.unsqueeze(0).to(device)\n    with torch.no_grad():\n        out = model(sample_t)\n        pred = out['logits'].argmax(dim=1)[0]\n    \n    orig = sample.cpu().numpy()\n    recon = pred.cpu().numpy()\n    air_set = set(AIR_TOKENS_TENSOR.tolist())\n    \n    orig_struct = ~np.isin(orig, list(air_set))\n    recon_struct = ~np.isin(recon, list(air_set))\n    \n    orig_vol = orig_struct.sum()\n    recon_vol = recon_struct.sum()\n    erased = (orig_struct & ~recon_struct).sum()\n    \n    recall = 1 - (erased / orig_vol) if orig_vol > 0 else 1.0\n    vol_ratio = recon_vol / orig_vol if orig_vol > 0 else 1.0\n    \n    sample_stats.append({\n        'idx': idx,\n        'orig_vol': orig_vol,\n        'recon_vol': recon_vol,\n        'erased': erased,\n        'recall': recall,\n        'vol_ratio': vol_ratio,\n        'sample': sample,\n        'pred': pred.cpu(),\n    })\n\n# Sort by worst recall (most erased)\nsample_stats.sort(key=lambda x: x['recall'])\n\n# Show worst 5\nprint(\"\\n\" + \"=\"*60)\nprint(\"5 WORST RECONSTRUCTIONS (most building erased)\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(5, 5, figsize=(20, 20))\n\nfor row, stats in enumerate(sample_stats[:5]):\n    idx = stats['idx']\n    orig = stats['sample'].numpy()\n    recon = stats['pred'].numpy()\n    \n    air_set = set(AIR_TOKENS_TENSOR.tolist())\n    orig_struct = ~np.isin(orig, list(air_set))\n    recon_struct = ~np.isin(recon, list(air_set))\n    \n    # Find best slice to show (most structure)\n    struct_per_slice = orig_struct.sum(axis=(1, 2))\n    best_y = struct_per_slice.argmax()\n    \n    # Original blocks\n    axes[row, 0].imshow(orig[best_y], cmap='tab20', interpolation='nearest')\n    axes[row, 0].set_title(f'Original (y={best_y})')\n    axes[row, 0].set_ylabel(f'Sample {idx}\\nRecall: {stats[\"recall\"]:.1%}')\n    \n    # Reconstructed blocks\n    axes[row, 1].imshow(recon[best_y], cmap='tab20', interpolation='nearest')\n    axes[row, 1].set_title(f'Reconstructed')\n    \n    # Original structure mask\n    axes[row, 2].imshow(orig_struct[best_y], cmap='Greens', interpolation='nearest')\n    axes[row, 2].set_title(f'Orig Structure')\n    \n    # Reconstructed structure mask\n    axes[row, 3].imshow(recon_struct[best_y], cmap='Blues', interpolation='nearest')\n    axes[row, 3].set_title(f'Recon Structure')\n    \n    # Difference (red = erased)\n    diff = np.zeros((*orig_struct[best_y].shape, 3))\n    erased = orig_struct[best_y] & ~recon_struct[best_y]\n    added = ~orig_struct[best_y] & recon_struct[best_y]\n    correct = orig_struct[best_y] & recon_struct[best_y]\n    diff[erased] = [1, 0, 0]\n    diff[added] = [0, 0, 1]\n    diff[correct] = [0, 0.5, 0]\n    axes[row, 4].imshow(diff)\n    axes[row, 4].set_title(f'Diff: {stats[\"erased\"]} erased')\n\nfor ax in axes.flat:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.suptitle('WORST 5 RECONSTRUCTIONS\\n'\n             'Red = erased blocks, Blue = added blocks, Green = preserved\\n'\n             'These show what the model struggles with most', \n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/worst_reconstructions.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\n# Summary stats\nprint(\"\\nWorst 5 samples:\")\nprint(f\"{'Idx':>5} {'Orig Vol':>10} {'Recon Vol':>10} {'Erased':>8} {'Recall':>8} {'Vol Ratio':>10}\")\nprint(\"-\"*55)\nfor stats in sample_stats[:5]:\n    print(f\"{stats['idx']:>5} {stats['orig_vol']:>10,} {stats['recon_vol']:>10,} \"\n          f\"{stats['erased']:>8,} {stats['recall']:>7.1%} {stats['vol_ratio']:>9.2f}\")\n\n# Show best 5 for comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"5 BEST RECONSTRUCTIONS (for comparison)\")\nprint(\"=\"*60)\nprint(f\"{'Idx':>5} {'Orig Vol':>10} {'Recon Vol':>10} {'Erased':>8} {'Recall':>8} {'Vol Ratio':>10}\")\nprint(\"-\"*55)\nfor stats in sample_stats[-5:]:\n    print(f\"{stats['idx']:>5} {stats['orig_vol']:>10,} {stats['recon_vol']:>10,} \"\n          f\"{stats['erased']:>8,} {stats['recall']:>7.1%} {stats['vol_ratio']:>9.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 17: Volume Distribution Analysis\n# ============================================================\n# Are buildings systematically shrinking or growing?\n# This shows the distribution of volume ratios across all samples\n\n# Extract volume stats (already computed in previous cell)\norig_volumes = [s['orig_vol'] for s in sample_stats]\nrecon_volumes = [s['recon_vol'] for s in sample_stats]\nvol_ratios = [s['vol_ratio'] for s in sample_stats]\nrecalls = [s['recall'] for s in sample_stats]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Histogram of volume ratios\nax = axes[0, 0]\nax.hist(vol_ratios, bins=50, color='blue', alpha=0.7, edgecolor='black')\nax.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='Perfect (1.0)')\nax.axvline(x=np.mean(vol_ratios), color='green', linestyle='-', linewidth=2, \n           label=f'Mean ({np.mean(vol_ratios):.2f})')\nax.set_xlabel('Volume Ratio (recon/orig)')\nax.set_ylabel('Count')\nax.set_title('Volume Ratio Distribution\\n<1.0 = shrinking, >1.0 = growing')\nax.legend()\n\n# 2. Scatter: Original vs Reconstructed volume\nax = axes[0, 1]\nax.scatter(orig_volumes, recon_volumes, alpha=0.5, s=20)\nmax_vol = max(max(orig_volumes), max(recon_volumes))\nax.plot([0, max_vol], [0, max_vol], 'r--', label='Perfect reconstruction')\nax.set_xlabel('Original Volume (blocks)')\nax.set_ylabel('Reconstructed Volume (blocks)')\nax.set_title('Original vs Reconstructed Volume\\nPoints below line = shrinking')\nax.legend()\n\n# 3. Histogram of structure recall\nax = axes[1, 0]\nax.hist(recalls, bins=50, color='green', alpha=0.7, edgecolor='black')\nax.axvline(x=0.9, color='red', linestyle='--', linewidth=2, label='Target (90%)')\nax.axvline(x=np.mean(recalls), color='blue', linestyle='-', linewidth=2,\n           label=f'Mean ({np.mean(recalls):.1%})')\nax.set_xlabel('Structure Recall')\nax.set_ylabel('Count')\nax.set_title('Structure Recall Distribution\\n% of original structure preserved')\nax.legend()\n\n# 4. Recall vs Volume ratio scatter\nax = axes[1, 1]\nax.scatter(vol_ratios, recalls, alpha=0.5, s=20, c=orig_volumes, cmap='viridis')\nax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='Target recall')\nax.axvline(x=1.0, color='red', linestyle='--', alpha=0.5, label='Perfect volume')\nax.set_xlabel('Volume Ratio')\nax.set_ylabel('Structure Recall')\nax.set_title('Recall vs Volume Ratio\\nColor = original volume')\ncbar = plt.colorbar(ax.collections[0], ax=ax)\ncbar.set_label('Original Volume')\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/volume_analysis.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\n# Summary statistics\nprint(\"\\n\" + \"=\"*60)\nprint(\"VOLUME ANALYSIS SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Volume Ratio:\")\nprint(f\"  Mean:   {np.mean(vol_ratios):.3f}\")\nprint(f\"  Median: {np.median(vol_ratios):.3f}\")\nprint(f\"  Std:    {np.std(vol_ratios):.3f}\")\nprint(f\"  Min:    {np.min(vol_ratios):.3f}\")\nprint(f\"  Max:    {np.max(vol_ratios):.3f}\")\nprint()\nprint(f\"Structure Recall:\")\nprint(f\"  Mean:   {np.mean(recalls):.1%}\")\nprint(f\"  Median: {np.median(recalls):.1%}\")\nprint(f\"  Std:    {np.std(recalls):.1%}\")\nprint(f\"  Min:    {np.min(recalls):.1%}\")\nprint(f\"  Max:    {np.max(recalls):.1%}\")\n\n# Interpretation\nprint(\"\\n\" + \"=\"*60)\nprint(\"INTERPRETATION\")\nprint(\"=\"*60)\nif np.mean(vol_ratios) < 0.9:\n    print(\"WARNING: Buildings are systematically SHRINKING!\")\n    print(f\"  Average volume loss: {(1 - np.mean(vol_ratios))*100:.1f}%\")\n    print(\"  The model is erasing more structure than it should.\")\nelif np.mean(vol_ratios) > 1.1:\n    print(\"NOTE: Buildings are systematically GROWING!\")\n    print(f\"  Average volume gain: {(np.mean(vol_ratios) - 1)*100:.1f}%\")\n    print(\"  The model is adding extra blocks.\")\nelse:\n    print(\"Good: Volume is well-preserved on average.\")\n\nbelow_90 = sum(1 for r in recalls if r < 0.9)\nbelow_50 = sum(1 for r in recalls if r < 0.5)\nprint(f\"\\nSamples below 90% recall: {below_90} ({100*below_90/len(recalls):.1f}%)\")\nprint(f\"Samples below 50% recall: {below_50} ({100*below_50/len(recalls):.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 18: Embedding Drift Visualization\n# ============================================================\n# Since embeddings are trainable in v4, we need to track how much they changed\n# Too much drift = lost semantic meaning\n# Too little drift = didn't adapt to task\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Get original and current embeddings\noriginal_emb = model.original_embeddings.cpu().numpy()\ncurrent_emb = model.block_emb.weight.detach().cpu().numpy()\n\n# Compute per-embedding drift\ndrifts = []\nfor i in range(len(original_emb)):\n    orig = original_emb[i]\n    curr = current_emb[i]\n    # Cosine similarity\n    cos_sim = np.dot(orig, curr) / (np.linalg.norm(orig) * np.linalg.norm(curr) + 1e-8)\n    # Euclidean distance\n    euc_dist = np.linalg.norm(curr - orig)\n    drifts.append({\n        'token': i,\n        'block': tok2block.get(i, f'tok_{i}'),\n        'cos_sim': cos_sim,\n        'euc_dist': euc_dist,\n    })\n\n# Sort by most drifted\ndrifts.sort(key=lambda x: x['cos_sim'])\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# 1. Histogram of cosine similarities\nax = axes[0, 0]\ncos_sims = [d['cos_sim'] for d in drifts]\nax.hist(cos_sims, bins=50, color='blue', alpha=0.7, edgecolor='black')\nax.axvline(x=np.mean(cos_sims), color='red', linestyle='--', \n           label=f'Mean: {np.mean(cos_sims):.3f}')\nax.axvline(x=0.9, color='green', linestyle=':', label='Stable threshold (0.9)')\nax.set_xlabel('Cosine Similarity (original vs trained)')\nax.set_ylabel('Count')\nax.set_title('Embedding Stability\\n1.0 = unchanged, lower = more drift')\nax.legend()\n\n# 2. Top 20 most drifted embeddings\nax = axes[0, 1]\nmost_drifted = drifts[:20]\nblocks = [d['block'][:20] for d in most_drifted]\nsims = [d['cos_sim'] for d in most_drifted]\ncolors = ['red' if s < 0.5 else 'orange' if s < 0.8 else 'yellow' for s in sims]\nax.barh(range(len(blocks)), sims, color=colors)\nax.set_yticks(range(len(blocks)))\nax.set_yticklabels(blocks, fontsize=8)\nax.set_xlabel('Cosine Similarity')\nax.set_title('20 Most Drifted Embeddings\\n(these changed the most during training)')\nax.set_xlim(0, 1)\nax.invert_yaxis()\n\n# 3. PCA visualization: original vs trained\nax = axes[1, 0]\n\n# Sample 500 embeddings for visualization\nsample_idx = np.random.choice(len(original_emb), min(500, len(original_emb)), replace=False)\n\n# Combine and do PCA\ncombined = np.vstack([original_emb[sample_idx], current_emb[sample_idx]])\npca = PCA(n_components=2)\ncombined_2d = pca.fit_transform(combined)\n\norig_2d = combined_2d[:len(sample_idx)]\ncurr_2d = combined_2d[len(sample_idx):]\n\n# Plot with arrows showing drift\nax.scatter(orig_2d[:, 0], orig_2d[:, 1], c='blue', alpha=0.3, s=20, label='Original')\nax.scatter(curr_2d[:, 0], curr_2d[:, 1], c='red', alpha=0.3, s=20, label='Trained')\n\n# Draw arrows for top 20 most drifted in sample\nsample_drifts = [(i, drifts[sample_idx[i]]['cos_sim']) for i in range(len(sample_idx))]\nsample_drifts.sort(key=lambda x: x[1])\nfor i, _ in sample_drifts[:20]:\n    ax.annotate('', xy=curr_2d[i], xytext=orig_2d[i],\n                arrowprops=dict(arrowstyle='->', color='gray', alpha=0.5))\n\nax.set_xlabel('PCA 1')\nax.set_ylabel('PCA 2')\nax.set_title('Embedding Space: Original (blue) vs Trained (red)\\nArrows show drift direction')\nax.legend()\n\n# 4. Drift by category\nax = axes[1, 1]\ncat_drifts = {}\nfor d in drifts:\n    cat = categorize_block(d['block'])\n    if cat not in cat_drifts:\n        cat_drifts[cat] = []\n    cat_drifts[cat].append(d['cos_sim'])\n\ncat_names = []\ncat_means = []\ncat_stds = []\nfor cat, sims in sorted(cat_drifts.items(), key=lambda x: np.mean(x[1])):\n    if len(sims) >= 5:  # Only categories with enough samples\n        cat_names.append(cat)\n        cat_means.append(np.mean(sims))\n        cat_stds.append(np.std(sims))\n\ncolors = ['red' if m < 0.7 else 'orange' if m < 0.85 else 'green' for m in cat_means]\nax.barh(cat_names, cat_means, xerr=cat_stds, color=colors, alpha=0.7, capsize=3)\nax.axvline(x=0.9, color='gray', linestyle='--', alpha=0.5)\nax.set_xlabel('Mean Cosine Similarity')\nax.set_title('Embedding Drift by Category\\nLower = category changed more')\nax.set_xlim(0, 1)\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/embedding_drift.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\n# Summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"EMBEDDING DRIFT SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Mean cosine similarity: {np.mean(cos_sims):.3f}\")\nprint(f\"Median: {np.median(cos_sims):.3f}\")\nprint(f\"Min: {np.min(cos_sims):.3f}\")\nprint(f\"Max: {np.max(cos_sims):.3f}\")\nprint()\n\n# Interpretation\nif np.mean(cos_sims) > 0.95:\n    print(\"Embeddings barely changed. The regularization may be too strong,\")\n    print(\"or the embedding learning rate too low.\")\nelif np.mean(cos_sims) < 0.7:\n    print(\"WARNING: Embeddings changed significantly!\")\n    print(\"This might mean semantic meaning was lost.\")\n    print(\"Consider increasing stability_weight or decreasing embedding LR.\")\nelse:\n    print(\"Good: Moderate embedding drift. Embeddings adapted while preserving structure.\")\n\nprint(\"\\n5 Most Drifted Embeddings:\")\nfor d in drifts[:5]:\n    print(f\"  {d['block'][:30]:<30} cos_sim={d['cos_sim']:.3f}\")\n\nprint(\"\\n5 Most Stable Embeddings:\")\nfor d in drifts[-5:]:\n    print(f\"  {d['block'][:30]:<30} cos_sim={d['cos_sim']:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 19: Final Diagnostic Summary\n# ============================================================\n# This cell summarizes all diagnostics and provides actionable recommendations\n\nprint(\"=\"*70)\nprint(\"COMPREHENSIVE DIAGNOSTIC SUMMARY\")\nprint(\"=\"*70)\n\nprint(\"\\n### 1. SHAPE PRESERVATION (Primary Goal) ###\")\nprint(f\"Structure Recall:    {np.mean(recalls):.1%} (target: >90%)\")\nprint(f\"False Air Rate:      {1 - np.mean(recalls):.1%} (target: <10%)\")\nprint(f\"Volume Ratio:        {np.mean(vol_ratios):.2f} (target: ~1.0)\")\n\nshape_ok = np.mean(recalls) > 0.85\nif shape_ok:\n    print(\" Shape preservation looks acceptable\")\nelse:\n    print(\" ISSUE: Shape is not being preserved well\")\n\nprint(\"\\n### 2. BLOCK SUBSTITUTION ###\")\nprint(f\"Structure->Air (erasure):   {total_s2a:,} errors\")\nprint(f\"Struct->Struct (confusion): {total_s2s:,} errors\")\n\nif total_s2a < total_s2s:\n    print(\" Material confusion > erasure (good - shape preserved)\")\nelse:\n    print(\" ISSUE: More erasure than confusion (buildings being deleted)\")\n\nprint(\"\\n### 3. PER-CATEGORY ###\")\nprint(f\"Categories with 0% accuracy: {len(zero_acc_cats)}\")\nprint(f\"Categories with <50% recall: {len(low_recall_cats)}\")\n\nif len(zero_acc_cats) < 5:\n    print(\" Most categories are being predicted\")\nelse:\n    print(\" ISSUE: Many categories never predicted correctly\")\n\nprint(\"\\n### 4. EMBEDDING DRIFT ###\")\nprint(f\"Mean embedding similarity: {np.mean(cos_sims):.3f}\")\n\nif 0.7 < np.mean(cos_sims) < 0.95:\n    print(\" Embeddings adapted moderately\")\nelif np.mean(cos_sims) > 0.95:\n    print(\" Embeddings barely changed (may not have adapted)\")\nelse:\n    print(\" ISSUE: Embeddings drifted too much (semantic meaning may be lost)\")\n\nprint(\"\\n### 5. WORST CASES ###\")\nprint(f\"Samples with <50% recall: {below_50}\")\nprint(f\"Samples with <90% recall: {below_90}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RECOMMENDATIONS\")\nprint(\"=\"*70)\n\nrecommendations = []\n\nif np.mean(recalls) < 0.85:\n    recommendations.append(\"- Increase FALSE_AIR_WEIGHT or STRUCTURE_TO_AIR_WEIGHT\")\n    recommendations.append(\"- Try longer training or lower learning rate\")\n\nif total_s2a > total_s2s:\n    recommendations.append(\"- Structure is being erased too much\")\n    recommendations.append(\"- Increase VOLUME_WEIGHT to penalize shrinking\")\n\nif len(zero_acc_cats) > 5:\n    recommendations.append(\"- Many block categories never predicted\")\n    recommendations.append(\"- Check if these blocks exist in training data\")\n    recommendations.append(\"- Consider class weighting or focal loss\")\n\nif np.mean(cos_sims) < 0.7:\n    recommendations.append(\"- Embeddings drifted too much\")\n    recommendations.append(\"- Increase STABILITY_WEIGHT\")\n\nif np.mean(cos_sims) > 0.95:\n    recommendations.append(\"- Embeddings may not be learning\")\n    recommendations.append(\"- Decrease STABILITY_WEIGHT or increase embedding LR\")\n\nif np.mean(vol_ratios) < 0.8:\n    recommendations.append(\"- Buildings are shrinking significantly\")\n    recommendations.append(\"- Volume preservation loss needs to be stronger\")\n\nif not recommendations:\n    print(\"No major issues detected! The model appears to be training well.\")\nelse:\n    for rec in recommendations:\n        print(rec)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"OUTPUT FILES SAVED TO GOOGLE DRIVE\")\nprint(\"=\"*70)\nprint(f\"Location: {OUTPUT_DIR}/\")\nprint()\nprint(\"Checkpoints:\")\nprint(\"  - vqvae_v4_best_checkpoint.pt    (best model by struct_recall)\")\nprint(\"  - vqvae_v4_final_checkpoint.pt   (final model)\")\nprint(\"  - vqvae_v4_results.json          (training history)\")\nprint()\nprint(\"Diagnostic Plots:\")\nprint(\"  - vqvae_v4_training.png          (training curves)\")\nprint(\"  - reconstruction_sample_*.png    (slice-by-slice reconstructions)\")\nprint(\"  - block_substitutions.png        (what's replacing what)\")\nprint(\"  - category_accuracy.png          (per-category breakdown)\")\nprint(\"  - error_heatmaps.png             (spatial error distribution)\")\nprint(\"  - worst_reconstructions.png      (failure cases)\")\nprint(\"  - volume_analysis.png            (volume distribution)\")\nprint(\"  - embedding_drift.png            (embedding changes)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}