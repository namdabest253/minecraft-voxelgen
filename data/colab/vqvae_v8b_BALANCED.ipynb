{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE v8-B Training - Self-Contained Version\n",
    "\n",
    "**âœ… This notebook is completely self-contained** - all model code is included directly!\n",
    "\n",
    "No external file imports needed. Just upload your data to Google Drive and run!\n",
    "\n",
    "## Changes from v6-freq\n",
    "\n",
    "| Change | v6-freq | v8-B |\n",
    "|--------|---------|------|\n",
    "| Latent resolution | 8Ã—8Ã—8 (512 positions) | **16Ã—16Ã—16 (4,096 positions)** |\n",
    "| Compression ratio | 64:1 | **8:1** |\n",
    "| Downsampling stages | 2 (32â†’16â†’8) | **1 (32â†’16)** |\n",
    "| ResBlocks at latent | 2 | **6 (more capacity)** |\n",
    "| Volume penalty | No | **Yes (fixes 1.68x over-prediction)** |\n",
    "| Perceptual loss | No | **Yes (spatial smoothness)** |\n",
    "\n",
    "## Goals\n",
    "\n",
    "| Metric | v6-freq | v8-B Target |\n",
    "|--------|---------|-------------|\n",
    "| Building Accuracy | 49.2% | **60-65%** |\n",
    "| Volume Ratio | 1.68x | **1.1-1.2x** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (uncomment for Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths - CHANGE THIS to match your Google Drive structure\n",
    "DRIVE_BASE = '/content/drive/MyDrive/minecraft_ai'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/minecraft_ai/vqvae_v8b'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Data directory: {DRIVE_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Set, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Code - FSQ and RFSQ Quantization\n",
    "\n",
    "All model code is included inline - no external imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FSQ (Finite Scalar Quantization)\n",
    "# ============================================================================\n",
    "\n",
    "class FSQ(nn.Module):\n",
    "    \"\"\"Finite Scalar Quantization.\"\"\"\n",
    "\n",
    "    def __init__(self, levels: List[int], eps: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.levels = levels\n",
    "        self.dim = len(levels)\n",
    "        self.eps = eps\n",
    "        self.codebook_size = int(np.prod(levels))\n",
    "\n",
    "        self.register_buffer('_levels', torch.tensor(levels, dtype=torch.float32))\n",
    "\n",
    "        basis = []\n",
    "        acc = 1\n",
    "        for L in reversed(levels):\n",
    "            basis.append(acc)\n",
    "            acc *= L\n",
    "        self.register_buffer('_basis', torch.tensor(list(reversed(basis)), dtype=torch.long))\n",
    "\n",
    "        half_levels = [(L - 1) / 2 for L in levels]\n",
    "        self.register_buffer('_half_levels', torch.tensor(half_levels, dtype=torch.float32))\n",
    "\n",
    "    @property\n",
    "    def num_codes(self) -> int:\n",
    "        return self.codebook_size\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        z_bounded = torch.tanh(z)\n",
    "        z_q = self._quantize(z_bounded)\n",
    "        z_q = z_bounded + (z_q - z_bounded).detach()  # Straight-through\n",
    "        indices = self._to_indices(z_q)\n",
    "        return z_q, indices\n",
    "\n",
    "    def _quantize(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        z_q_list = []\n",
    "        for i in range(self.dim):\n",
    "            L = self._levels[i]\n",
    "            half_L = self._half_levels[i]\n",
    "            z_i = z[..., i]\n",
    "            z_i = z_i * half_L\n",
    "            z_i = torch.round(z_i)\n",
    "            z_i = torch.clamp(z_i, -half_L, half_L)\n",
    "            z_i = z_i / half_L\n",
    "            z_q_list.append(z_i)\n",
    "        return torch.stack(z_q_list, dim=-1)\n",
    "\n",
    "    def _to_indices(self, z_q: torch.Tensor) -> torch.Tensor:\n",
    "        indices = torch.zeros(z_q.shape[:-1], dtype=torch.long, device=z_q.device)\n",
    "        for i in range(self.dim):\n",
    "            L = self._levels[i].long()\n",
    "            half_L = self._half_levels[i]\n",
    "            z_i = z_q[..., i]\n",
    "            level_idx = ((z_i * half_L) + half_L).round().long()\n",
    "            level_idx = torch.clamp(level_idx, 0, L - 1)\n",
    "            indices = indices + level_idx * self._basis[i]\n",
    "        return indices\n",
    "\n",
    "    def get_codebook_usage(self, indices: torch.Tensor) -> Tuple[float, float]:\n",
    "        flat_indices = indices.flatten()\n",
    "        counts = torch.bincount(flat_indices, minlength=self.codebook_size).float()\n",
    "        usage = (counts > 0).float().mean().item()\n",
    "        probs = counts / counts.sum()\n",
    "        probs = probs[probs > 0]\n",
    "        entropy = -(probs * torch.log(probs)).sum()\n",
    "        perplexity = torch.exp(entropy).item()\n",
    "        return usage, perplexity\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RFSQ (Residual FSQ with LayerNorm)\n",
    "# ============================================================================\n",
    "\n",
    "class InvertibleLayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm that stores statistics for exact inverse transformation.\"\"\"\n",
    "\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer('stored_mean', None, persistent=False)\n",
    "        self.register_buffer('stored_std', None, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        channels_last = x.shape[-1] == self.num_features\n",
    "        if channels_last:\n",
    "            self.stored_mean = x.mean(dim=(1, 2, 3), keepdim=True)\n",
    "            self.stored_std = x.std(dim=(1, 2, 3), keepdim=True) + self.eps\n",
    "            x_norm = (x - self.stored_mean) / self.stored_std\n",
    "            return x_norm * self.weight + self.bias\n",
    "        else:\n",
    "            self.stored_mean = x.mean(dim=(2, 3, 4), keepdim=True)\n",
    "            self.stored_std = x.std(dim=(2, 3, 4), keepdim=True) + self.eps\n",
    "            x_norm = (x - self.stored_mean) / self.stored_std\n",
    "            return x_norm * self.weight.view(1, -1, 1, 1, 1) + self.bias.view(1, -1, 1, 1, 1)\n",
    "\n",
    "    def inverse(self, x_norm: torch.Tensor) -> torch.Tensor:\n",
    "        if self.stored_mean is None or self.stored_std is None:\n",
    "            raise RuntimeError(\"Must call forward() before inverse()\")\n",
    "        channels_last = x_norm.shape[-1] == self.num_features\n",
    "        if channels_last:\n",
    "            x = (x_norm - self.bias) / self.weight\n",
    "            return x * self.stored_std + self.stored_mean\n",
    "        else:\n",
    "            x = (x_norm - self.bias.view(1, -1, 1, 1, 1)) / self.weight.view(1, -1, 1, 1, 1)\n",
    "            return x * self.stored_std + self.stored_mean\n",
    "\n",
    "\n",
    "class RFSQStage(nn.Module):\n",
    "    \"\"\"Single stage of Residual FSQ with LayerNorm conditioning.\"\"\"\n",
    "\n",
    "    def __init__(self, levels: List[int]):\n",
    "        super().__init__()\n",
    "        self.levels = levels\n",
    "        self.fsq = FSQ(levels)\n",
    "        self.layernorm = InvertibleLayerNorm(len(levels))\n",
    "\n",
    "    @property\n",
    "    def codebook_size(self) -> int:\n",
    "        return self.fsq.codebook_size\n",
    "\n",
    "    def forward(self, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        z_norm = self.layernorm(residual)\n",
    "        z_q_norm, indices = self.fsq(z_norm)\n",
    "        z_q = self.layernorm.inverse(z_q_norm)\n",
    "        new_residual = residual - z_q\n",
    "        return z_q, new_residual, indices\n",
    "\n",
    "\n",
    "class RFSQ(nn.Module):\n",
    "    \"\"\"Robust Residual FSQ with multiple stages.\"\"\"\n",
    "\n",
    "    def __init__(self, levels_per_stage: List[int], num_stages: int = 2):\n",
    "        super().__init__()\n",
    "        self.levels_per_stage = levels_per_stage\n",
    "        self.num_stages = num_stages\n",
    "        self.dim = len(levels_per_stage)\n",
    "\n",
    "        self.stages = nn.ModuleList([\n",
    "            RFSQStage(levels_per_stage) for _ in range(num_stages)\n",
    "        ])\n",
    "\n",
    "        codes_per_stage = int(np.prod(levels_per_stage))\n",
    "        self.codebook_size = codes_per_stage ** num_stages\n",
    "        self.codes_per_stage = codes_per_stage\n",
    "        self._usage_indices = []\n",
    "\n",
    "    @property\n",
    "    def num_codes(self) -> int:\n",
    "        return self.codebook_size\n",
    "\n",
    "    def reset_usage(self):\n",
    "        self._usage_indices = []\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        residual = z\n",
    "        z_q_sum = torch.zeros_like(z)\n",
    "        all_indices = []\n",
    "\n",
    "        for stage in self.stages:\n",
    "            z_q, residual, indices = stage(residual)\n",
    "            z_q_sum = z_q_sum + z_q\n",
    "            all_indices.append(indices)\n",
    "\n",
    "        self._usage_indices.append(all_indices)\n",
    "        return z_q_sum, all_indices\n",
    "\n",
    "    def get_usage_stats(self) -> Dict[str, Tuple[float, float]]:\n",
    "        if not self._usage_indices:\n",
    "            return {}\n",
    "        stats = {}\n",
    "        for stage_idx in range(self.num_stages):\n",
    "            all_stage_indices = torch.cat([\n",
    "                batch[stage_idx].flatten()\n",
    "                for batch in self._usage_indices\n",
    "            ])\n",
    "            usage, perplexity = self.stages[stage_idx].fsq.get_codebook_usage(all_stage_indices)\n",
    "            stats[f'stage{stage_idx}'] = (usage, perplexity)\n",
    "        return stats\n",
    "\n",
    "print(\"âœ“ FSQ and RFSQ modules defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Code - VQ-VAE v8-B Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock3D(nn.Module):\n",
    "    \"\"\"3D residual block with BatchNorm.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int = None):\n",
    "        super().__init__()\n",
    "        if out_channels is None:\n",
    "            out_channels = in_channels\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv3d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class EncoderV8B(nn.Module):\n",
    "    \"\"\"Encoder: 32x32x32 â†’ 16x16x16 latent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 40,\n",
    "        hidden_dim: int = 192,\n",
    "        rfsq_dim: int = 4,\n",
    "        num_resblocks_per_stage: int = 2,\n",
    "        num_resblocks_latent: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm3d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv3d(hidden_dim, hidden_dim, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(dropout)\n",
    "        )\n",
    "\n",
    "        self.stage_blocks = nn.Sequential(*[\n",
    "            ResidualBlock3D(hidden_dim)\n",
    "            for _ in range(num_resblocks_per_stage)\n",
    "        ])\n",
    "\n",
    "        self.latent_blocks = nn.Sequential(*[\n",
    "            ResidualBlock3D(hidden_dim)\n",
    "            for _ in range(num_resblocks_latent)\n",
    "        ])\n",
    "\n",
    "        self.latent_proj = nn.Conv3d(hidden_dim, rfsq_dim, 3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.initial(x)\n",
    "        x = self.downsample(x)\n",
    "        x = self.stage_blocks(x)\n",
    "        x = self.latent_blocks(x)\n",
    "        z_e = self.latent_proj(x)\n",
    "        return z_e\n",
    "\n",
    "\n",
    "class DecoderV8B(nn.Module):\n",
    "    \"\"\"Decoder: 16x16x16 latent â†’ 32x32x32 output.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rfsq_dim: int = 4,\n",
    "        hidden_dim: int = 192,\n",
    "        num_blocks: int = 3717,\n",
    "        num_resblocks_per_stage: int = 2,\n",
    "        num_resblocks_latent: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv3d(rfsq_dim, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm3d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.latent_blocks = nn.Sequential(*[\n",
    "            ResidualBlock3D(hidden_dim)\n",
    "            for _ in range(num_resblocks_latent)\n",
    "        ])\n",
    "\n",
    "        self.stage_blocks = nn.Sequential(*[\n",
    "            ResidualBlock3D(hidden_dim)\n",
    "            for _ in range(num_resblocks_per_stage)\n",
    "        ])\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose3d(hidden_dim, hidden_dim, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(dropout)\n",
    "        )\n",
    "\n",
    "        self.final = nn.Conv3d(hidden_dim, num_blocks, 3, padding=1)\n",
    "\n",
    "    def forward(self, z_q: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.initial(z_q)\n",
    "        x = self.latent_blocks(x)\n",
    "        x = self.stage_blocks(x)\n",
    "        x = self.upsample(x)\n",
    "        logits = self.final(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class VQVAEv8B(nn.Module):\n",
    "    \"\"\"VQ-VAE v8-B with 16Ã—16Ã—16 latent resolution.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 3717,\n",
    "        emb_dim: int = 40,\n",
    "        hidden_dim: int = 192,\n",
    "        rfsq_levels: List[int] = None,\n",
    "        num_stages: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        pretrained_embeddings: torch.Tensor = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if rfsq_levels is None:\n",
    "            rfsq_levels = [5, 5, 5, 5]\n",
    "\n",
    "        self.rfsq_dim = len(rfsq_levels)\n",
    "\n",
    "        self.block_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.block_emb.weight.data.copy_(pretrained_embeddings)\n",
    "            self.block_emb.weight.requires_grad = False\n",
    "\n",
    "        self.encoder = EncoderV8B(\n",
    "            in_channels=emb_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            rfsq_dim=self.rfsq_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.quantizer = RFSQ(\n",
    "            levels_per_stage=rfsq_levels,\n",
    "            num_stages=num_stages\n",
    "        )\n",
    "        self.decoder = DecoderV8B(\n",
    "            rfsq_dim=self.rfsq_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_blocks=vocab_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, block_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
    "        z_e = self.encode(block_ids)\n",
    "        z_q, indices = self.quantize(z_e)\n",
    "        logits = self.decode(z_q)\n",
    "        return logits, z_q, indices\n",
    "\n",
    "    def encode(self, block_ids: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.block_emb(block_ids)\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        z_e = self.encoder(x)\n",
    "        z_e = z_e.permute(0, 2, 3, 4, 1)\n",
    "        return z_e\n",
    "\n",
    "    def quantize(self, z_e: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        z_q, indices = self.quantizer(z_e)\n",
    "        return z_q, indices\n",
    "\n",
    "    def decode(self, z_q: torch.Tensor) -> torch.Tensor:\n",
    "        z_q = z_q.permute(0, 4, 1, 2, 3)\n",
    "        logits = self.decoder(z_q)\n",
    "        return logits\n",
    "\n",
    "print(\"âœ“ VQ-VAE v8-B architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Code - Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyWeightedLoss(nn.Module):\n",
    "    \"\"\"Frequency-weighted CE loss with BALANCED volume control.\n",
    "\n",
    "    This loss function ensures:\n",
    "    - volume_ratio approaches 1.0 (no over-prediction)\n",
    "    - recall stays high (no structure erasure)\n",
    "\n",
    "    The key: Volume loss and False Air loss work TOGETHER.\n",
    "    - Volume loss: \"don't predict too many blocks overall\"\n",
    "    - False air loss: \"don't erase existing structure blocks\"\n",
    "\n",
    "    Both are DIFFERENTIABLE through softmax!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        frequency_weights: torch.Tensor,\n",
    "        frequency_cap: float = 5.0,\n",
    "        volume_penalty_weight: float = 10.0,      # Prevent over-prediction\n",
    "        false_air_penalty_weight: float = 5.0,    # Protect recall\n",
    "        perceptual_weight: float = 0.1,\n",
    "        air_tokens: Optional[Set[int]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        clamped_weights = frequency_weights.clamp(max=frequency_cap)\n",
    "        self.register_buffer('freq_weights', clamped_weights)\n",
    "        self.volume_penalty_weight = volume_penalty_weight\n",
    "        self.false_air_penalty_weight = false_air_penalty_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        if air_tokens is None:\n",
    "            air_tokens = {102, 576, 3352}\n",
    "        self.air_tokens = list(air_tokens)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        z_q: torch.Tensor,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        vocab_size = logits.shape[1]\n",
    "\n",
    "        # 1. Frequency-weighted CE loss\n",
    "        logits_flat = logits.permute(0, 2, 3, 4, 1).reshape(-1, vocab_size)\n",
    "        target_flat = target.reshape(-1)\n",
    "        ce_loss = F.cross_entropy(logits_flat, target_flat, weight=self.freq_weights, reduction='mean')\n",
    "\n",
    "        # 2. DIFFERENTIABLE Volume Loss\n",
    "        # Get softmax probabilities (gradients flow through!)\n",
    "        probs = F.softmax(logits, dim=1)  # [B, vocab_size, H, W, D]\n",
    "\n",
    "        # Sum probability of predicting air tokens\n",
    "        air_prob = torch.zeros_like(probs[:, 0, :, :, :])  # [B, H, W, D]\n",
    "        for air_tok in self.air_tokens:\n",
    "            if air_tok < probs.shape[1]:\n",
    "                air_prob = air_prob + probs[:, air_tok, :, :, :]\n",
    "\n",
    "        # Soft volume = sum of (1 - air_probability)\n",
    "        non_air_prob = 1.0 - air_prob\n",
    "        pred_volume_soft = non_air_prob.sum()\n",
    "\n",
    "        # Ground truth volume\n",
    "        air_tokens_tensor = torch.tensor(self.air_tokens, device=target.device, dtype=target.dtype)\n",
    "        gt_is_air = torch.isin(target, air_tokens_tensor)\n",
    "        gt_volume = (~gt_is_air).float().sum()\n",
    "\n",
    "        # Volume ratio and loss (differentiable!)\n",
    "        volume_ratio_soft = pred_volume_soft / (gt_volume + 1e-6)\n",
    "        volume_loss = (volume_ratio_soft - 1.0) ** 2\n",
    "\n",
    "        # 3. FALSE AIR PENALTY - Protects recall!\n",
    "        # Penalize predicting air where ground truth has structure\n",
    "        gt_is_structure = ~gt_is_air\n",
    "        if gt_is_structure.any():\n",
    "            # High air_prob at structure locations = bad = high penalty\n",
    "            false_air_loss = air_prob[gt_is_structure].mean()\n",
    "        else:\n",
    "            false_air_loss = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "        # 4. Perceptual loss (spatial smoothness)\n",
    "        diff_h = (z_q[:, :, 1:, :, :] - z_q[:, :, :-1, :, :]).abs().mean()\n",
    "        diff_w = (z_q[:, :, :, 1:, :] - z_q[:, :, :, :-1, :]).abs().mean()\n",
    "        diff_d = (z_q[:, :, :, :, 1:] - z_q[:, :, :, :, :-1]).abs().mean()\n",
    "        perceptual_loss = (diff_h + diff_w + diff_d) / 3.0\n",
    "\n",
    "        # Combined loss\n",
    "        total_loss = (\n",
    "            ce_loss +\n",
    "            self.volume_penalty_weight * volume_loss +\n",
    "            self.false_air_penalty_weight * false_air_loss +\n",
    "            self.perceptual_weight * perceptual_loss\n",
    "        )\n",
    "\n",
    "        # Compute metrics for logging (non-differentiable)\n",
    "        with torch.no_grad():\n",
    "            pred_hard = torch.argmax(logits, dim=1)\n",
    "            pred_is_air_hard = torch.isin(pred_hard, air_tokens_tensor)\n",
    "            pred_volume_hard = (~pred_is_air_hard).float().sum()\n",
    "            volume_ratio_hard = pred_volume_hard / (gt_volume + 1e-6)\n",
    "\n",
    "            # Recall = structure preserved / total structure\n",
    "            structure_preserved = gt_is_structure & (~pred_is_air_hard)\n",
    "            recall = structure_preserved.float().sum() / (gt_is_structure.float().sum() + 1e-6)\n",
    "\n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'ce_loss': ce_loss.detach(),\n",
    "            'volume_loss': volume_loss.detach(),\n",
    "            'false_air_loss': false_air_loss.detach() if torch.is_tensor(false_air_loss) else false_air_loss,\n",
    "            'perceptual_loss': perceptual_loss.detach(),\n",
    "            'volume_ratio': volume_ratio_hard,\n",
    "            'recall': recall,\n",
    "        }\n",
    "\n",
    "\n",
    "def compute_frequency_weights(\n",
    "    block_ids: torch.Tensor,\n",
    "    vocab_size: int,\n",
    "    smoothing: float = 0.5,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute frequency-based weights.\"\"\"\n",
    "    counts = torch.bincount(block_ids.flatten(), minlength=vocab_size).clamp(min=1)\n",
    "    total = counts.sum()\n",
    "    weights = (total.float() / counts.float()) ** smoothing\n",
    "    return weights\n",
    "\n",
    "print(\"[OK] BALANCED loss function defined!\")\n",
    "print(\"  - Volume penalty (differentiable): prevents over-prediction\")\n",
    "print(\"  - False air penalty (differentiable): protects recall\")\n",
    "print(\"=\"*70)\n",
    "print(\"ALL MODEL CODE LOADED - Ready to train!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Data Paths ===\nDATA_DIR = f\"{DRIVE_BASE}/splits/train\"\nVAL_DIR = f\"{DRIVE_BASE}/splits/val\"\nVOCAB_PATH = f\"{DRIVE_BASE}/vocabulary/tok2block.json\"\nV3_EMBEDDINGS_PATH = f\"{DRIVE_BASE}/vocabulary/block_embeddings_v3.npy\"\n\n# Verify paths\nprint(\"Checking paths...\")\nfor name, path in [('DATA_DIR', DATA_DIR), ('VAL_DIR', VAL_DIR), \n                    ('VOCAB_PATH', VOCAB_PATH), ('V3_EMBEDDINGS_PATH', V3_EMBEDDINGS_PATH)]:\n    exists = Path(path).exists()\n    print(f\"  {name}: {'[OK]' if exists else '[NOT FOUND]'}\")\n    if not exists:\n        print(f\"    Path: {path}\")\n\n# === V8-B Architecture ===\nHIDDEN_DIM = 192\nRFSQ_LEVELS_PER_STAGE = [5, 5, 5, 5]\nNUM_STAGES = 2\nDROPOUT = 0.1\nNUM_RESBLOCKS_LATENT = 6\n\n# === Loss Weights (BALANCED) ===\nFREQUENCY_WEIGHT_CAP = 5.0\nVOLUME_PENALTY_WEIGHT = 10.0        # Prevent over-prediction\nFALSE_AIR_PENALTY_WEIGHT = 5.0      # Protect recall (CRITICAL!)\nPERCEPTUAL_WEIGHT = 0.1\n\n# === Training ===\nTOTAL_EPOCHS = 35\nBATCH_SIZE = 2\nBASE_LR = 2e-4\nUSE_AMP = True\nGRAD_ACCUM_STEPS = 8\nSEED = 42\nNUM_WORKERS = 2\n\nprint(f\"\\nConfiguration:\")\nprint(f\"  Latent: 16x16x16 (4,096 positions)\")\nprint(f\"  Hidden dim: {HIDDEN_DIM}\")\nprint(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS})\")\nprint(f\"  Epochs: {TOTAL_EPOCHS}\")\nprint(f\"  Learning rate: {BASE_LR}\")\nprint(f\"\\nLoss weights:\")\nprint(f\"  Volume penalty: {VOLUME_PENALTY_WEIGHT} (prevents over-prediction)\")\nprint(f\"  False air penalty: {FALSE_AIR_PENALTY_WEIGHT} (protects recall)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Vocabulary and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Find air tokens\n",
    "AIR_TOKENS: Set[int] = set()\n",
    "for tok, block in tok2block.items():\n",
    "    if 'air' in block.lower() and 'stair' not in block.lower():\n",
    "        AIR_TOKENS.add(tok)\n",
    "        print(f\"  Air token: {tok} = {block}\")\n",
    "\n",
    "AIR_TOKENS_TENSOR = torch.tensor(sorted(AIR_TOKENS), dtype=torch.long)\n",
    "\n",
    "# Load embeddings\n",
    "v3_embeddings = np.load(V3_EMBEDDINGS_PATH).astype(np.float32)\n",
    "EMBEDDING_DIM = v3_embeddings.shape[1]\n",
    "print(f\"\\nV3 embeddings: {v3_embeddings.shape} (dim={EMBEDDING_DIM})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Frequency Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing block frequencies from training data...\")\n",
    "print(\"(This may take a few minutes)\")\n",
    "\n",
    "all_block_ids = []\n",
    "train_files = sorted(Path(DATA_DIR).glob(\"*.h5\"))\n",
    "\n",
    "for h5_file in tqdm(train_files, desc=\"Scanning\"):\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        key = list(f.keys())[0]\n",
    "        structure = f[key][:].flatten()\n",
    "        all_block_ids.append(torch.from_numpy(structure).long())\n",
    "\n",
    "all_block_ids = torch.cat(all_block_ids)\n",
    "print(f\"\\nTotal blocks scanned: {len(all_block_ids):,}\")\n",
    "\n",
    "FREQUENCY_WEIGHT_TENSOR = compute_frequency_weights(\n",
    "    all_block_ids,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    smoothing=0.5\n",
    ").clamp(max=FREQUENCY_WEIGHT_CAP)\n",
    "\n",
    "print(f\"Frequency weights computed (cap={FREQUENCY_WEIGHT_CAP}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAEDataset(Dataset):\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        if not self.h5_files:\n",
    "            raise ValueError(f\"No H5 files in {data_dir}\")\n",
    "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h5_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_files[idx], 'r') as f:\n",
    "            key = list(f.keys())[0]\n",
    "            structure = f[key][:].astype(np.int64)\n",
    "        return torch.from_numpy(structure).long()\n",
    "\n",
    "train_dataset = VQVAEDataset(DATA_DIR)\n",
    "val_dataset = VQVAEDataset(VAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "model = VQVAEv8B(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    rfsq_levels=RFSQ_LEVELS_PER_STAGE,\n",
    "    num_stages=NUM_STAGES,\n",
    "    dropout=DROPOUT,\n",
    "    pretrained_embeddings=torch.from_numpy(v3_embeddings),\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {total_params:,} parameters\")\n",
    "\n",
    "criterion = FrequencyWeightedLoss(\n",
    "    frequency_weights=FREQUENCY_WEIGHT_TENSOR,\n",
    "    frequency_cap=FREQUENCY_WEIGHT_CAP,\n",
    "    volume_penalty_weight=VOLUME_PENALTY_WEIGHT,\n",
    "    false_air_penalty_weight=FALSE_AIR_PENALTY_WEIGHT,\n",
    "    perceptual_weight=PERCEPTUAL_WEIGHT,\n",
    "    air_tokens=AIR_TOKENS,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS, eta_min=1e-5)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "print(\"Model, criterion, optimizer created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_metrics(logits, targets, air_tokens):\n    \"\"\"Compute key metrics.\"\"\"\n    device = logits.device\n    B, C, X, Y, Z = logits.shape\n    \n    logits_flat = logits.permute(0, 2, 3, 4, 1).reshape(-1, C)\n    targets_flat = targets.view(-1)\n    \n    air_dev = air_tokens.to(device)\n    is_air = torch.isin(targets_flat, air_dev)\n    is_building = ~is_air\n    \n    preds = logits_flat.argmax(dim=1)\n    is_air_pred = torch.isin(preds, air_dev)\n    correct = (preds == targets_flat).float()\n    \n    metrics = {}\n    metrics['overall_acc'] = correct.mean()\n    \n    if is_building.any():\n        metrics['building_acc'] = correct[is_building].mean()\n        metrics['building_recall'] = (is_building & ~is_air_pred).sum().float() / is_building.sum()\n    else:\n        metrics['building_acc'] = torch.tensor(0.0, device=device)\n        metrics['building_recall'] = torch.tensor(0.0, device=device)\n    \n    is_struct = ~is_air\n    metrics['struct_recall'] = (is_struct & ~is_air_pred).sum().float() / is_struct.sum() if is_struct.any() else torch.tensor(0.0, device=device)\n    \n    pred_building = ~is_air_pred\n    pred_vol = pred_building.sum().float()\n    orig_vol = is_struct.sum().float()\n    metrics['volume_ratio'] = pred_vol / orig_vol if orig_vol > 0 else torch.tensor(1.0, device=device)\n    \n    return metrics\n\n\ndef train_epoch(model, criterion, loader, optimizer, scaler, device, air_tokens):\n    model.train()\n    model.quantizer.reset_usage()\n    \n    # Include ALL metrics from loss function\n    metrics_sum = {\n        'loss': 0.0, 'ce_loss': 0.0, 'volume_loss': 0.0, \n        'false_air_loss': 0.0, 'perceptual_loss': 0.0,\n        'overall_acc': 0.0, 'building_acc': 0.0, 'building_recall': 0.0,\n        'struct_recall': 0.0, 'volume_ratio': 0.0, 'recall': 0.0\n    }\n    n = 0\n    \n    optimizer.zero_grad()\n    \n    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n        batch = batch.to(device)\n        \n        with torch.amp.autocast('cuda', enabled=USE_AMP):\n            logits, z_q, indices = model(batch)\n            loss_dict = criterion(logits, batch, z_q)\n            loss = loss_dict['loss'] / GRAD_ACCUM_STEPS\n        \n        scaler.scale(loss).backward()\n        \n        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        with torch.no_grad():\n            # Collect loss metrics\n            for k in ['loss', 'ce_loss', 'volume_loss', 'false_air_loss', 'perceptual_loss', 'volume_ratio', 'recall']:\n                if k in loss_dict:\n                    val = loss_dict[k]\n                    metrics_sum[k] += val.item() if torch.is_tensor(val) else val\n            \n            batch_metrics = compute_metrics(logits, batch, air_tokens)\n            for k, v in batch_metrics.items():\n                metrics_sum[k] += v.item()\n        \n        n += 1\n    \n    metrics = {k: v / n for k, v in metrics_sum.items()}\n    \n    stage_stats = model.quantizer.get_usage_stats()\n    for stage_name, (usage, perp) in stage_stats.items():\n        metrics[f'{stage_name}_usage'] = usage\n        metrics[f'{stage_name}_perplexity'] = perp\n    \n    return metrics\n\n\n@torch.no_grad()\ndef validate(model, criterion, loader, device, air_tokens):\n    model.eval()\n    model.quantizer.reset_usage()\n    \n    # Include ALL metrics from loss function\n    metrics_sum = {\n        'loss': 0.0, 'ce_loss': 0.0, 'volume_loss': 0.0,\n        'false_air_loss': 0.0, 'perceptual_loss': 0.0,\n        'overall_acc': 0.0, 'building_acc': 0.0, 'building_recall': 0.0,\n        'struct_recall': 0.0, 'volume_ratio': 0.0, 'recall': 0.0\n    }\n    n = 0\n    \n    for batch in tqdm(loader, desc=\"Val\", leave=False):\n        batch = batch.to(device)\n        \n        with torch.amp.autocast('cuda', enabled=USE_AMP):\n            logits, z_q, indices = model(batch)\n            loss_dict = criterion(logits, batch, z_q)\n        \n        # Collect loss metrics\n        for k in ['loss', 'ce_loss', 'volume_loss', 'false_air_loss', 'perceptual_loss', 'volume_ratio', 'recall']:\n            if k in loss_dict:\n                val = loss_dict[k]\n                metrics_sum[k] += val.item() if torch.is_tensor(val) else val\n        \n        batch_metrics = compute_metrics(logits, batch, air_tokens)\n        for k, v in batch_metrics.items():\n            metrics_sum[k] += v.item()\n        \n        n += 1\n    \n    metrics = {k: v / n for k, v in metrics_sum.items()}\n    \n    stage_stats = model.quantizer.get_usage_stats()\n    for stage_name, (usage, perp) in stage_stats.items():\n        metrics[f'{stage_name}_usage'] = usage\n        metrics[f'{stage_name}_perplexity'] = perp\n    \n    return metrics\n\nprint(\"Training functions defined\")\nprint(\"  - Tracking: volume_ratio, recall, false_air_loss\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"VQ-VAE V8-B TRAINING - BALANCED LOSS\")\nprint(\"=\"*70)\nprint(f\"Target: 60-65% building accuracy\")\nprint(f\"Target: volume_ratio ~1.0x (not 2x!)\")\nprint(f\"Target: recall >90% (preserve structures)\")\nprint()\n\nhistory = {\n    'train_loss': [], 'train_building_acc': [], 'train_vol_ratio': [], 'train_recall': [],\n    'val_loss': [], 'val_building_acc': [], 'val_vol_ratio': [], 'val_recall': [],\n    'train_stage0_perplexity': [], 'val_stage0_perplexity': [],\n    'learning_rate': [],\n}\n\nbest_building_acc = 0\nbest_epoch = 0\nstart_time = time.time()\n\nfor epoch in range(TOTAL_EPOCHS):\n    train_m = train_epoch(model, criterion, train_loader, optimizer, scaler, device, AIR_TOKENS_TENSOR)\n    val_m = validate(model, criterion, val_loader, device, AIR_TOKENS_TENSOR)\n    \n    scheduler.step()\n    current_lr = scheduler.get_last_lr()[0]\n    \n    # Record metrics\n    history['train_loss'].append(train_m['loss'])\n    history['train_building_acc'].append(train_m['building_acc'])\n    history['train_vol_ratio'].append(train_m['volume_ratio'])\n    history['train_recall'].append(train_m.get('recall', train_m.get('struct_recall', 0)))\n    history['val_loss'].append(val_m['loss'])\n    history['val_building_acc'].append(val_m['building_acc'])\n    history['val_vol_ratio'].append(val_m['volume_ratio'])\n    history['val_recall'].append(val_m.get('recall', val_m.get('struct_recall', 0)))\n    history['train_stage0_perplexity'].append(train_m.get('stage0_perplexity', 0))\n    history['val_stage0_perplexity'].append(val_m.get('stage0_perplexity', 0))\n    history['learning_rate'].append(current_lr)\n    \n    # Save best model\n    if val_m['building_acc'] > best_building_acc:\n        best_building_acc = val_m['building_acc']\n        best_epoch = epoch + 1\n        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/vqvae_v8b_best.pt\")\n    \n    # Save checkpoint every 5 epochs\n    if (epoch + 1) % 5 == 0:\n        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/vqvae_v8b_epoch{epoch+1}.pt\")\n    \n    # Print with recall to monitor shape preservation\n    val_recall = val_m.get('recall', val_m.get('struct_recall', 0))\n    print(f\"Epoch {epoch+1:2d} | \"\n          f\"Build: {train_m['building_acc']:.1%}/{val_m['building_acc']:.1%} | \"\n          f\"Vol: {val_m['volume_ratio']:.2f}x | \"\n          f\"Recall: {val_recall:.1%} | \"\n          f\"LR: {current_lr:.2e}\")\n\ntrain_time = time.time() - start_time\nprint(f\"\\nTraining complete in {train_time/60:.1f} minutes\")\nprint(f\"Best val building accuracy: {best_building_acc:.1%} at epoch {best_epoch}\")\nprint(f\"Final volume ratio: {history['val_vol_ratio'][-1]:.2f}x\")\nprint(f\"Final recall: {history['val_recall'][-1]:.1%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "epochs = range(1, TOTAL_EPOCHS + 1)\n",
    "\n",
    "# Building accuracy\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs, history['train_building_acc'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_building_acc'], 'r--', label='Val')\n",
    "ax.axhline(y=0.492, color='g', linestyle=':', alpha=0.5, label='v6-freq (49.2%)')\n",
    "ax.axhline(y=0.60, color='orange', linestyle='--', alpha=0.5, label='Target (60%)')\n",
    "ax.set_title('Building Accuracy', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Volume ratio\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs, history['train_vol_ratio'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_vol_ratio'], 'r--', label='Val')\n",
    "ax.axhline(y=1.0, color='g', linestyle='--', alpha=0.5, label='Target (1.0x)')\n",
    "ax.axhline(y=1.68, color='orange', linestyle=':', alpha=0.5, label='v6-freq (1.68x)')\n",
    "ax.set_title('Volume Ratio', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs, history['train_loss'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_loss'], 'r--', label='Val')\n",
    "ax.set_title('Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# RFSQ perplexity\n",
    "ax = axes[1, 1]\n",
    "ax.plot(epochs, history['train_stage0_perplexity'], 'b-', label='Train')\n",
    "ax.plot(epochs, history['val_stage0_perplexity'], 'r--', label='Val')\n",
    "ax.set_title('RFSQ Stage 0 Perplexity')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/vqvae_v8b_training.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best building accuracy: {best_building_acc:.1%} (epoch {best_epoch})\")\n",
    "print(f\"Final volume ratio: {history['val_vol_ratio'][-1]:.2f}x\")\n",
    "print(f\"Training time: {train_time/60:.1f} minutes\")\n",
    "print()\n",
    "if best_building_acc >= 0.60 and history['val_vol_ratio'][-1] <= 1.3:\n",
    "    print(\"[SUCCESS] Stage 1 targets met! Ready for Stage 2 (v8-C)\")\n",
    "elif best_building_acc >= 0.55:\n",
    "    print(\"[CLOSE] Consider Stage 2 with caution\")\n",
    "else:\n",
    "    print(\"[BELOW TARGET] Analyze before Stage 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'config': {\n",
    "        'version': 'v8-B',\n",
    "        'latent_resolution': '16x16x16',\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'total_epochs': TOTAL_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'base_lr': BASE_LR,\n",
    "        'seed': SEED,\n",
    "    },\n",
    "    'results': {\n",
    "        'best_building_acc': float(best_building_acc),\n",
    "        'best_epoch': best_epoch,\n",
    "        'final_building_acc': float(history['val_building_acc'][-1]),\n",
    "        'final_volume_ratio': float(history['val_vol_ratio'][-1]),\n",
    "        'training_time_min': float(train_time / 60),\n",
    "        'target_60pct_met': bool(best_building_acc >= 0.60),\n",
    "        'volume_target_met': bool(history['val_vol_ratio'][-1] <= 1.3),\n",
    "    },\n",
    "    'history': {k: [float(x) for x in v] for k, v in history.items()},\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/vqvae_v8b_results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "torch.save(model.state_dict(), f\"{OUTPUT_DIR}/vqvae_v8b_final.pt\")\n",
    "\n",
    "print(\"\\nResults saved to:\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v8b_results.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v8b_best.pt\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v8b_final.pt\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v8b_training.png\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}