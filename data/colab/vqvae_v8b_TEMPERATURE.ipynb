{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE v8-B Training - TEMPERATURE-SCALED Volume Control\\n",
    "\\n",
    "**FIX FOR 2.0x VOLUME RATIO**\\n",
    "\\n",
    "## Why Direct Logit Failed\\n",
    "\\n",
    "The direct logit approach optimized margin violations to ~0, but:\\n",
    "- 10% of GT air locations still had small violations\\n",
    "- This 10% error = 2.0x volume ratio\\n",
    "- CE loss (0.45) dominated volume loss (0.002)\\n",
    "\\n",
    "## New Approach: Temperature-Scaled Softmax\\n",
    "\\n",
    "```python\\n",
    "soft_probs = F.softmax(logits / 0.1, dim=1)  # temp=0.1 = near-argmax\\n",
    "soft_volume_ratio = soft_structure_count / gt_structure_count\\n",
    "volume_loss = (soft_volume_ratio - 1.0) ** 2  # Direct ratio penalty!\\n",
    "```\\n",
    "\\n",
    "This loss does NOT diminish to zero when volume ratio is wrong!\\n",
    "\\n",
    "## Expected Results\\n",
    "\\n",
    "| Metric | Previous (stuck) | Expected Now |\\n",
    "|--------|-----------------|--------------|\\n",
    "| Volume Ratio | 2.0x (all epochs) | **~1.0-1.3x** |\\n",
    "| Building Acc | 80% (inflated) | **55-65%** (real) |\\n",
    "| Recall | 99.7% | **>90%** |\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (uncomment for Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths - CHANGE THIS to match your Google Drive structure\n",
    "DRIVE_BASE = '/content/drive/MyDrive/minecraft_ai'\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/minecraft_ai/vqvae_v8b'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Data directory: {DRIVE_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Set, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Code - FSQ and RFSQ Quantization\n",
    "\n",
    "All model code is included inline - no external imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FSQ (Finite Scalar Quantization)\n",
    "# ============================================================================\n",
    "\n",
    "class FSQ(nn.Module):\n",
    "    \"\"\"Finite Scalar Quantization.\"\"\"\n",
    "\n",
    "    def __init__(self, levels: List[int], eps: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.levels = levels\n",
    "        self.dim = len(levels)\n",
    "        self.eps = eps\n",
    "        self.codebook_size = int(np.prod(levels))\n",
    "\n",
    "        self.register_buffer('_levels', torch.tensor(levels, dtype=torch.float32))\n",
    "\n",
    "        basis = []\n",
    "        acc = 1\n",
    "        for L in reversed(levels):\n",
    "            basis.append(acc)\n",
    "            acc *= L\n",
    "        self.register_buffer('_basis', torch.tensor(list(reversed(basis)), dtype=torch.long))\n",
    "\n",
    "        half_levels = [(L - 1) / 2 for L in levels]\n",
    "        self.register_buffer('_half_levels', torch.tensor(half_levels, dtype=torch.float32))\n",
    "\n",
    "    @property\n",
    "    def num_codes(self) -> int:\n",
    "        return self.codebook_size\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        z_bounded = torch.tanh(z)\n",
    "        z_q = self._quantize(z_bounded)\n",
    "        z_q = z_bounded + (z_q - z_bounded).detach()  # Straight-through\n",
    "        indices = self._to_indices(z_q)\n",
    "        return z_q, indices\n",
    "\n",
    "    def _quantize(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        z_q_list = []\n",
    "        for i in range(self.dim):\n",
    "            L = self._levels[i]\n",
    "            half_L = self._half_levels[i]\n",
    "            z_i = z[..., i]\n",
    "            z_i = z_i * half_L\n",
    "            z_i = torch.round(z_i)\n",
    "            z_i = torch.clamp(z_i, -half_L, half_L)\n",
    "            z_i = z_i / half_L\n",
    "            z_q_list.append(z_i)\n",
    "        return torch.stack(z_q_list, dim=-1)\n",
    "\n",
    "    def _to_indices(self, z_q: torch.Tensor) -> torch.Tensor:\n",
    "        indices = torch.zeros(z_q.shape[:-1], dtype=torch.long, device=z_q.device)\n",
    "        for i in range(self.dim):\n",
    "            L = self._levels[i].long()\n",
    "            half_L = self._half_levels[i]\n",
    "            z_i = z_q[..., i]\n",
    "            level_idx = ((z_i * half_L) + half_L).round().long()\n",
    "            level_idx = torch.clamp(level_idx, 0, L - 1)\n",
    "            indices = indices + level_idx * self._basis[i]\n",
    "        return indices\n",
    "\n",
    "    def get_codebook_usage(self, indices: torch.Tensor) -> Tuple[float, float]:\n",
    "        flat_indices = indices.flatten()\n",
    "        counts = torch.bincount(flat_indices, minlength=self.codebook_size).float()\n",
    "        usage = (counts > 0).float().mean().item()\n",
    "        probs = counts / counts.sum()\n",
    "        probs = probs[probs > 0]\n",
    "        entropy = -(probs * torch.log(probs)).sum()\n",
    "        perplexity = torch.exp(entropy).item()\n",
    "        return usage, perplexity\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RFSQ (Residual FSQ with LayerNorm)\n",
    "# ============================================================================\n",
    "\n",
    "class InvertibleLayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm that stores statistics for exact inverse transformation.\"\"\"\n",
    "\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer('stored_mean', None, persistent=False)\n",
    "        self.register_buffer('stored_std', None, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        channels_last = x.shape[-1] == self.num_features\n",
    "        if channels_last:\n",
    "            self.stored_mean = x.mean(dim=(1, 2, 3), keepdim=True)\n",
    "            self.stored_std = x.std(dim=(1, 2, 3), keepdim=True) + self.eps\n",
    "            x_norm = (x - self.stored_mean) / self.stored_std\n",
    "            return x_norm * self.weight + self.bias\n",
    "        else:\n",
    "            self.stored_mean = x.mean(dim=(2, 3, 4), keepdim=True)\n",
    "            self.stored_std = x.std(dim=(2, 3, 4), keepdim=True) + self.eps\n",
    "            x_norm = (x - self.stored_mean) / self.stored_std\n",
    "            return x_norm * self.weight.view(1, -1, 1, 1, 1) + self.bias.view(1, -1, 1, 1, 1)\n",
    "\n",
    "    def inverse(self, x_norm: torch.Tensor) -> torch.Tensor:\n",
    "        if self.stored_mean is None or self.stored_std is None:\n",
    "            raise RuntimeError(\"Must call forward() before inverse()\")\n",
    "        channels_last = x_norm.shape[-1] == self.num_features\n",
    "        if channels_last:\n",
    "            x = (x_norm - self.bias) / self.weight\n",
    "            return x * self.stored_std + self.stored_mean\n",
    "        else:\n",
    "            x = (x_norm - self.bias.view(1, -1, 1, 1, 1)) / self.weight.view(1, -1, 1, 1, 1)\n",
    "            return x * self.stored_std + self.stored_mean\n",
    "\n",
    "\n",
    "class RFSQStage(nn.Module):\n",
    "    \"\"\"Single stage of Residual FSQ with LayerNorm conditioning.\"\"\"\n",
    "\n",
    "    def __init__(self, levels: List[int]):\n",
    "        super().__init__()\n",
    "        self.levels = levels\n",
    "        self.fsq = FSQ(levels)\n",
    "        self.layernorm = InvertibleLayerNorm(len(levels))\n",
    "\n",
    "    @property\n",
    "    def codebook_size(self) -> int:\n",
    "        return self.fsq.codebook_size\n",
    "\n",
    "    def forward(self, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        z_norm = self.layernorm(residual)\n",
    "        z_q_norm, indices = self.fsq(z_norm)\n",
    "        z_q = self.layernorm.inverse(z_q_norm)\n",
    "        new_residual = residual - z_q\n",
    "        return z_q, new_residual, indices\n",
    "\n",
    "\n",
    "class RFSQ(nn.Module):\n",
    "    \"\"\"Robust Residual FSQ with multiple stages.\"\"\"\n",
    "\n",
    "    def __init__(self, levels_per_stage: List[int], num_stages: int = 2):\n",
    "        super().__init__()\n",
    "        self.levels_per_stage = levels_per_stage\n",
    "        self.num_stages = num_stages\n",
    "        self.dim = len(levels_per_stage)\n",
    "\n",
    "        self.stages = nn.ModuleList([\n",
    "            RFSQStage(levels_per_stage) for _ in range(num_stages)\n",
    "        ])\n",
    "\n",
    "        codes_per_stage = int(np.prod(levels_per_stage))\n",
    "        self.codebook_size = codes_per_stage ** num_stages\n",
    "        self.codes_per_stage = codes_per_stage\n",
    "        self._usage_indices = []\n",
    "\n",
    "    @property\n",
    "    def num_codes(self) -> int:\n",
    "        return self.codebook_size\n",
    "\n",
    "    def reset_usage(self):\n",
    "        self._usage_indices = []\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        residual = z\n",
    "        z_q_sum = torch.zeros_like(z)\n",
    "        all_indices = []\n",
    "\n",
    "        for stage in self.stages:\n",
    "            z_q, residual, indices = stage(residual)\n",
    "            z_q_sum = z_q_sum + z_q\n",
    "            all_indices.append(indices)\n",
    "\n",
    "        self._usage_indices.append(all_indices)\n",
    "        return z_q_sum, all_indices\n",
    "\n",
    "    def get_usage_stats(self) -> Dict[str, Tuple[float, float]]:\n",
    "        if not self._usage_indices:\n",
    "            return {}\n",
    "        stats = {}\n",
    "        for stage_idx in range(self.num_stages):\n",
    "            all_stage_indices = torch.cat([\n",
    "                batch[stage_idx].flatten()\n",
    "                for batch in self._usage_indices\n",
    "            ])\n",
    "            usage, perplexity = self.stages[stage_idx].fsq.get_codebook_usage(all_stage_indices)\n",
    "            stats[f'stage{stage_idx}'] = (usage, perplexity)\n",
    "        return stats\n",
    "\n",
    "print(\"âœ“ FSQ and RFSQ modules defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Code - VQ-VAE v8-B Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock3D(nn.Module):\n",
    "    \"\"\"3D residual block with BatchNorm.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int = None):\n",
    "        super().__init__()\n",
    "        if out_channels is None:\n",
    "            out_channels = in_channels\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv3d(in_channels, out_channels, 1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class EncoderV8B(nn.Module):\n",
    "    \"\"\"Encoder: 32x32x32 â†’ 16x16x16 latent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 40,\n",
    "        hidden_dim: int = 192,\n",
    "        rfsq_dim: int = 4,\n",
    "        num_resblocks_per_stage: int = 2,\n",
    "        num_resblocks_latent: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm3d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv3d(hidden_dim, hidden_dim, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(dropout)\n",
    "        )\n",
    "\n",
    "        self.stage_blocks = nn.Sequential(*[\n",
    "            ResidualBlock3D(hidden_dim)\n",
    "            for _ in range(num_resblocks_per_stage)\n",
    "        ])\n",
    "\n",
    "        self.latent_blocks = nn.Sequential(*[\n",
    "            ResidualBlock3D(hidden_dim)\n",
    "            for _ in range(num_resblocks_latent)\n",
    "        ])\n",
    "\n",
    "        self.latent_proj = nn.Conv3d(hidden_dim, rfsq_dim, 3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.initial(x)\n",
    "        x = self.downsample(x)\n",
    "        x = self.stage_blocks(x)\n",
    "        x = self.latent_blocks(x)\n",
    "        z_e = self.latent_proj(x)\n",
    "        return z_e\n",
    "\n",
    "\n",
    "class DecoderV8B(nn.Module):\n",
    "    \"\"\"Decoder: 16x16x16 latent â†’ 32x32x32 output.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rfsq_dim: int = 4,\n",
    "        hidden_dim: int = 192,\n",
    "        num_blocks: int = 3717,\n",
    "        num_resblocks_per_stage: int = 2,\n",
    "        num_resblocks_latent: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv3d(rfsq_dim, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm3d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.latent_blocks = nn.Sequential(*[\n",
    "            ResidualBlock3D(hidden_dim)\n",
    "            for _ in range(num_resblocks_latent)\n",
    "        ])\n",
    "\n",
    "        self.stage_blocks = nn.Sequential(*[\n",
    "            ResidualBlock3D(hidden_dim)\n",
    "            for _ in range(num_resblocks_per_stage)\n",
    "        ])\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose3d(hidden_dim, hidden_dim, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(dropout)\n",
    "        )\n",
    "\n",
    "        self.final = nn.Conv3d(hidden_dim, num_blocks, 3, padding=1)\n",
    "\n",
    "    def forward(self, z_q: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.initial(z_q)\n",
    "        x = self.latent_blocks(x)\n",
    "        x = self.stage_blocks(x)\n",
    "        x = self.upsample(x)\n",
    "        logits = self.final(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class VQVAEv8B(nn.Module):\n",
    "    \"\"\"VQ-VAE v8-B with 16Ã—16Ã—16 latent resolution.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 3717,\n",
    "        emb_dim: int = 40,\n",
    "        hidden_dim: int = 192,\n",
    "        rfsq_levels: List[int] = None,\n",
    "        num_stages: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        pretrained_embeddings: torch.Tensor = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if rfsq_levels is None:\n",
    "            rfsq_levels = [5, 5, 5, 5]\n",
    "\n",
    "        self.rfsq_dim = len(rfsq_levels)\n",
    "\n",
    "        self.block_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.block_emb.weight.data.copy_(pretrained_embeddings)\n",
    "            self.block_emb.weight.requires_grad = False\n",
    "\n",
    "        self.encoder = EncoderV8B(\n",
    "            in_channels=emb_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            rfsq_dim=self.rfsq_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.quantizer = RFSQ(\n",
    "            levels_per_stage=rfsq_levels,\n",
    "            num_stages=num_stages\n",
    "        )\n",
    "        self.decoder = DecoderV8B(\n",
    "            rfsq_dim=self.rfsq_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_blocks=vocab_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, block_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
    "        z_e = self.encode(block_ids)\n",
    "        z_q, indices = self.quantize(z_e)\n",
    "        logits = self.decode(z_q)\n",
    "        return logits, z_q, indices\n",
    "\n",
    "    def encode(self, block_ids: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.block_emb(block_ids)\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        z_e = self.encoder(x)\n",
    "        z_e = z_e.permute(0, 2, 3, 4, 1)\n",
    "        return z_e\n",
    "\n",
    "    def quantize(self, z_e: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        z_q, indices = self.quantizer(z_e)\n",
    "        return z_q, indices\n",
    "\n",
    "    def decode(self, z_q: torch.Tensor) -> torch.Tensor:\n",
    "        z_q = z_q.permute(0, 4, 1, 2, 3)\n",
    "        logits = self.decoder(z_q)\n",
    "        return logits\n",
    "\n",
    "print(\"âœ“ VQ-VAE v8-B architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Code - Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyWeightedLoss(nn.Module):\n    \"\"\"Frequency-weighted CE loss with TEMPERATURE-SCALED volume control.\n\n    THE PROBLEM WITH DIRECT LOGIT APPROACH:\n    - Mean margin violation goes to ~0, but volume ratio stays at 2.0x\n    - 10% of GT air locations have small violations = 2.0x volume ratio\n    - CE loss (0.45) dominates volume loss (0.002)\n\n    THE FIX - TEMPERATURE-SCALED SOFTMAX:\n    - Use very low temperature (0.1) to approximate argmax\n    - Softmax(logits / 0.1) is nearly one-hot but DIFFERENTIABLE\n    - Volume loss = (soft_volume_ratio - 1.0)^2\n    - This loss does NOT go to zero when volume ratio is wrong!\n    \"\"\"\n\n    def __init__(\n        self,\n        frequency_weights: torch.Tensor,\n        frequency_cap: float = 5.0,\n        volume_penalty_weight: float = 50.0,   # Increased from 10!\n        false_air_penalty_weight: float = 25.0, # Increased from 5!\n        perceptual_weight: float = 0.1,\n        temperature: float = 0.1,  # NEW: Low temp = near-argmax\n        air_tokens: Optional[Set[int]] = None,\n    ):\n        super().__init__()\n        clamped_weights = frequency_weights.clamp(max=frequency_cap)\n        self.register_buffer('freq_weights', clamped_weights)\n        self.volume_penalty_weight = volume_penalty_weight\n        self.false_air_penalty_weight = false_air_penalty_weight\n        self.perceptual_weight = perceptual_weight\n        self.temperature = temperature\n        if air_tokens is None:\n            air_tokens = {102, 576, 3352}\n        self.air_tokens = list(air_tokens)\n\n    def forward(\n        self,\n        logits: torch.Tensor,\n        target: torch.Tensor,\n        z_q: torch.Tensor,\n    ) -> Dict[str, torch.Tensor]:\n        vocab_size = logits.shape[1]\n\n        # 1. Frequency-weighted CE loss\n        logits_flat = logits.permute(0, 2, 3, 4, 1).reshape(-1, vocab_size)\n        target_flat = target.reshape(-1)\n        ce_loss = F.cross_entropy(logits_flat, target_flat, weight=self.freq_weights, reduction='mean')\n\n        # 2. TEMPERATURE-SCALED VOLUME CONTROL\n        # Low temperature softmax approximates argmax but keeps gradients!\n        # temp=0.1 means softmax is nearly one-hot\n        scaled_logits = logits / self.temperature\n        soft_probs = F.softmax(scaled_logits, dim=1)  # [B, vocab_size, H, W, D]\n\n        # Sum probability of predicting air tokens\n        air_prob = torch.zeros_like(soft_probs[:, 0, :, :, :])  # [B, H, W, D]\n        for air_tok in self.air_tokens:\n            if air_tok < soft_probs.shape[1]:\n                air_prob = air_prob + soft_probs[:, air_tok, :, :, :]\n\n        # Soft structure probability (1 - air_prob)\n        structure_prob = 1.0 - air_prob\n\n        # Ground truth masks\n        air_tokens_tensor = torch.tensor(self.air_tokens, device=target.device, dtype=target.dtype)\n        gt_is_air = torch.isin(target, air_tokens_tensor)\n        gt_is_structure = ~gt_is_air\n\n        # Soft volumes\n        soft_pred_volume = structure_prob.sum()\n        gt_volume = gt_is_structure.float().sum()\n\n        # Volume ratio loss - DIRECTLY penalizes wrong ratio!\n        soft_volume_ratio = soft_pred_volume / (gt_volume + 1e-6)\n        volume_loss = (soft_volume_ratio - 1.0) ** 2\n\n        # 3. FALSE AIR PENALTY - penalize structure_prob at GT air locations\n        # If structure_prob is high at GT air locations, that's bad\n        if gt_is_air.any():\n            # Mean structure probability at GT air locations\n            # Should be LOW (close to 0)\n            false_structure_at_air = structure_prob[gt_is_air].mean()\n            false_air_loss = false_structure_at_air\n        else:\n            false_air_loss = torch.tensor(0.0, device=logits.device)\n\n        # 4. STRUCTURE PRESERVATION - penalize air_prob at GT structure locations\n        # If air_prob is high at GT structure locations, that's bad\n        if gt_is_structure.any():\n            # Mean air probability at GT structure locations\n            # Should be LOW (close to 0)\n            false_air_at_structure = air_prob[gt_is_structure].mean()\n            structure_loss = false_air_at_structure\n        else:\n            structure_loss = torch.tensor(0.0, device=logits.device)\n\n        # 5. Perceptual loss (spatial smoothness)\n        diff_h = (z_q[:, :, 1:, :, :] - z_q[:, :, :-1, :, :]).abs().mean()\n        diff_w = (z_q[:, :, :, 1:, :] - z_q[:, :, :, :-1, :]).abs().mean()\n        diff_d = (z_q[:, :, :, :, 1:] - z_q[:, :, :, :, :-1]).abs().mean()\n        perceptual_loss = (diff_h + diff_w + diff_d) / 3.0\n\n        # Combined loss\n        # false_air_loss: don't predict structure at GT air\n        # structure_loss: don't predict air at GT structure\n        total_loss = (\n            ce_loss +\n            self.volume_penalty_weight * volume_loss +\n            self.false_air_penalty_weight * false_air_loss +\n            self.false_air_penalty_weight * structure_loss +  # Same weight\n            self.perceptual_weight * perceptual_loss\n        )\n\n        # Compute hard metrics for logging (non-differentiable)\n        with torch.no_grad():\n            pred_hard = torch.argmax(logits, dim=1)\n            pred_is_air_hard = torch.isin(pred_hard, air_tokens_tensor)\n            pred_volume_hard = (~pred_is_air_hard).float().sum()\n            volume_ratio_hard = pred_volume_hard / (gt_volume + 1e-6)\n\n            # Recall = structure voxels preserved / total structure voxels\n            structure_preserved = gt_is_structure & (~pred_is_air_hard)\n            recall = structure_preserved.float().sum() / (gt_is_structure.float().sum() + 1e-6)\n\n        return {\n            'loss': total_loss,\n            'ce_loss': ce_loss.detach(),\n            'volume_loss': volume_loss.detach(),\n            'false_air_loss': false_air_loss.detach() if torch.is_tensor(false_air_loss) else false_air_loss,\n            'structure_loss': structure_loss.detach() if torch.is_tensor(structure_loss) else structure_loss,\n            'perceptual_loss': perceptual_loss.detach(),\n            'volume_ratio': volume_ratio_hard,\n            'soft_volume_ratio': soft_volume_ratio.detach(),\n            'recall': recall,\n        }\n\n\ndef compute_frequency_weights(\n    block_ids: torch.Tensor,\n    vocab_size: int,\n    smoothing: float = 0.5,\n) -> torch.Tensor:\n    \"\"\"Compute frequency-based weights.\"\"\"\n    counts = torch.bincount(block_ids.flatten(), minlength=vocab_size).clamp(min=1)\n    total = counts.sum()\n    weights = (total.float() / counts.float()) ** smoothing\n    return weights\n\nprint(\"=\"*70)\nprint(\"TEMPERATURE-SCALED VOLUME CONTROL\")\nprint(\"=\"*70)\nprint(\"WHY THIS WORKS:\")\nprint(\"  - Low temperature (0.1) makes softmax nearly one-hot\")\nprint(\"  - Soft volume ratio approximates hard volume ratio\")\nprint(\"  - Loss = (soft_ratio - 1.0)^2 does NOT go to zero!\")\nprint(\"  - Strong direct signal to fix volume ratio\")\nprint()\nprint(\"KEY CHANGES:\")\nprint(\"  - temperature = 0.1 (near-argmax)\")\nprint(\"  - volume_penalty_weight = 50 (5x stronger)\")\nprint(\"  - false_air_penalty_weight = 25 (5x stronger)\")\nprint(\"  - Added structure_loss (protect recall)\")\nprint(\"=\"*70)\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loss Weights (TEMPERATURE-SCALED) ===\nFREQUENCY_WEIGHT_CAP = 5.0\nVOLUME_PENALTY_WEIGHT = 50.0        # 5x stronger than before!\nFALSE_AIR_PENALTY_WEIGHT = 25.0     # 5x stronger than before!\nPERCEPTUAL_WEIGHT = 0.1\nTEMPERATURE = 0.1                   # Low temp = near-argmax softmax\\n",
    "Configuration:\")\nprint(f\"  Latent: 16x16x16 (4,096 positions)\")\nprint(f\"  Hidden dim: {HIDDEN_DIM}\")\nprint(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS})\")\nprint(f\"  Epochs: {TOTAL_EPOCHS}\")\nprint(f\"  Learning rate: {BASE_LR}\")\nprint(f\"\\n",
    "Loss weights:\")\nprint(f\"  Volume penalty: {VOLUME_PENALTY_WEIGHT} (prevents over-prediction)\")\nprint(f\"  False air penalty: {FALSE_AIR_PENALTY_WEIGHT} (protects recall)\")\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Vocabulary and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Find air tokens\n",
    "AIR_TOKENS: Set[int] = set()\n",
    "for tok, block in tok2block.items():\n",
    "    if 'air' in block.lower() and 'stair' not in block.lower():\n",
    "        AIR_TOKENS.add(tok)\n",
    "        print(f\"  Air token: {tok} = {block}\")\n",
    "\n",
    "AIR_TOKENS_TENSOR = torch.tensor(sorted(AIR_TOKENS), dtype=torch.long)\n",
    "\n",
    "# Load embeddings\n",
    "v3_embeddings = np.load(V3_EMBEDDINGS_PATH).astype(np.float32)\n",
    "EMBEDDING_DIM = v3_embeddings.shape[1]\n",
    "print(f\"\\nV3 embeddings: {v3_embeddings.shape} (dim={EMBEDDING_DIM})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Frequency Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing block frequencies from training data...\")\n",
    "print(\"(This may take a few minutes)\")\n",
    "\n",
    "all_block_ids = []\n",
    "train_files = sorted(Path(DATA_DIR).glob(\"*.h5\"))\n",
    "\n",
    "for h5_file in tqdm(train_files, desc=\"Scanning\"):\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        key = list(f.keys())[0]\n",
    "        structure = f[key][:].flatten()\n",
    "        all_block_ids.append(torch.from_numpy(structure).long())\n",
    "\n",
    "all_block_ids = torch.cat(all_block_ids)\n",
    "print(f\"\\nTotal blocks scanned: {len(all_block_ids):,}\")\n",
    "\n",
    "FREQUENCY_WEIGHT_TENSOR = compute_frequency_weights(\n",
    "    all_block_ids,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    smoothing=0.5\n",
    ").clamp(max=FREQUENCY_WEIGHT_CAP)\n",
    "\n",
    "print(f\"Frequency weights computed (cap={FREQUENCY_WEIGHT_CAP}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAEDataset(Dataset):\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        if not self.h5_files:\n",
    "            raise ValueError(f\"No H5 files in {data_dir}\")\n",
    "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h5_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_files[idx], 'r') as f:\n",
    "            key = list(f.keys())[0]\n",
    "            structure = f[key][:].astype(np.int64)\n",
    "        return torch.from_numpy(structure).long()\n",
    "\n",
    "train_dataset = VQVAEDataset(DATA_DIR)\n",
    "val_dataset = VQVAEDataset(VAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FrequencyWeightedLoss(\n    frequency_weights=FREQUENCY_WEIGHT_TENSOR,\n    frequency_cap=FREQUENCY_WEIGHT_CAP,\n    volume_penalty_weight=VOLUME_PENALTY_WEIGHT,\n    false_air_penalty_weight=FALSE_AIR_PENALTY_WEIGHT,\n    perceptual_weight=PERCEPTUAL_WEIGHT,\n    temperature=TEMPERATURE,\n    air_tokens=AIR_TOKENS,\n).to(device)\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits, targets, air_tokens):\n    \"\"\"Compute key metrics.\"\"\"\n    device = logits.device\n    B, C, X, Y, Z = logits.shape\n    \n    logits_flat = logits.permute(0, 2, 3, 4, 1).reshape(-1, C)\n    targets_flat = targets.view(-1)\n    \n    air_dev = air_tokens.to(device)\n    is_air = torch.isin(targets_flat, air_dev)\n    is_building = ~is_air\n    \n    preds = logits_flat.argmax(dim=1)\n    is_air_pred = torch.isin(preds, air_dev)\n    correct = (preds == targets_flat).float()\n    \n    metrics = {}\n    metrics['overall_acc'] = correct.mean()\n    \n    if is_building.any():\n        metrics['building_acc'] = correct[is_building].mean()\n        metrics['building_recall'] = (is_building & ~is_air_pred).sum().float() / is_building.sum()\n    else:\n        metrics['building_acc'] = torch.tensor(0.0, device=device)\n        metrics['building_recall'] = torch.tensor(0.0, device=device)\n    \n    is_struct = ~is_air\n    metrics['struct_recall'] = (is_struct & ~is_air_pred).sum().float() / is_struct.sum() if is_struct.any() else torch.tensor(0.0, device=device)\n    \n    pred_building = ~is_air_pred\n    pred_vol = pred_building.sum().float()\n    orig_vol = is_struct.sum().float()\n    metrics['volume_ratio'] = pred_vol / orig_vol if orig_vol > 0 else torch.tensor(1.0, device=device)\n    \n    return metrics\n\n\ndef train_epoch(model, criterion, loader, optimizer, scaler, device, air_tokens):\n    model.train()\n    model.quantizer.reset_usage()\n    \n    # Include ALL metrics from loss function\n    metrics_sum = {\n        'loss': 0.0, 'ce_loss': 0.0, 'volume_loss': 0.0, \n        'false_air_loss': 0.0, 'perceptual_loss': 0.0,\n        'overall_acc': 0.0, 'building_acc': 0.0, 'building_recall': 0.0,\n        'struct_recall': 0.0, 'volume_ratio': 0.0, 'recall': 0.0\n    }\n    n = 0\n    \n    optimizer.zero_grad()\n    \n    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n        batch = batch.to(device)\n        \n        with torch.amp.autocast('cuda', enabled=USE_AMP):\n            logits, z_q, indices = model(batch)\n            loss_dict = criterion(logits, batch, z_q)\n            loss = loss_dict['loss'] / GRAD_ACCUM_STEPS\n        \n        scaler.scale(loss).backward()\n        \n        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        with torch.no_grad():\n            # Collect loss metrics\n            for k in ['loss', 'ce_loss', 'volume_loss', 'false_air_loss', 'structure_loss', 'perceptual_loss', 'volume_ratio', 'soft_volume_ratio', 'recall']:\n                if k in loss_dict:\n                    val = loss_dict[k]\n                    metrics_sum[k] += val.item() if torch.is_tensor(val) else val\n            \n            batch_metrics = compute_metrics(logits, batch, air_tokens)\n            for k, v in batch_metrics.items():\n                metrics_sum[k] += v.item()\n        \n        n += 1\n    \n    metrics = {k: v / n for k, v in metrics_sum.items()}\n    \n    stage_stats = model.quantizer.get_usage_stats()\n    for stage_name, (usage, perp) in stage_stats.items():\n        metrics[f'{stage_name}_usage'] = usage\n        metrics[f'{stage_name}_perplexity'] = perp\n    \n    return metrics\n\n\n@torch.no_grad()\ndef validate(model, criterion, loader, device, air_tokens):\n    model.eval()\n    model.quantizer.reset_usage()\n    \n    # Include ALL metrics from loss function\n    metrics_sum = {\n        'loss': 0.0, 'ce_loss': 0.0, 'volume_loss': 0.0,\n        'false_air_loss': 0.0, 'perceptual_loss': 0.0,\n        'overall_acc': 0.0, 'building_acc': 0.0, 'building_recall': 0.0,\n        'struct_recall': 0.0, 'volume_ratio': 0.0, 'recall': 0.0\n    }\n    n = 0\n    \n    for batch in tqdm(loader, desc=\"Val\", leave=False):\n        batch = batch.to(device)\n        \n        with torch.amp.autocast('cuda', enabled=USE_AMP):\n            logits, z_q, indices = model(batch)\n            loss_dict = criterion(logits, batch, z_q)\n        \n        # Collect loss metrics\n        for k in ['loss', 'ce_loss', 'volume_loss', 'false_air_loss', 'structure_loss', 'perceptual_loss', 'volume_ratio', 'soft_volume_ratio', 'recall']:\n            if k in loss_dict:\n                val = loss_dict[k]\n                metrics_sum[k] += val.item() if torch.is_tensor(val) else val\n        \n        batch_metrics = compute_metrics(logits, batch, air_tokens)\n        for k, v in batch_metrics.items():\n            metrics_sum[k] += v.item()\n        \n        n += 1\n    \n    metrics = {k: v / n for k, v in metrics_sum.items()}\n    \n    stage_stats = model.quantizer.get_usage_stats()\n    for stage_name, (usage, perp) in stage_stats.items():\n        metrics[f'{stage_name}_usage'] = usage\n        metrics[f'{stage_name}_perplexity'] = perp\n    \n    return metrics\n\nprint(\"Training functions defined\")\nprint(\"  - Tracking: volume_ratio, recall, false_air_loss\")\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n    # Core metrics\n    'train_loss': [], 'val_loss': [],\n    'train_building_acc': [], 'val_building_acc': [],\n    'train_vol_ratio': [], 'val_vol_ratio': [],\n    'train_recall': [], 'val_recall': [],\n\n    # Loss components\n    'train_ce_loss': [], 'val_ce_loss': [],\n    'train_volume_loss': [], 'val_volume_loss': [],\n    'train_false_air_loss': [], 'val_false_air_loss': [],\n    'train_structure_loss': [], 'val_structure_loss': [],\n    'train_perceptual_loss': [], 'val_perceptual_loss': [],\n\n    # Soft volume ratio (should track hard ratio)\n    'train_soft_vol_ratio': [], 'val_soft_vol_ratio': [],\n\n    # RFSQ metrics\n    'train_stage0_perplexity': [], 'val_stage0_perplexity': [],\n\n    # Training info\n    'learning_rate': [],\n}\\n",
    "Training complete in {train_time/60:.1f} minutes\")\nprint(f\"Best val building accuracy: {best_building_acc:.1%} at epoch {best_epoch}\")\nprint(f\"Final volume ratio: {history['val_vol_ratio'][-1]:.2f}x\")\nprint(f\"Final recall: {history['val_recall'][-1]:.1%}\")\n\n\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(14, 15))\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# 1. Building Accuracy (TOP LEFT)\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs, history['train_building_acc'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs, history['val_building_acc'], 'r--', label='Val', linewidth=2)\n",
    "ax.axhline(y=0.492, color='g', linestyle=':', alpha=0.5, label='v6-freq (49.2%)')\n",
    "ax.axhline(y=0.60, color='orange', linestyle='--', alpha=0.5, label='Target (60%)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Building Accuracy', fontweight='bold', fontsize=12)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Volume Ratio (TOP RIGHT) - THE KEY METRIC TO WATCH!\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs, history['train_vol_ratio'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs, history['val_vol_ratio'], 'r--', label='Val', linewidth=2)\n",
    "ax.axhline(y=1.0, color='g', linestyle='--', linewidth=2, label='Target (1.0x)')\n",
    "ax.axhline(y=1.3, color='orange', linestyle=':', alpha=0.7, label='Max acceptable (1.3x)')\n",
    "ax.axhline(y=2.0, color='red', linestyle=':', alpha=0.5, label='Previous stuck (2.0x)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Volume Ratio')\n",
    "ax.set_title('Volume Ratio (MUST decrease from 2.0!)', fontweight='bold', fontsize=12, color='darkred')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, max(2.5, max(history['val_vol_ratio']) * 1.1))\n",
    "\n",
    "# 3. Recall (MIDDLE LEFT) - Structure preservation\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs, history['train_recall'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs, history['val_recall'], 'r--', label='Val', linewidth=2)\n",
    "ax.axhline(y=0.90, color='g', linestyle='--', alpha=0.5, label='Target (90%)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.set_title('Recall (Structure Preservation)', fontweight='bold', fontsize=12)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "# 4. Loss Components (MIDDLE RIGHT) - Debugging\n",
    "ax = axes[1, 1]\n",
    "ax.plot(epochs, history['val_ce_loss'], 'b-', label='CE Loss', linewidth=2)\n",
    "ax.plot(epochs, history['val_volume_loss'], 'r-', label='Volume Loss', linewidth=2)\n",
    "ax.plot(epochs, history['val_false_air_loss'], 'g-', label='False Air Loss', linewidth=2)\n",
    "ax.plot(epochs, history['val_perceptual_loss'], 'm-', label='Perceptual Loss', linewidth=1, alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss Value')\n",
    "ax.set_title('Loss Components (Val)', fontweight='bold', fontsize=12)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Total Loss (BOTTOM LEFT)\n",
    "ax = axes[2, 0]\n",
    "ax.plot(epochs, history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs, history['val_loss'], 'r--', label='Val', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total Loss')\n",
    "ax.set_title('Total Loss', fontweight='bold', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. RFSQ Perplexity (BOTTOM RIGHT)\n",
    "ax = axes[2, 1]\n",
    "ax.plot(epochs, history['train_stage0_perplexity'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(epochs, history['val_stage0_perplexity'], 'r--', label='Val', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Perplexity')\n",
    "ax.set_title('RFSQ Stage 0 Perplexity', fontweight='bold', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/vqvae_v8b_training.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary with ALL key metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best building accuracy: {best_building_acc:.1%} (epoch {best_epoch})\")\n",
    "print(f\"Final building accuracy: {history['val_building_acc'][-1]:.1%}\")\n",
    "print(f\"Final volume ratio: {history['val_vol_ratio'][-1]:.2f}x\")\n",
    "print(f\"Final recall: {history['val_recall'][-1]:.1%}\")\n",
    "print(f\"Training time: {train_time/60:.1f} minutes\")\n",
    "print()\n",
    "print(\"Loss components (final epoch):\")\n",
    "print(f\"  CE Loss: {history['val_ce_loss'][-1]:.4f}\")\n",
    "print(f\"  Volume Loss: {history['val_volume_loss'][-1]:.4f}\")\n",
    "print(f\"  False Air Loss: {history['val_false_air_loss'][-1]:.4f}\")\n",
    "print(f\"  Perceptual Loss: {history['val_perceptual_loss'][-1]:.4f}\")\n",
    "print()\n",
    "\n",
    "# Success criteria check\n",
    "vol_ok = history['val_vol_ratio'][-1] <= 1.3\n",
    "recall_ok = history['val_recall'][-1] >= 0.90\n",
    "acc_ok = best_building_acc >= 0.60\n",
    "\n",
    "print(\"Success Criteria:\")\n",
    "print(f\"  Volume ratio <= 1.3x: {'[PASS]' if vol_ok else '[FAIL]'} ({history['val_vol_ratio'][-1]:.2f}x)\")\n",
    "print(f\"  Recall >= 90%: {'[PASS]' if recall_ok else '[FAIL]'} ({history['val_recall'][-1]:.1%})\")\n",
    "print(f\"  Building acc >= 60%: {'[PASS]' if acc_ok else '[FAIL]'} ({best_building_acc:.1%})\")\n",
    "print()\n",
    "\n",
    "if vol_ok and recall_ok and acc_ok:\n",
    "    print(\"[SUCCESS] All targets met! Ready for Stage 2 (v8-C)\")\n",
    "elif vol_ok and recall_ok:\n",
    "    print(\"[PARTIAL] Volume and recall OK, but accuracy below target\")\n",
    "elif not vol_ok:\n",
    "    print(\"[CRITICAL] Volume ratio still too high - direct logit fix may not be working\")\n",
    "else:\n",
    "    print(\"[NEEDS ANALYSIS] Check the loss component plots above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'config': {\n",
    "        'version': 'v8-B-DIRECT-LOGIT',\n",
    "        'latent_resolution': '16x16x16',\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'total_epochs': TOTAL_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'base_lr': BASE_LR,\n",
    "        'seed': SEED,\n",
    "        'volume_penalty_weight': VOLUME_PENALTY_WEIGHT,\n",
    "        'false_air_penalty_weight': FALSE_AIR_PENALTY_WEIGHT,\n",
    "    },\n",
    "    'results': {\n",
    "        'best_building_acc': float(best_building_acc),\n",
    "        'best_epoch': best_epoch,\n",
    "        'final_building_acc': float(history['val_building_acc'][-1]),\n",
    "        'final_volume_ratio': float(history['val_vol_ratio'][-1]),\n",
    "        'final_recall': float(history['val_recall'][-1]),\n",
    "        'final_ce_loss': float(history['val_ce_loss'][-1]),\n",
    "        'final_volume_loss': float(history['val_volume_loss'][-1]),\n",
    "        'final_false_air_loss': float(history['val_false_air_loss'][-1]),\n",
    "        'training_time_min': float(train_time / 60),\n",
    "        'target_60pct_met': bool(best_building_acc >= 0.60),\n",
    "        'volume_target_met': bool(history['val_vol_ratio'][-1] <= 1.3),\n",
    "        'recall_target_met': bool(history['val_recall'][-1] >= 0.90),\n",
    "    },\n",
    "    'history': {k: [float(x) for x in v] for k, v in history.items()},\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/vqvae_v8b_results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "torch.save(model.state_dict(), f\"{OUTPUT_DIR}/vqvae_v8b_final.pt\")\n",
    "\n",
    "print(\"\\nResults saved to:\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v8b_results.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v8b_best.pt\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v8b_final.pt\")\n",
    "print(f\"  - {OUTPUT_DIR}/vqvae_v8b_training.png\")\n",
    "print(\"\\nDone!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}