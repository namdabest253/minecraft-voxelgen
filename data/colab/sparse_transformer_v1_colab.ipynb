{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è Sparse Structure Transformer for Minecraft Build Reconstruction\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "This notebook trains a **Sparse Structure Transformer** - a neural network that learns to reconstruct Minecraft builds. The model takes a 3D structure (like a house or castle) and learns to compress it into a compact representation, then decompress it back to the original structure.\n",
        "\n",
        "**The Problem We're Solving:**\n",
        "Minecraft builds are stored as 32√ó32√ó32 grids of blocks (32,768 total positions). However, ~80% of these positions are just air! Traditional approaches (like VQ-VAE) waste most of their computation on empty space and end up predicting \"air\" everywhere, failing completely on important blocks like stairs, doors, and slabs.\n",
        "\n",
        "**Our Solution:**\n",
        "Instead of processing the entire dense grid, we treat each build as a **sparse set of (position, block) pairs**. A house with 500 blocks becomes a set of 500 elements, not 32,768. This is like describing a room by listing the furniture, not by describing every cubic inch of air.\n",
        "\n",
        "---\n",
        "\n",
        "## What is a Sparse Transformer?\n",
        "\n",
        "### The Core Idea\n",
        "\n",
        "A **Sparse Transformer** processes variable-length sets of elements using self-attention, rather than fixed-size grids using convolutions.\n",
        "\n",
        "```\n",
        "Traditional VQ-VAE (Dense):\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  32√ó32√ó32 Grid  ‚Üí  3D CNN Encoder  ‚Üí  Latent  ‚Üí  3D CNN Decoder  ‚Üí  Grid  ‚îÇ\n",
        "‚îÇ  (32,768 voxels)      (80% air!)      Codes         (still)       (32,768) ‚îÇ\n",
        "‚îÇ                                                    predicts                 ‚îÇ\n",
        "‚îÇ                                                    mostly air               ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Sparse Transformer:\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Set of ~500 blocks  ‚Üí  Transformer  ‚Üí  Latent  ‚Üí  Transformer  ‚Üí  Set    ‚îÇ\n",
        "‚îÇ  {(pos‚ÇÅ, oak_planks),    Encoder       Codes      Decoder       of ~500   ‚îÇ\n",
        "‚îÇ   (pos‚ÇÇ, oak_stairs),  (attention)               (cross-attn)   predicted ‚îÇ\n",
        "‚îÇ   (pos‚ÇÉ, glass_pane),                                           blocks    ‚îÇ\n",
        "‚îÇ   ...}                                                                     ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **Sparse Representation**: Extract only non-air blocks as (x, y, z, block_embedding) tuples\n",
        "2. **Fourier Positional Encoding**: Encode 3D coordinates as high-frequency sinusoidal features (like NeRF)\n",
        "3. **Transformer Encoder**: Self-attention lets each block \"see\" all other blocks in the structure\n",
        "4. **Set Pooling**: Compress variable-length encoded blocks into fixed-size latent codes\n",
        "5. **Vector Quantization (optional)**: Discretize latent space for generation\n",
        "6. **Transformer Decoder**: Cross-attention from target positions to latent codes\n",
        "7. **Embedding Prediction**: Output block embeddings, match to vocabulary via nearest neighbor\n",
        "\n",
        "### Why This Works Better\n",
        "\n",
        "| Aspect | Dense VQ-VAE | Sparse Transformer |\n",
        "|--------|-------------|-------------------|\n",
        "| **Representation** | 32¬≥ = 32,768 voxels | ~500 non-air blocks |\n",
        "| **Air handling** | 80% of computation wasted | Air not represented |\n",
        "| **Loss function** | 3717-way classification | Embedding regression (MSE) |\n",
        "| **Class imbalance** | Air dominates training | Equal weight per block |\n",
        "| **Embedding usage** | Frozen, mostly ignored | Direct input AND output target |\n",
        "| **Stairs/doors accuracy** | 0% | Expected >50% |\n",
        "\n",
        "---\n",
        "\n",
        "## How to Test With Your Own Build\n",
        "\n",
        "After training, you can test the model on any Minecraft structure:\n",
        "\n",
        "```python\n",
        "# 1. Load your build (32√ó32√ó32 grid of block IDs)\n",
        "with h5py.File(\"my_build.h5\", 'r') as f:\n",
        "    structure = f['structure'][:]\n",
        "\n",
        "# 2. Convert to sparse format\n",
        "positions, block_ids, embeddings = extract_sparse(structure, all_embeddings)\n",
        "\n",
        "# 3. Run through model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(positions, embeddings, attention_mask)\n",
        "    pred_embeddings = outputs[\"pred_embeddings\"]\n",
        "\n",
        "# 4. Find nearest block for each prediction\n",
        "pred_block_ids = torch.cdist(pred_embeddings, all_embeddings).argmin(dim=-1)\n",
        "\n",
        "# 5. Reconstruct to dense grid\n",
        "reconstructed = reconstruct_dense(positions, pred_block_ids)\n",
        "\n",
        "# 6. Compare!\n",
        "accuracy = (reconstructed == structure).mean()\n",
        "```\n",
        "\n",
        "The notebook includes a full example of this at the end.\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Results\n",
        "\n",
        "Based on our VQ-VAE baseline (~49% non-air accuracy, 0% on stairs/doors):\n",
        "\n",
        "| Metric | VQ-VAE Baseline | Expected Sparse Transformer |\n",
        "|--------|-----------------|----------------------------|\n",
        "| Overall accuracy | ~49% | 70-80% |\n",
        "| Stairs | 0% | >50% |\n",
        "| Slabs | 0% | >50% |\n",
        "| Doors | 0% | >50% |\n",
        "| Fences | 0% | >50% |\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. **Setup** - Install dependencies and mount Google Drive\n",
        "2. **Configuration** - Set paths and hyperparameters\n",
        "3. **Data Loading** - Load embeddings and vocabulary\n",
        "4. **Dataset** - Define sparse structure dataset\n",
        "5. **Model Components** - Positional encoding, pooling, VQ\n",
        "6. **Sparse Transformer** - The main model architecture\n",
        "7. **Training Functions** - Loss computation and training loop\n",
        "8. **Training** - Run the training\n",
        "9. **Visualizations** - Training curves, category accuracy, confusion matrix\n",
        "10. **Testing** - Test on a real structure\n",
        "11. **Save Results** - Export model and metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1. Setup - Install Dependencies and Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1.1 Mount Google Drive and Install Dependencies\n",
        "# ============================================================\n",
        "# Mount Google Drive to access data and save results\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install any missing dependencies (h5py should be pre-installed)\n",
        "# Using %pip for better Colab compatibility\n",
        "%pip install -q h5py tqdm\n",
        "\n",
        "print(\"‚úì Google Drive mounted\")\n",
        "print(\"‚úì Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1.2 Import Libraries\n",
        "# ============================================================\n",
        "\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any, Optional, Set\n",
        "from collections import defaultdict\n",
        "\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Training will be very slow.\")\n",
        "    print(\"  Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2. Configuration - Set Paths and Hyperparameters\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT**: Update the paths below to match your Google Drive structure.\n",
        "\n",
        "The data files needed are:\n",
        "- `block_embeddings_v3.npy` - Block2Vec embeddings (40-dimensional vectors for each block type)\n",
        "- `tok2block.json` - Vocabulary mapping token IDs to block names\n",
        "- `train/` folder with `.h5` structure files\n",
        "- `val/` folder with `.h5` structure files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2.1 Path Configuration (Google Colab / Google Drive)\n",
        "# ============================================================\n",
        "# ‚ö†Ô∏è UPDATE THESE PATHS to match your Google Drive structure!\n",
        "\n",
        "# Base path in Google Drive\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/minecraft_ai\"\n",
        "\n",
        "# Data paths\n",
        "DATA_DIR = f\"{DRIVE_BASE}/data/splits/train\"\n",
        "VAL_DIR = f\"{DRIVE_BASE}/data/splits/val\"\n",
        "EMBEDDINGS_PATH = f\"{DRIVE_BASE}/embeddings/block_embeddings_v3.npy\"\n",
        "VOCAB_PATH = f\"{DRIVE_BASE}/vocabulary/tok2block.json\"\n",
        "OUTPUT_DIR = \"/content/output\"  # Local output, copy to Drive later\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Verify paths exist\n",
        "paths_ok = True\n",
        "for name, path in [(\"Embeddings\", EMBEDDINGS_PATH), (\"Vocabulary\", VOCAB_PATH), \n",
        "                    (\"Train data\", DATA_DIR), (\"Val data\", VAL_DIR)]:\n",
        "    if os.path.exists(path):\n",
        "        print(f\"‚úì {name}: {path}\")\n",
        "    else:\n",
        "        print(f\"‚úó {name} NOT FOUND: {path}\")\n",
        "        paths_ok = False\n",
        "\n",
        "if not paths_ok:\n",
        "    print(\"\\n‚ö†Ô∏è Some paths are missing! Please update the paths above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2.2 Model and Training Hyperparameters\n",
        "# ============================================================\n",
        "\n",
        "# Air tokens will be detected dynamically from vocabulary (see Cell 9)\n",
        "AIR_TOKENS: Set[int] = set()  # Placeholder - populated after loading vocab\n",
        "\n",
        "# === Model Architecture ===\n",
        "EMBED_DIM = 40           # Block2Vec embedding dimension\n",
        "HIDDEN_DIM = 256         # Transformer hidden dimension\n",
        "N_ENCODER_LAYERS = 6     # Number of transformer encoder layers\n",
        "N_DECODER_LAYERS = 6     # Number of transformer decoder layers\n",
        "N_HEADS = 8              # Number of attention heads\n",
        "NUM_LATENT_CODES = 16    # Number of latent codes after pooling\n",
        "VQ_NUM_EMBEDDINGS = 1024 # VQ codebook size (0 = disable VQ)\n",
        "DROPOUT = 0.1            # Dropout rate\n",
        "\n",
        "# === Training Settings ===\n",
        "EPOCHS = 20              # Number of training epochs\n",
        "BATCH_SIZE = 16          # Batch size (reduce if OOM)\n",
        "LEARNING_RATE = 1e-4     # Initial learning rate\n",
        "WEIGHT_DECAY = 0.01      # AdamW weight decay\n",
        "AUX_WEIGHT = 0.1         # Weight for auxiliary classification loss\n",
        "MAX_BLOCKS = 2048        # Maximum blocks per structure\n",
        "\n",
        "# === Other ===\n",
        "SEED = 42                # Random seed for reproducibility\n",
        "NUM_WORKERS = 2          # DataLoader workers\n",
        "\n",
        "# Print configuration summary\n",
        "print(\"=\" * 60)\n",
        "print(\"CONFIGURATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model: {HIDDEN_DIM}d hidden, {N_ENCODER_LAYERS}+{N_DECODER_LAYERS} layers, {N_HEADS} heads\")\n",
        "print(f\"Latent: {NUM_LATENT_CODES} codes\" + (f\", VQ with {VQ_NUM_EMBEDDINGS} entries\" if VQ_NUM_EMBEDDINGS > 0 else \", no VQ\"))\n",
        "print(f\"Training: {EPOCHS} epochs, batch={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
        "print(f\"Max blocks per structure: {MAX_BLOCKS}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 3. Data Loading - Load Embeddings and Vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3.1 Set Random Seeds for Reproducibility\n",
        "# ============================================================\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(f\"‚úì Random seeds set to {SEED}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3.2 Load Block2Vec Embeddings and Vocabulary\n",
        "# ============================================================\n",
        "\n",
        "# Load Block2Vec embeddings\n",
        "# These are 40-dimensional vectors that capture semantic similarity between blocks\n",
        "# e.g., oak_stairs is close to spruce_stairs, oak_planks, etc.\n",
        "all_embeddings = np.load(EMBEDDINGS_PATH).astype(np.float32)\n",
        "\n",
        "# Check for NaN/Inf in embeddings\n",
        "if np.isnan(all_embeddings).any():\n",
        "    print(\"‚ö†Ô∏è  WARNING: NaN values found in embeddings! Replacing with zeros.\")\n",
        "    all_embeddings = np.nan_to_num(all_embeddings, nan=0.0)\n",
        "if np.isinf(all_embeddings).any():\n",
        "    print(\"‚ö†Ô∏è  WARNING: Inf values found in embeddings! Replacing with zeros.\")\n",
        "    all_embeddings = np.nan_to_num(all_embeddings, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# Normalize embeddings if they have extreme values\n",
        "embed_max = np.abs(all_embeddings).max()\n",
        "if embed_max > 100:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Embeddings have large values (max={embed_max:.2f}). Normalizing.\")\n",
        "    all_embeddings = all_embeddings / embed_max * 10\n",
        "\n",
        "all_embeddings_tensor = torch.from_numpy(all_embeddings).to(device)\n",
        "VOCAB_SIZE = all_embeddings.shape[0]\n",
        "print(f\"‚úì Loaded embeddings: {all_embeddings.shape}\")\n",
        "print(f\"  - {VOCAB_SIZE} unique block types\")\n",
        "print(f\"  - {EMBED_DIM}-dimensional embeddings\")\n",
        "print(f\"  - Value range: [{all_embeddings.min():.3f}, {all_embeddings.max():.3f}]\")\n",
        "\n",
        "# Load vocabulary (token ID ‚Üí block name mapping)\n",
        "with open(VOCAB_PATH, 'r') as f:\n",
        "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
        "print(f\"‚úì Loaded vocabulary: {len(tok2block)} blocks\")\n",
        "\n",
        "# Dynamically detect air tokens from vocabulary\n",
        "# This ensures we use the correct tokens regardless of vocabulary version\n",
        "global AIR_TOKENS\n",
        "AIR_TOKENS = set()\n",
        "for tok, block in tok2block.items():\n",
        "    block_lower = block.lower()\n",
        "    # Match \"air\", \"cave_air\", \"void_air\" but not \"stairs\"\n",
        "    if 'air' in block_lower and 'stair' not in block_lower:\n",
        "        AIR_TOKENS.add(tok)\n",
        "        \n",
        "print(f\"‚úì Detected {len(AIR_TOKENS)} air tokens: {sorted(AIR_TOKENS)}\")\n",
        "for tok in sorted(AIR_TOKENS):\n",
        "    print(f\"    {tok}: {tok2block[tok]}\")\n",
        "\n",
        "# Show some example blocks\n",
        "print(\"\\nExample blocks in vocabulary:\")\n",
        "for tok in [0, 100, 500, 1000, 2000, 3000]:\n",
        "    if tok in tok2block:\n",
        "        print(f\"  Token {tok}: {tok2block[tok]}\")\n",
        "\n",
        "# Verify air tokens\n",
        "print(\"\\nAir tokens:\")\n",
        "for tok in AIR_TOKENS:\n",
        "    if tok in tok2block:\n",
        "        print(f\"  Token {tok}: {tok2block[tok]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 4. Dataset - Sparse Structure Dataset\n",
        "\n",
        "This is where the magic happens! Instead of loading 32√ó32√ó32 dense grids, we extract only the non-air blocks as a sparse set of (position, block_id, embedding) tuples.\n",
        "\n",
        "**Key insight**: A typical Minecraft structure has ~500-2000 non-air blocks out of 32,768 total positions. By focusing only on these, we:\n",
        "1. Reduce memory by ~16x\n",
        "2. Give equal weight to every block (no air domination)\n",
        "3. Process structures in proportion to their complexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4.1 Sparse Structure Dataset Class\n",
        "# ============================================================\n",
        "\n",
        "class SparseStructureDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that converts dense 32x32x32 structures to sparse (position, block) sets.\n",
        "    \n",
        "    Instead of representing a structure as 32,768 voxels (80% air), we extract only\n",
        "    the non-air blocks as a set of (x, y, z, block_id) tuples.\n",
        "    \n",
        "    For example, a small house with 500 blocks becomes:\n",
        "    {\n",
        "        positions: [[0,0,0], [0,0,1], [0,1,0], ...],  # 500 x 3\n",
        "        block_ids: [oak_planks, oak_planks, glass, ...],  # 500\n",
        "        embeddings: [[0.1, 0.2, ...], ...]  # 500 x 40\n",
        "    }\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str,\n",
        "        embeddings: np.ndarray,\n",
        "        max_files: Optional[int] = None,\n",
        "        max_blocks: int = 2048,\n",
        "        augment: bool = False,\n",
        "        seed: int = 42,\n",
        "    ):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.embeddings = embeddings\n",
        "        self.max_blocks = max_blocks\n",
        "        self.augment = augment\n",
        "        self.rng = random.Random(seed)\n",
        "        \n",
        "        # Find all .h5 structure files\n",
        "        # First try the directory directly\n",
        "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
        "        \n",
        "        # If no files found, try searching recursively in subdirectories\n",
        "        if len(self.h5_files) == 0:\n",
        "            print(f\"  No .h5 files in root directory, searching subdirectories...\")\n",
        "            self.h5_files = sorted(self.data_dir.glob(\"**/*.h5\"))\n",
        "            if len(self.h5_files) > 0:\n",
        "                print(f\"  Found {len(self.h5_files)} .h5 files in subdirectories\")\n",
        "        \n",
        "        if max_files:\n",
        "            self.h5_files = self.h5_files[:max_files]\n",
        "        \n",
        "        if len(self.h5_files) == 0:\n",
        "            # Provide helpful error message\n",
        "            error_msg = f\"No .h5 files found in {data_dir}\\n\"\n",
        "            error_msg += f\"  Please check:\\n\"\n",
        "            error_msg += f\"  1. The path is correct: {data_dir}\\n\"\n",
        "            error_msg += f\"  2. Files are uploaded to Google Drive\\n\"\n",
        "            error_msg += f\"  3. Files have .h5 extension\"\n",
        "            raise ValueError(error_msg)\n",
        "        \n",
        "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.h5_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load the dense 32x32x32 structure\n",
        "        with h5py.File(self.h5_files[idx], 'r') as f:\n",
        "            key = list(f.keys())[0]\n",
        "            structure = f[key][:].astype(np.int64)\n",
        "        \n",
        "        # Extract non-air blocks (the sparse representation!)\n",
        "        non_air_mask = ~np.isin(structure, list(AIR_TOKENS))\n",
        "        positions = np.argwhere(non_air_mask).astype(np.float32)  # [N, 3]\n",
        "        block_ids = structure[non_air_mask]  # [N]\n",
        "        \n",
        "        # Handle empty structures: add a dummy block at origin\n",
        "        if len(block_ids) == 0:\n",
        "            positions = np.array([[0.0, 0.0, 0.0]], dtype=np.float32)\n",
        "            block_ids = np.array([0], dtype=np.int64)  # Use first block in vocab\n",
        "        \n",
        "        # Clamp block IDs to valid embedding range\n",
        "        max_block_id = len(self.embeddings) - 1\n",
        "        block_ids = np.clip(block_ids, 0, max_block_id)\n",
        "        \n",
        "        # Randomly sample if too many blocks (for memory efficiency)\n",
        "        n_blocks = len(block_ids)\n",
        "        if n_blocks > self.max_blocks:\n",
        "            indices = self.rng.sample(range(n_blocks), self.max_blocks)\n",
        "            indices = sorted(indices)  # Keep spatial ordering\n",
        "            positions = positions[indices]\n",
        "            block_ids = block_ids[indices]\n",
        "        \n",
        "        # Data augmentation: random rotation and flips\n",
        "        if self.augment:\n",
        "            positions = self._augment(positions)\n",
        "        \n",
        "        # Look up embeddings for each block\n",
        "        embeddings = self.embeddings[block_ids]\n",
        "        \n",
        "        # Check for NaN in embeddings (shouldn't happen but safety check)\n",
        "        if np.isnan(embeddings).any():\n",
        "            print(f\"Warning: NaN in embeddings for file {self.h5_files[idx]}\")\n",
        "            embeddings = np.nan_to_num(embeddings, 0.0)\n",
        "        \n",
        "        return {\n",
        "            \"positions\": torch.from_numpy(positions).float(),\n",
        "            \"block_ids\": torch.from_numpy(block_ids).long(),\n",
        "            \"embeddings\": torch.from_numpy(embeddings).float(),\n",
        "            \"num_blocks\": torch.tensor(len(block_ids), dtype=torch.long),\n",
        "        }\n",
        "    \n",
        "    def _augment(self, positions, grid_size=32):\n",
        "        \"\"\"Apply random 90¬∞ rotation around Y axis and horizontal flips.\"\"\"\n",
        "        positions = positions.copy()\n",
        "        \n",
        "        # Random rotation around Y axis (0, 90, 180, or 270 degrees)\n",
        "        k = self.rng.randint(0, 3)\n",
        "        if k > 0:\n",
        "            x, y, z = positions[:, 0], positions[:, 1], positions[:, 2]\n",
        "            for _ in range(k):\n",
        "                new_x = grid_size - 1 - z\n",
        "                new_z = x.copy()\n",
        "                x, z = new_x, new_z\n",
        "            positions[:, 0], positions[:, 2] = x, z\n",
        "        \n",
        "        # Random horizontal flips\n",
        "        if self.rng.random() > 0.5:\n",
        "            positions[:, 0] = grid_size - 1 - positions[:, 0]\n",
        "        if self.rng.random() > 0.5:\n",
        "            positions[:, 2] = grid_size - 1 - positions[:, 2]\n",
        "        \n",
        "        return positions\n",
        "\n",
        "\n",
        "def collate_sparse(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function that pads variable-length sequences.\n",
        "    \n",
        "    Since structures have different numbers of blocks, we pad to the\n",
        "    maximum length in the batch and create attention masks.\n",
        "    \"\"\"\n",
        "    positions = pad_sequence([b[\"positions\"] for b in batch], batch_first=True)\n",
        "    block_ids = pad_sequence([b[\"block_ids\"] for b in batch], batch_first=True)\n",
        "    embeddings = pad_sequence([b[\"embeddings\"] for b in batch], batch_first=True)\n",
        "    num_blocks = torch.stack([b[\"num_blocks\"] for b in batch])\n",
        "    \n",
        "    # Create attention mask: True for valid positions, False for padding\n",
        "    max_len = positions.size(1)\n",
        "    attention_mask = torch.arange(max_len).unsqueeze(0) < num_blocks.unsqueeze(1)\n",
        "    \n",
        "    return {\n",
        "        \"positions\": positions,\n",
        "        \"block_ids\": block_ids,\n",
        "        \"embeddings\": embeddings,\n",
        "        \"num_blocks\": num_blocks,\n",
        "        \"attention_mask\": attention_mask,\n",
        "    }\n",
        "\n",
        "print(\"‚úì SparseStructureDataset defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 5. Model Components - Positional Encoding, Pooling, VQ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5.1 Fourier Positional Encoding\n",
        "# ============================================================\n",
        "# Transforms 3D coordinates into high-dimensional features using\n",
        "# sin/cos functions at multiple frequencies. This allows the network\n",
        "# to learn both low and high frequency spatial patterns.\n",
        "\n",
        "class FourierPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Fourier feature encoding for 3D coordinates (like NeRF).\n",
        "    \n",
        "    Maps (x, y, z) ‚Üí [sin(2^0 * œÄ * x), cos(2^0 * œÄ * x), sin(2^1 * œÄ * x), ...]\n",
        "    \n",
        "    This creates 3 * num_frequencies * 2 features (sin + cos for each freq and coord).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_frequencies: int = 10, max_coord: int = 32, include_input: bool = True):\n",
        "        super().__init__()\n",
        "        self.num_frequencies = num_frequencies\n",
        "        self.max_coord = max_coord\n",
        "        self.include_input = include_input\n",
        "        \n",
        "        # Frequency bands: 2^0, 2^1, 2^2, ..., 2^(L-1)\n",
        "        freq_bands = 2.0 ** torch.linspace(0, num_frequencies - 1, num_frequencies)\n",
        "        self.register_buffer(\"freq_bands\", freq_bands)\n",
        "        \n",
        "        # Output dimension\n",
        "        self.output_dim = 3 * num_frequencies * 2  # 3 coords * L freqs * 2 (sin+cos)\n",
        "        if include_input:\n",
        "            self.output_dim += 3  # Also include normalized input coords\n",
        "    \n",
        "    def forward(self, positions):\n",
        "        # Normalize to [-1, 1]\n",
        "        normalized = positions / self.max_coord * 2 - 1\n",
        "        \n",
        "        # Apply frequencies: [B, N, 3, num_freq]\n",
        "        scaled = normalized.unsqueeze(-1) * self.freq_bands * math.pi\n",
        "        \n",
        "        # Sin and cos: [B, N, 3, num_freq * 2]\n",
        "        encoded = torch.cat([torch.sin(scaled), torch.cos(scaled)], dim=-1)\n",
        "        \n",
        "        # Flatten: [B, N, 3 * num_freq * 2]\n",
        "        encoded = encoded.view(*positions.shape[:-1], -1)\n",
        "        \n",
        "        if self.include_input:\n",
        "            encoded = torch.cat([normalized, encoded], dim=-1)\n",
        "        \n",
        "        return encoded\n",
        "\n",
        "print(\"‚úì FourierPositionalEncoding defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5.2 Set Pooling and Vector Quantization\n",
        "# ============================================================\n",
        "\n",
        "class SetPooling(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention-based pooling to compress variable-length sets into fixed-size.\n",
        "    \n",
        "    Uses learnable \"seed\" vectors that attend to the input set, producing\n",
        "    a fixed number of output vectors regardless of input length.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, num_outputs=16, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_outputs = num_outputs\n",
        "        self.seeds = nn.Parameter(torch.randn(num_outputs, input_dim))\n",
        "        self.attention = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
        "        self.proj = nn.Linear(input_dim, output_dim)\n",
        "        self.norm = nn.LayerNorm(output_dim)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size = x.size(0)\n",
        "        seeds = self.seeds.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        key_padding_mask = ~mask if mask is not None else None\n",
        "        \n",
        "        pooled, _ = self.attention(seeds, x, x, key_padding_mask=key_padding_mask)\n",
        "        return self.norm(self.proj(pooled))\n",
        "\n",
        "\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    \"\"\"\n",
        "    Vector Quantization with Exponential Moving Average codebook updates.\n",
        "    \n",
        "    Discretizes continuous latent vectors by mapping each to its nearest\n",
        "    codebook entry. Uses EMA (not gradients) to update the codebook.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_embeddings=1024, embedding_dim=256, commitment_cost=0.5, decay=0.99, epsilon=1e-5):\n",
        "        super().__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.commitment_cost = commitment_cost\n",
        "        self.decay = decay\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "        # Initialize codebook with smaller variance (0.1 instead of 1.0)\n",
        "        self.register_buffer(\"codebook\", torch.randn(num_embeddings, embedding_dim) * 0.1)\n",
        "        # Initialize cluster sizes to 1 (not 0) to prevent division issues\n",
        "        self.register_buffer(\"ema_cluster_size\", torch.ones(num_embeddings))\n",
        "        # Initialize embed_sum to match codebook\n",
        "        self.register_buffer(\"ema_embed_sum\", self.codebook.clone())\n",
        "        self.register_buffer(\"initialized\", torch.tensor(False))\n",
        "    \n",
        "    def forward(self, z_e):\n",
        "        B, K, D = z_e.shape\n",
        "        flat = z_e.view(-1, D)\n",
        "        \n",
        "        # Initialize codebook from first batch (k-means++ style)\n",
        "        if not self.initialized and self.training:\n",
        "            n_samples = min(flat.size(0), self.num_embeddings)\n",
        "            indices = torch.randperm(flat.size(0))[:n_samples]\n",
        "            self.codebook.data[:n_samples] = flat[indices].detach()\n",
        "            self.ema_embed_sum.data[:n_samples] = flat[indices].detach()\n",
        "            self.initialized.fill_(True)\n",
        "            print(\"  ‚Üí VQ codebook initialized from data\")\n",
        "        \n",
        "        # Find nearest codebook entries\n",
        "        distances = (flat ** 2).sum(1, keepdim=True) + (self.codebook ** 2).sum(1) - 2 * flat @ self.codebook.t()\n",
        "        indices = distances.argmin(dim=1)\n",
        "        z_q_flat = F.embedding(indices, self.codebook)\n",
        "        \n",
        "        # EMA codebook update (only for used entries)\n",
        "        if self.training:\n",
        "            encodings = F.one_hot(indices, self.num_embeddings).float()\n",
        "            cluster_counts = encodings.sum(0)\n",
        "            \n",
        "            # Update EMA stats\n",
        "            self.ema_cluster_size.mul_(self.decay).add_(cluster_counts, alpha=1-self.decay)\n",
        "            embed_sum = encodings.t() @ flat\n",
        "            self.ema_embed_sum.mul_(self.decay).add_(embed_sum, alpha=1-self.decay)\n",
        "            \n",
        "            # Update codebook with Laplace smoothing\n",
        "            n = self.ema_cluster_size.sum()\n",
        "            smoothed = (self.ema_cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n\n",
        "            # Only update where we have sufficient counts\n",
        "            new_codebook = self.ema_embed_sum / (smoothed.unsqueeze(1) + self.epsilon)\n",
        "            # Clamp to prevent extreme values\n",
        "            new_codebook = torch.clamp(new_codebook, -10, 10)\n",
        "            self.codebook.data.copy_(new_codebook)\n",
        "        \n",
        "        z_q = z_q_flat.view(B, K, D)\n",
        "        vq_loss = self.commitment_cost * F.mse_loss(z_e, z_q.detach())\n",
        "        z_q = z_e + (z_q - z_e).detach()  # Straight-through estimator\n",
        "        \n",
        "        return z_q, vq_loss, indices.view(B, K)\n",
        "\n",
        "print(\"‚úì SetPooling and VectorQuantizerEMA defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 6. Sparse Structure Transformer - The Main Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6.1 Sparse Structure Transformer Architecture\n",
        "# ============================================================\n",
        "\n",
        "class SparseStructureTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based autoencoder for sparse Minecraft structures.\n",
        "    \n",
        "    Architecture:\n",
        "    1. Input: Set of (position, block_embedding) pairs\n",
        "    2. Positional Encoding: Fourier features for 3D coordinates\n",
        "    3. Transformer Encoder: Self-attention over all blocks\n",
        "    4. Set Pooling: Compress to fixed-size latent codes\n",
        "    5. Vector Quantization: Discretize latent space (optional)\n",
        "    6. Transformer Decoder: Cross-attention from positions to latent\n",
        "    7. Output: Predicted block embeddings ‚Üí nearest neighbor lookup\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int = 40,\n",
        "        hidden_dim: int = 256,\n",
        "        n_encoder_layers: int = 6,\n",
        "        n_decoder_layers: int = 6,\n",
        "        n_heads: int = 8,\n",
        "        num_latent_codes: int = 16,\n",
        "        vq_num_embeddings: int = 1024,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_vq = vq_num_embeddings > 0\n",
        "        \n",
        "        # Positional encoding for 3D coordinates\n",
        "        self.pos_encoder = FourierPositionalEncoding(num_frequencies=10, max_coord=32)\n",
        "        pos_dim = self.pos_encoder.output_dim\n",
        "        \n",
        "        # Input projection: position features + block embedding ‚Üí hidden_dim\n",
        "        self.input_proj = nn.Linear(pos_dim + embed_dim, hidden_dim)\n",
        "        \n",
        "        # Transformer Encoder: self-attention over blocks\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            hidden_dim, n_heads, hidden_dim * 4, dropout, batch_first=True, norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, n_encoder_layers)\n",
        "        \n",
        "        # Set Pooling: variable-length ‚Üí fixed-size latent\n",
        "        self.pool = SetPooling(hidden_dim, hidden_dim, num_latent_codes, n_heads)\n",
        "        \n",
        "        # Vector Quantization (optional)\n",
        "        self.vq = None\n",
        "        if self.use_vq:\n",
        "            self.vq = VectorQuantizerEMA(vq_num_embeddings, hidden_dim)\n",
        "        \n",
        "        # Transformer Decoder: cross-attention from positions to latent\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            hidden_dim, n_heads, hidden_dim * 4, dropout, batch_first=True, norm_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, n_decoder_layers)\n",
        "        \n",
        "        # Position query projection for decoder\n",
        "        self.pos_query_proj = nn.Linear(pos_dim, hidden_dim)\n",
        "        \n",
        "        # Output projection: hidden_dim ‚Üí block embedding\n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "        )\n",
        "        \n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    \n",
        "    def encode(self, positions, embeddings, attention_mask=None):\n",
        "        \"\"\"Encode sparse structure to latent codes.\"\"\"\n",
        "        # Combine position features and block embeddings\n",
        "        pos_features = self.pos_encoder(positions)\n",
        "        x = self.input_proj(torch.cat([pos_features, embeddings], dim=-1))\n",
        "        \n",
        "        # Transformer encoder with masking\n",
        "        src_key_padding_mask = ~attention_mask if attention_mask is not None else None\n",
        "        encoded = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        \n",
        "        # Pool to fixed-size latent\n",
        "        latent = self.pool(encoded, attention_mask)\n",
        "        \n",
        "        # Optional VQ\n",
        "        vq_loss = None\n",
        "        if self.use_vq and self.vq is not None:\n",
        "            latent, vq_loss, _ = self.vq(latent)\n",
        "        \n",
        "        return latent, vq_loss\n",
        "    \n",
        "    def decode(self, latent, positions, attention_mask=None):\n",
        "        \"\"\"Decode latent codes to block embeddings at given positions.\"\"\"\n",
        "        # Create position queries\n",
        "        pos_features = self.pos_encoder(positions)\n",
        "        queries = self.pos_query_proj(pos_features)\n",
        "        \n",
        "        # Cross-attention: queries attend to latent codes\n",
        "        tgt_key_padding_mask = ~attention_mask if attention_mask is not None else None\n",
        "        decoded = self.decoder(queries, latent, tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        \n",
        "        # Project to embedding space\n",
        "        return self.output_proj(decoded)\n",
        "    \n",
        "    def forward(self, positions, embeddings, attention_mask=None):\n",
        "        \"\"\"Full forward pass: encode ‚Üí decode.\"\"\"\n",
        "        latent, vq_loss = self.encode(positions, embeddings, attention_mask)\n",
        "        pred_embeddings = self.decode(latent, positions, attention_mask)\n",
        "        \n",
        "        return {\n",
        "            \"pred_embeddings\": pred_embeddings,\n",
        "            \"vq_loss\": vq_loss if vq_loss is not None else torch.tensor(0.0, device=positions.device),\n",
        "        }\n",
        "\n",
        "print(\"‚úì SparseStructureTransformer defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6.2 Create Datasets and Model\n",
        "# ============================================================\n",
        "\n",
        "# Debug: Check what's in the directories before creating datasets\n",
        "print(\"=\" * 60)\n",
        "print(\"DEBUGGING: Checking data directories...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dir_name, dir_path in [(\"Train\", DATA_DIR), (\"Val\", VAL_DIR)]:\n",
        "    print(f\"\\n{dir_name} directory: {dir_path}\")\n",
        "    if os.path.exists(dir_path):\n",
        "        print(f\"  ‚úì Path exists\")\n",
        "        # List all items\n",
        "        items = list(Path(dir_path).iterdir())\n",
        "        print(f\"  Items in directory: {len(items)}\")\n",
        "        \n",
        "        # Count .h5 files\n",
        "        h5_files = list(Path(dir_path).glob(\"*.h5\"))\n",
        "        print(f\"  .h5 files found: {len(h5_files)}\")\n",
        "        \n",
        "        if len(h5_files) == 0:\n",
        "            print(f\"  ‚ö†Ô∏è  No .h5 files found!\")\n",
        "            # Show first few items\n",
        "            print(f\"  First 10 items:\")\n",
        "            for item in items[:10]:\n",
        "                item_type = \"DIR\" if item.is_dir() else \"FILE\"\n",
        "                print(f\"    [{item_type}] {item.name}\")\n",
        "            \n",
        "            # Check subdirectories\n",
        "            subdirs = [d for d in items if d.is_dir()]\n",
        "            if subdirs:\n",
        "                print(f\"\\n  Found {len(subdirs)} subdirectories:\")\n",
        "                for subdir in subdirs[:5]:\n",
        "                    h5_in_sub = list(subdir.glob(\"*.h5\"))\n",
        "                    print(f\"    {subdir.name}/: {len(h5_in_sub)} .h5 files\")\n",
        "        else:\n",
        "            print(f\"  ‚úì Found {len(h5_files)} .h5 files (showing first 3):\")\n",
        "            for f in h5_files[:3]:\n",
        "                print(f\"    {f.name}\")\n",
        "    else:\n",
        "        print(f\"  ‚úó Path does NOT exist!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Creating datasets...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SparseStructureDataset(\n",
        "    DATA_DIR, all_embeddings, max_blocks=MAX_BLOCKS, augment=True, seed=SEED\n",
        ")\n",
        "val_dataset = SparseStructureDataset(\n",
        "    VAL_DIR, all_embeddings, max_blocks=MAX_BLOCKS, augment=False, seed=SEED\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, collate_fn=collate_sparse, pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, collate_fn=collate_sparse, pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"‚úì Train: {len(train_dataset)} structures, {len(train_loader)} batches\")\n",
        "print(f\"‚úì Val: {len(val_dataset)} structures, {len(val_loader)} batches\")\n",
        "\n",
        "# Create model\n",
        "model = SparseStructureTransformer(\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    n_encoder_layers=N_ENCODER_LAYERS,\n",
        "    n_decoder_layers=N_DECODER_LAYERS,\n",
        "    n_heads=N_HEADS,\n",
        "    num_latent_codes=NUM_LATENT_CODES,\n",
        "    vq_num_embeddings=VQ_NUM_EMBEDDINGS,\n",
        "    dropout=DROPOUT,\n",
        ").to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"‚úì Model created: {num_params:,} parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 7. Training Functions - Loss Computation and Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7.1 Helper Functions and Category Classification\n",
        "# ============================================================\n",
        "\n",
        "def get_category(block_name):\n",
        "    \"\"\"Categorize a block by its shape/type for detailed metrics.\"\"\"\n",
        "    name = block_name.replace(\"minecraft:\", \"\").split(\"[\")[0].lower()\n",
        "    if \"stair\" in name: return \"stairs\"\n",
        "    if \"slab\" in name: return \"slabs\"\n",
        "    if \"door\" in name: return \"doors\"\n",
        "    if \"fence\" in name: return \"fences\"\n",
        "    if \"wall\" in name and \"_wall\" in name: return \"walls\"\n",
        "    if \"planks\" in name: return \"planks\"\n",
        "    if \"log\" in name or \"wood\" in name: return \"logs\"\n",
        "    if \"glass\" in name: return \"glass\"\n",
        "    if \"wool\" in name: return \"wool\"\n",
        "    if \"concrete\" in name: return \"concrete\"\n",
        "    if \"stone\" in name or \"cobble\" in name: return \"stone\"\n",
        "    if \"brick\" in name: return \"bricks\"\n",
        "    if \"button\" in name: return \"buttons\"\n",
        "    if \"torch\" in name or \"lantern\" in name: return \"lighting\"\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "def compute_loss(model, batch, all_embeddings, aux_weight=0.1):\n",
        "    \"\"\"Compute training loss with MSE for embeddings and optional classification.\"\"\"\n",
        "    positions = batch[\"positions\"].to(device)\n",
        "    embeddings = batch[\"embeddings\"].to(device)\n",
        "    block_ids = batch[\"block_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    \n",
        "    outputs = model(positions, embeddings, attention_mask)\n",
        "    pred_embeddings = outputs[\"pred_embeddings\"]\n",
        "    vq_loss = outputs[\"vq_loss\"]\n",
        "    \n",
        "    # Primary loss: Embedding MSE (with epsilon to prevent div by zero)\n",
        "    mask = attention_mask.unsqueeze(-1)\n",
        "    embed_diff = (pred_embeddings - embeddings) ** 2\n",
        "    mask_sum = mask.sum().clamp(min=1.0)  # Prevent division by zero\n",
        "    embed_loss = (embed_diff * mask).sum() / mask_sum / pred_embeddings.size(-1)\n",
        "    \n",
        "    # Check for NaN and replace with zero\n",
        "    if torch.isnan(embed_loss):\n",
        "        print(\"Warning: NaN in embed_loss, replacing with 0\")\n",
        "        embed_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
        "    if torch.isnan(vq_loss):\n",
        "        print(\"Warning: NaN in vq_loss, replacing with 0\")\n",
        "        vq_loss = torch.tensor(0.0, device=device)\n",
        "    \n",
        "    total_loss = embed_loss + vq_loss\n",
        "    \n",
        "    # Auxiliary loss: Classification via nearest neighbor\n",
        "    aux_loss = torch.tensor(0.0, device=device)\n",
        "    accuracy = torch.tensor(0.0, device=device)\n",
        "    \n",
        "    if aux_weight > 0:\n",
        "        pred_flat = pred_embeddings.view(-1, pred_embeddings.size(-1))\n",
        "        distances = torch.cdist(pred_flat, all_embeddings)\n",
        "        logits = -distances  # Negative distance as \"logit\"\n",
        "        \n",
        "        targets_flat = block_ids.view(-1)\n",
        "        mask_flat = attention_mask.view(-1)\n",
        "        \n",
        "        ce_loss = F.cross_entropy(logits, targets_flat, reduction='none')\n",
        "        mask_sum_flat = mask_flat.sum().clamp(min=1.0)\n",
        "        aux_loss = (ce_loss * mask_flat.float()).sum() / mask_sum_flat\n",
        "        \n",
        "        # Check for NaN\n",
        "        if torch.isnan(aux_loss):\n",
        "            print(\"Warning: NaN in aux_loss, replacing with 0\")\n",
        "            aux_loss = torch.tensor(0.0, device=device)\n",
        "        \n",
        "        total_loss = total_loss + aux_weight * aux_loss\n",
        "        \n",
        "        # Compute accuracy\n",
        "        with torch.no_grad():\n",
        "            preds = distances.argmin(dim=1)\n",
        "            correct = (preds == targets_flat).float()\n",
        "            accuracy = (correct * mask_flat.float()).sum() / mask_sum_flat\n",
        "    \n",
        "    return {\n",
        "        \"loss\": total_loss,\n",
        "        \"embed_loss\": embed_loss,\n",
        "        \"vq_loss\": vq_loss,\n",
        "        \"aux_loss\": aux_loss,\n",
        "        \"accuracy\": accuracy,\n",
        "    }\n",
        "\n",
        "print(\"‚úì Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7.2 Training and Validation Functions\n",
        "# ============================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, all_embeddings):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    metrics = {\"loss\": 0, \"embed_loss\": 0, \"vq_loss\": 0, \"accuracy\": 0}\n",
        "    n = 0\n",
        "    nan_batches = 0\n",
        "    \n",
        "    for batch in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        losses = compute_loss(model, batch, all_embeddings, AUX_WEIGHT)\n",
        "        \n",
        "        # Skip batch if loss is NaN\n",
        "        if torch.isnan(losses[\"loss\"]) or torch.isinf(losses[\"loss\"]):\n",
        "            nan_batches += 1\n",
        "            continue\n",
        "        \n",
        "        losses[\"loss\"].backward()\n",
        "        \n",
        "        # Check for NaN gradients\n",
        "        has_nan_grad = False\n",
        "        for param in model.parameters():\n",
        "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
        "                has_nan_grad = True\n",
        "                break\n",
        "        \n",
        "        if has_nan_grad:\n",
        "            optimizer.zero_grad()  # Clear bad gradients\n",
        "            nan_batches += 1\n",
        "            continue\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        for k in metrics:\n",
        "            metrics[k] += losses[k].item()\n",
        "        n += 1\n",
        "    \n",
        "    if nan_batches > 0:\n",
        "        print(f\"  ‚ö†Ô∏è  Skipped {nan_batches} batches with NaN\")\n",
        "    \n",
        "    if n == 0:\n",
        "        print(\"  ‚ùå All batches had NaN! Check your data/model.\")\n",
        "        return {\"loss\": float('nan'), \"embed_loss\": float('nan'), \"vq_loss\": float('nan'), \"accuracy\": 0}\n",
        "    \n",
        "    return {k: v / n for k, v in metrics.items()}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, all_embeddings, detailed=False):\n",
        "    \"\"\"Validate and compute detailed metrics.\"\"\"\n",
        "    model.eval()\n",
        "    metrics = {\"loss\": 0, \"embed_loss\": 0, \"accuracy\": 0}\n",
        "    total_blocks = 0\n",
        "    total_correct = 0\n",
        "    n = 0\n",
        "    \n",
        "    # Per-category tracking\n",
        "    category_correct = defaultdict(int)\n",
        "    category_total = defaultdict(int)\n",
        "    confusion = defaultdict(lambda: defaultdict(int))  # true_cat -> pred_cat -> count\n",
        "    \n",
        "    for batch in tqdm(loader, desc=\"Val\", leave=False):\n",
        "        positions = batch[\"positions\"].to(device)\n",
        "        embeddings = batch[\"embeddings\"].to(device)\n",
        "        block_ids = batch[\"block_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        num_blocks = batch[\"num_blocks\"]\n",
        "        \n",
        "        outputs = model(positions, embeddings, attention_mask)\n",
        "        pred_embeddings = outputs[\"pred_embeddings\"]\n",
        "        \n",
        "        # Embedding loss (with epsilon to prevent div by zero)\n",
        "        mask = attention_mask.unsqueeze(-1)\n",
        "        embed_diff = (pred_embeddings - embeddings) ** 2\n",
        "        mask_sum = mask.sum().clamp(min=1.0)\n",
        "        embed_loss = (embed_diff * mask).sum() / mask_sum / pred_embeddings.size(-1)\n",
        "        \n",
        "        # Skip if NaN\n",
        "        if torch.isnan(embed_loss):\n",
        "            continue\n",
        "            \n",
        "        metrics[\"embed_loss\"] += embed_loss.item()\n",
        "        metrics[\"loss\"] += embed_loss.item()\n",
        "        \n",
        "        # Nearest neighbor prediction\n",
        "        B, N, D = pred_embeddings.shape\n",
        "        pred_flat = pred_embeddings.view(-1, D)\n",
        "        distances = torch.cdist(pred_flat, all_embeddings)\n",
        "        pred_ids = distances.argmin(dim=1).view(B, N)\n",
        "        \n",
        "        # Accuracy\n",
        "        correct = (pred_ids == block_ids) & attention_mask\n",
        "        total_correct += correct.sum().item()\n",
        "        total_blocks += attention_mask.sum().item()\n",
        "        \n",
        "        # Per-category metrics (sample for speed)\n",
        "        for b in range(B):\n",
        "            for i in range(min(num_blocks[b].item(), 200)):\n",
        "                true_id = block_ids[b, i].item()\n",
        "                pred_id = pred_ids[b, i].item()\n",
        "                if true_id in tok2block:\n",
        "                    true_cat = get_category(tok2block[true_id])\n",
        "                    category_total[true_cat] += 1\n",
        "                    if pred_id == true_id:\n",
        "                        category_correct[true_cat] += 1\n",
        "                    if detailed and pred_id in tok2block:\n",
        "                        pred_cat = get_category(tok2block[pred_id])\n",
        "                        confusion[true_cat][pred_cat] += 1\n",
        "        \n",
        "        n += 1\n",
        "    \n",
        "    metrics[\"accuracy\"] = total_correct / max(total_blocks, 1)\n",
        "    metrics[\"loss\"] /= max(n, 1)\n",
        "    metrics[\"embed_loss\"] /= max(n, 1)\n",
        "    \n",
        "    # Category accuracy\n",
        "    cat_acc = {}\n",
        "    for cat in category_total:\n",
        "        if category_total[cat] > 0:\n",
        "            cat_acc[cat] = category_correct[cat] / category_total[cat]\n",
        "    metrics[\"category_accuracy\"] = cat_acc\n",
        "    metrics[\"category_total\"] = dict(category_total)\n",
        "    \n",
        "    if detailed:\n",
        "        metrics[\"confusion\"] = {k: dict(v) for k, v in confusion.items()}\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"‚úì Training and validation functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 8. Training - Run the Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8.1 Training Loop\n",
        "# ============================================================\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE/10)\n",
        "\n",
        "history = {\n",
        "    \"train_loss\": [], \"train_embed_loss\": [], \"train_vq_loss\": [], \"train_accuracy\": [],\n",
        "    \"val_loss\": [], \"val_accuracy\": [], \"learning_rate\": [],\n",
        "}\n",
        "best_val_acc = 0\n",
        "category_history = []  # Track category accuracy over epochs\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üöÄ SPARSE STRUCTURE TRANSFORMER TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Train\n",
        "    train_metrics = train_epoch(model, train_loader, optimizer, all_embeddings_tensor)\n",
        "    \n",
        "    # Validate\n",
        "    val_metrics = validate(model, val_loader, all_embeddings_tensor, detailed=(epoch == EPOCHS - 1))\n",
        "    \n",
        "    scheduler.step()\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    \n",
        "    # Record history\n",
        "    history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
        "    history[\"train_embed_loss\"].append(train_metrics[\"embed_loss\"])\n",
        "    history[\"train_vq_loss\"].append(train_metrics[\"vq_loss\"])\n",
        "    history[\"train_accuracy\"].append(train_metrics[\"accuracy\"])\n",
        "    history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
        "    history[\"val_accuracy\"].append(val_metrics[\"accuracy\"])\n",
        "    history[\"learning_rate\"].append(current_lr)\n",
        "    category_history.append(val_metrics[\"category_accuracy\"])\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    # Print progress\n",
        "    print(f\"\\nüìä Epoch {epoch+1}/{EPOCHS} ({epoch_time:.1f}s)\")\n",
        "    print(f\"   Train: loss={train_metrics['loss']:.4f}, acc={train_metrics['accuracy']:.2%}\")\n",
        "    print(f\"   Val:   loss={val_metrics['loss']:.4f}, acc={val_metrics['accuracy']:.2%}\")\n",
        "    \n",
        "    # Category accuracy preview\n",
        "    if val_metrics[\"category_accuracy\"]:\n",
        "        cats = sorted(val_metrics[\"category_accuracy\"].items(), key=lambda x: -x[1])[:5]\n",
        "        cat_str = \" | \".join([f\"{c}: {a:.0%}\" for c, a in cats])\n",
        "        print(f\"   Categories: {cat_str}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_metrics[\"accuracy\"] > best_val_acc:\n",
        "        best_val_acc = val_metrics[\"accuracy\"]\n",
        "        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/sparse_transformer_best.pt\")\n",
        "        print(f\"   ‚úì New best model saved!\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"‚úÖ Training complete in {total_time/60:.1f} minutes\")\n",
        "print(f\"üèÜ Best validation accuracy: {best_val_acc:.2%}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 9. Visualizations - Training Curves, Category Accuracy, Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9.1 Training Curves - Loss and Accuracy\n",
        "# ============================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Loss curves\n",
        "ax = axes[0, 0]\n",
        "ax.plot(history[\"train_loss\"], label=\"Train\", linewidth=2, color='#2196F3')\n",
        "ax.plot(history[\"val_loss\"], label=\"Val\", linewidth=2, color='#FF5722')\n",
        "ax.set_title(\"üìâ Total Loss\", fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Loss\")\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "ax = axes[0, 1]\n",
        "ax.plot([a * 100 for a in history[\"train_accuracy\"]], label=\"Train\", linewidth=2, color='#2196F3')\n",
        "ax.plot([a * 100 for a in history[\"val_accuracy\"]], label=\"Val\", linewidth=2, color='#FF5722')\n",
        "ax.axhline(y=49, color='gray', linestyle='--', alpha=0.5, label='VQ-VAE Baseline')\n",
        "ax.set_title(\"üìà Block Accuracy (%)\", fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Accuracy (%)\")\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# VQ Loss (if using)\n",
        "ax = axes[1, 0]\n",
        "if VQ_NUM_EMBEDDINGS > 0:\n",
        "    ax.plot(history[\"train_vq_loss\"], linewidth=2, color='#9C27B0')\n",
        "    ax.set_title(\"üîÆ VQ Commitment Loss\", fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, \"VQ Disabled\", ha='center', va='center', fontsize=14, color='gray')\n",
        "    ax.set_title(\"üîÆ VQ Loss\", fontsize=14, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "# Learning rate\n",
        "ax = axes[1, 1]\n",
        "ax.plot(history[\"learning_rate\"], linewidth=2, color='#4CAF50')\n",
        "ax.set_title(\"üìö Learning Rate Schedule\", fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Learning Rate\")\n",
        "ax.set_yscale('log')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9.2 Per-Category Accuracy Bar Chart\n",
        "# ============================================================\n",
        "\n",
        "# Load best model for final evaluation\n",
        "model.load_state_dict(torch.load(f\"{OUTPUT_DIR}/sparse_transformer_best.pt\"))\n",
        "final_metrics = validate(model, val_loader, all_embeddings_tensor, detailed=True)\n",
        "\n",
        "# Prepare data for bar chart\n",
        "categories = sorted(final_metrics[\"category_accuracy\"].keys())\n",
        "accuracies = [final_metrics[\"category_accuracy\"][c] * 100 for c in categories]\n",
        "totals = [final_metrics[\"category_total\"].get(c, 0) for c in categories]\n",
        "\n",
        "# Sort by accuracy\n",
        "sorted_data = sorted(zip(categories, accuracies, totals), key=lambda x: -x[1])\n",
        "categories, accuracies, totals = zip(*sorted_data)\n",
        "\n",
        "# Color code by performance\n",
        "colors = []\n",
        "for acc in accuracies:\n",
        "    if acc >= 70: colors.append('#4CAF50')  # Green - good\n",
        "    elif acc >= 50: colors.append('#FF9800')  # Orange - decent\n",
        "    elif acc >= 30: colors.append('#FF5722')  # Red-orange - poor\n",
        "    else: colors.append('#F44336')  # Red - bad\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "bars = ax.barh(range(len(categories)), accuracies, color=colors, edgecolor='white', linewidth=0.5)\n",
        "\n",
        "# Add labels\n",
        "for i, (cat, acc, total) in enumerate(zip(categories, accuracies, totals)):\n",
        "    ax.text(acc + 1, i, f\"{acc:.1f}% ({total:,})\", va='center', fontsize=10)\n",
        "\n",
        "ax.set_yticks(range(len(categories)))\n",
        "ax.set_yticklabels(categories)\n",
        "ax.set_xlabel(\"Accuracy (%)\", fontsize=12)\n",
        "ax.set_title(\"üìä Per-Category Reconstruction Accuracy\", fontsize=14, fontweight='bold')\n",
        "ax.axvline(x=49, color='gray', linestyle='--', alpha=0.7, label='VQ-VAE Baseline (49%)')\n",
        "ax.axvline(x=final_metrics[\"accuracy\"]*100, color='blue', linestyle='-', alpha=0.7, label=f'Overall ({final_metrics[\"accuracy\"]*100:.1f}%)')\n",
        "ax.legend(loc='lower right')\n",
        "ax.set_xlim(0, 105)\n",
        "ax.grid(True, axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/category_accuracy.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüèÜ Overall Accuracy: {final_metrics['accuracy']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9.3 VQ-VAE vs Sparse Transformer Comparison\n",
        "# ============================================================\n",
        "\n",
        "# Critical categories that VQ-VAE failed on\n",
        "critical_cats = ['stairs', 'slabs', 'doors', 'fences', 'walls', 'buttons', 'logs', 'planks']\n",
        "vqvae_baseline = {'stairs': 0, 'slabs': 0, 'doors': 0, 'fences': 0, 'walls': 0, \n",
        "                  'buttons': 0, 'logs': 10, 'planks': 15}  # Approximate VQ-VAE performance\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(critical_cats))\n",
        "width = 0.35\n",
        "\n",
        "# VQ-VAE baseline\n",
        "vqvae_acc = [vqvae_baseline.get(c, 0) for c in critical_cats]\n",
        "bars1 = ax.bar(x - width/2, vqvae_acc, width, label='VQ-VAE (Baseline)', color='#B0BEC5', edgecolor='white')\n",
        "\n",
        "# Sparse Transformer\n",
        "sparse_acc = [final_metrics[\"category_accuracy\"].get(c, 0) * 100 for c in critical_cats]\n",
        "bars2 = ax.bar(x + width/2, sparse_acc, width, label='Sparse Transformer', color='#4CAF50', edgecolor='white')\n",
        "\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('üÜö Critical Category Comparison: VQ-VAE vs Sparse Transformer', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(critical_cats, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 100)\n",
        "ax.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars1, vqvae_acc):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.0f}%', \n",
        "            ha='center', va='bottom', fontsize=9, color='gray')\n",
        "for bar, val in zip(bars2, sparse_acc):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.0f}%', \n",
        "            ha='center', va='bottom', fontsize=9, color='#2E7D32', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/vqvae_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Summary table\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìã CRITICAL CATEGORY COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Category':<12} {'VQ-VAE':<12} {'Sparse Trans.':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 60)\n",
        "for cat in critical_cats:\n",
        "    vq = vqvae_baseline.get(cat, 0)\n",
        "    sp = final_metrics[\"category_accuracy\"].get(cat, 0) * 100\n",
        "    imp = sp - vq\n",
        "    print(f\"{cat:<12} {vq:>8.1f}%    {sp:>8.1f}%       +{imp:>6.1f}%\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 10. Testing - Test on a Real Structure\n",
        "\n",
        "This section shows how to test the model on an individual Minecraft structure. The model:\n",
        "1. Takes a sparse structure (positions + block embeddings)\n",
        "2. Encodes it to a latent representation\n",
        "3. Decodes back to predicted block embeddings\n",
        "4. Matches each prediction to the nearest block in the vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10.1 Test on a Single Structure\n",
        "# ============================================================\n",
        "\n",
        "def test_single_structure(model, structure_path, embeddings, all_embeddings_tensor, tok2block):\n",
        "    \"\"\"\n",
        "    Test the model on a single structure and visualize the results.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained SparseStructureTransformer\n",
        "        structure_path: Path to .h5 structure file\n",
        "        embeddings: Block2Vec embeddings numpy array\n",
        "        all_embeddings_tensor: Embeddings on GPU\n",
        "        tok2block: Token to block name mapping\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with reconstruction metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Load structure\n",
        "    with h5py.File(structure_path, 'r') as f:\n",
        "        key = list(f.keys())[0]\n",
        "        structure = f[key][:].astype(np.int64)\n",
        "    \n",
        "    # Extract sparse representation\n",
        "    non_air_mask = ~np.isin(structure, list(AIR_TOKENS))\n",
        "    positions = np.argwhere(non_air_mask).astype(np.float32)\n",
        "    block_ids = structure[non_air_mask]\n",
        "    block_embeddings = embeddings[block_ids]\n",
        "    \n",
        "    print(f\"üì¶ Structure: {structure_path}\")\n",
        "    print(f\"   Shape: {structure.shape}\")\n",
        "    print(f\"   Non-air blocks: {len(block_ids)}\")\n",
        "    print(f\"   Air percentage: {(1 - len(block_ids) / structure.size) * 100:.1f}%\")\n",
        "    \n",
        "    # Convert to tensors\n",
        "    positions_t = torch.from_numpy(positions).float().unsqueeze(0).to(device)\n",
        "    embeddings_t = torch.from_numpy(block_embeddings).float().unsqueeze(0).to(device)\n",
        "    block_ids_t = torch.from_numpy(block_ids).long().unsqueeze(0).to(device)\n",
        "    attention_mask = torch.ones(1, len(block_ids), dtype=torch.bool, device=device)\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(positions_t, embeddings_t, attention_mask)\n",
        "        pred_embeddings = outputs[\"pred_embeddings\"]\n",
        "    \n",
        "    # Predict blocks via nearest neighbor\n",
        "    pred_flat = pred_embeddings.view(-1, pred_embeddings.size(-1))\n",
        "    distances = torch.cdist(pred_flat, all_embeddings_tensor)\n",
        "    pred_ids = distances.argmin(dim=1).cpu().numpy()\n",
        "    \n",
        "    # Compute metrics\n",
        "    correct = (pred_ids == block_ids)\n",
        "    accuracy = correct.mean()\n",
        "    \n",
        "    # Category breakdown\n",
        "    category_correct = defaultdict(int)\n",
        "    category_total = defaultdict(int)\n",
        "    \n",
        "    for i, (true_id, pred_id) in enumerate(zip(block_ids, pred_ids)):\n",
        "        if true_id in tok2block:\n",
        "            cat = get_category(tok2block[true_id])\n",
        "            category_total[cat] += 1\n",
        "            if true_id == pred_id:\n",
        "                category_correct[cat] += 1\n",
        "    \n",
        "    print(f\"\\nüìä Results:\")\n",
        "    print(f\"   Overall Accuracy: {accuracy:.2%}\")\n",
        "    print(f\"   Correct blocks: {correct.sum()} / {len(block_ids)}\")\n",
        "    \n",
        "    print(f\"\\n   Per-category:\")\n",
        "    for cat in sorted(category_total.keys()):\n",
        "        if category_total[cat] > 0:\n",
        "            acc = category_correct[cat] / category_total[cat]\n",
        "            print(f\"     {cat:<12}: {acc:.2%} ({category_correct[cat]}/{category_total[cat]})\")\n",
        "    \n",
        "    # Show some examples of predictions\n",
        "    print(f\"\\n   Sample predictions (first 10 blocks):\")\n",
        "    for i in range(min(10, len(block_ids))):\n",
        "        true_name = tok2block.get(int(block_ids[i]), \"unknown\")\n",
        "        pred_name = tok2block.get(int(pred_ids[i]), \"unknown\")\n",
        "        status = \"‚úì\" if block_ids[i] == pred_ids[i] else \"‚úó\"\n",
        "        true_short = true_name.replace(\"minecraft:\", \"\").split(\"[\")[0]\n",
        "        pred_short = pred_name.replace(\"minecraft:\", \"\").split(\"[\")[0]\n",
        "        print(f\"     {status} True: {true_short:<20} ‚Üí Pred: {pred_short}\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"correct\": correct.sum(),\n",
        "        \"total\": len(block_ids),\n",
        "        \"category_accuracy\": {c: category_correct[c]/category_total[c] for c in category_total},\n",
        "    }\n",
        "\n",
        "\n",
        "# Test on a random validation structure\n",
        "val_files = list(Path(VAL_DIR).glob(\"*.h5\"))\n",
        "if val_files:\n",
        "    test_file = random.choice(val_files)\n",
        "    result = test_single_structure(model, test_file, all_embeddings, all_embeddings_tensor, tok2block)\n",
        "else:\n",
        "    print(\"No validation files found to test on.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 11. Save Results - Export Model and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 11.1 Save All Results\n",
        "# ============================================================\n",
        "\n",
        "# Prepare results dictionary\n",
        "results = {\n",
        "    \"best_val_accuracy\": best_val_acc,\n",
        "    \"final_val_accuracy\": final_metrics[\"accuracy\"],\n",
        "    \"category_accuracy\": final_metrics[\"category_accuracy\"],\n",
        "    \"category_total\": final_metrics.get(\"category_total\", {}),\n",
        "    \"training_time_minutes\": total_time / 60,\n",
        "    \"num_epochs\": EPOCHS,\n",
        "    \"history\": history,\n",
        "    \"config\": {\n",
        "        \"embed_dim\": EMBED_DIM,\n",
        "        \"hidden_dim\": HIDDEN_DIM,\n",
        "        \"n_encoder_layers\": N_ENCODER_LAYERS,\n",
        "        \"n_decoder_layers\": N_DECODER_LAYERS,\n",
        "        \"n_heads\": N_HEADS,\n",
        "        \"num_latent_codes\": NUM_LATENT_CODES,\n",
        "        \"vq_num_embeddings\": VQ_NUM_EMBEDDINGS,\n",
        "        \"dropout\": DROPOUT,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"aux_weight\": AUX_WEIGHT,\n",
        "        \"max_blocks\": MAX_BLOCKS,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save results JSON\n",
        "with open(f\"{OUTPUT_DIR}/sparse_transformer_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Copy outputs to Google Drive\n",
        "import shutil\n",
        "drive_output = f\"{DRIVE_BASE}/outputs/sparse_transformer_v1\"\n",
        "os.makedirs(drive_output, exist_ok=True)\n",
        "\n",
        "for fname in [\"sparse_transformer_best.pt\", \"sparse_transformer_results.json\", \n",
        "              \"training_curves.png\", \"category_accuracy.png\", \"vqvae_comparison.png\"]:\n",
        "    src = f\"{OUTPUT_DIR}/{fname}\"\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, drive_output)\n",
        "        print(f\"‚úì Copied {fname} to Drive\")\n",
        "\n",
        "print(f\"\\nüìÅ Results saved to: {drive_output}\")\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ SPARSE STRUCTURE TRANSFORMER - FINAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìä Best Validation Accuracy: {best_val_acc:.2%}\")\n",
        "print(f\"‚è±Ô∏è  Training Time: {total_time/60:.1f} minutes\")\n",
        "print(f\"üî¢ Model Parameters: {num_params:,}\")\n",
        "print(f\"\\nüìà Improvement over VQ-VAE Baseline (49%):\")\n",
        "improvement = (best_val_acc - 0.49) / 0.49 * 100\n",
        "print(f\"   +{improvement:.1f}% relative improvement\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
