{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VQ-VAE Embedding Validation - Google Colab Version\n\n## Setup Instructions\n\n1. Upload the following to your Google Drive under `minecraft_ai/`:\n   - `splits/train/` - Training H5 files\n   - `splits/val/` - Validation H5 files  \n   - `tok2block.json` - Vocabulary file\n   - `block_embeddings.npy` - V1 embeddings\n   - `block_embeddings_v3.npy` - V3 embeddings\n\n2. Run all cells in order\n\n3. **Keep the browser tab open** (but you can minimize it)\n\n## Idle Disconnect Prevention\n\nCell 0 includes a JavaScript keep-alive script that pings every 60 seconds to prevent Colab from disconnecting due to inactivity. You still need to keep the tab open, but you don't need to interact with it.\n\n## Configuration\n\n- **Epochs**: 12\n- **Seeds**: 1 (single run per embedding, ~5 hours total)\n- **Structure weight**: 10x (to counter 80% air imbalance)\n- **Embeddings**: V1, V3, Random\n\n## Runtime Limits\n\nGoogle doesn't publish exact limits. Commonly reported:\n- Free tier: ~12 hours max, ~90 min idle timeout\n- Check official FAQ: https://research.google.com/colaboratory/faq.html"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 0: Mount Google Drive & Prevent Idle Disconnect\n# ============================================================\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Verify mount\nimport os\nprint(\"Drive mounted. Checking for minecraft_ai folder...\")\ndrive_path = \"/content/drive/MyDrive/minecraft_ai\"\nif os.path.exists(drive_path):\n    print(f\"Found: {drive_path}\")\n    print(f\"Contents: {os.listdir(drive_path)}\")\nelse:\n    print(f\"WARNING: {drive_path} not found!\")\n    print(\"Please create this folder and upload your data.\")\n\n# --- Prevent Colab from disconnecting due to idle timeout ---\n# This injects JavaScript that clicks the connect button every 60 seconds\nfrom IPython.display import display, Javascript\n\nkeep_alive_js = Javascript('''\nfunction KeepAlive() {\n    console.log(\"Keep-alive ping at \" + new Date().toLocaleTimeString());\n    // Try multiple selectors for different Colab versions\n    var buttons = document.querySelectorAll(\"colab-connect-button, colab-toolbar-button#connect\");\n    buttons.forEach(function(btn) {\n        if (btn) btn.click();\n    });\n}\n// Run every 60 seconds\nsetInterval(KeepAlive, 60000);\nconsole.log(\"Keep-alive script activated - will ping every 60 seconds\");\n''')\n\ndisplay(keep_alive_js)\nprint(\"\\nKeep-alive script activated to prevent idle disconnect.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Configuration\n",
    "# ============================================================\n",
    "\n",
    "# === Google Drive Paths ===\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/minecraft_ai\"\n",
    "\n",
    "DATA_DIR = f\"{DRIVE_BASE}/splits/train\"\n",
    "VAL_DIR = f\"{DRIVE_BASE}/splits/val\"\n",
    "VOCAB_PATH = f\"{DRIVE_BASE}/tok2block.json\"\n",
    "\n",
    "# Embeddings\n",
    "V1_EMBEDDINGS_PATH = f\"{DRIVE_BASE}/block_embeddings.npy\"\n",
    "V3_EMBEDDINGS_PATH = f\"{DRIVE_BASE}/block_embeddings_v3.npy\"\n",
    "\n",
    "# Output to Drive so results persist\n",
    "OUTPUT_DIR = f\"{DRIVE_BASE}/output/vqvae_validation\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Model Architecture ===\n",
    "HIDDEN_DIMS = [64, 128, 256]\n",
    "LATENT_DIM = 256\n",
    "NUM_CODEBOOK_ENTRIES = 512\n",
    "\n",
    "# === VQ-VAE Settings ===\n",
    "COMMITMENT_COST = 0.5\n",
    "EMA_DECAY = 0.99\n",
    "DEAD_CODE_THRESHOLD = 2\n",
    "STRUCTURE_WEIGHT = 10.0\n",
    "\n",
    "# === Training ===\n",
    "EPOCHS = 12\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 3e-4\n",
    "USE_AMP = True\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# === Statistical Rigor ===\n",
    "NUM_SEEDS = 1\n",
    "BASE_SEEDS = [42]\n",
    "\n",
    "# === Diagnostics ===\n",
    "NUM_RECONSTRUCTION_SAMPLES = 5\n",
    "TOP_K_BLOCKS = 20\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"Colab Validation Configuration:\")\n",
    "print(f\"  Drive base: {DRIVE_BASE}\")\n",
    "print(f\"  Embeddings: V1, V3, Random\")\n",
    "print(f\"  Seeds: {BASE_SEEDS} ({NUM_SEEDS} runs per embedding)\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "\n",
    "# Verify paths exist\n",
    "for path, name in [(DATA_DIR, \"train\"), (VAL_DIR, \"val\"), (VOCAB_PATH, \"vocab\"), \n",
    "                   (V1_EMBEDDINGS_PATH, \"V1 emb\"), (V3_EMBEDDINGS_PATH, \"V3 emb\")]:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  {name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load Vocabulary and Categorize Blocks\n",
    "# ============================================================\n",
    "\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "block2tok = {v: k for k, v in tok2block.items()}\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Find air tokens\n",
    "AIR_TOKENS = set()\n",
    "for tok, block in tok2block.items():\n",
    "    if 'air' in block.lower() and 'stair' not in block.lower():\n",
    "        AIR_TOKENS.add(tok)\n",
    "        print(f\"  Air token: {tok} = {block}\")\n",
    "\n",
    "AIR_TOKENS_LIST = sorted(AIR_TOKENS)\n",
    "AIR_TOKENS_TENSOR = torch.tensor(AIR_TOKENS_LIST, dtype=torch.long)\n",
    "\n",
    "# Categorize blocks for analysis\n",
    "BLOCK_CATEGORIES = {\n",
    "    'wood': [], 'stone': [], 'metal': [], 'glass': [],\n",
    "    'wool': [], 'concrete': [], 'terracotta': [],\n",
    "    'stairs': [], 'slabs': [], 'walls': [], 'fences': [],\n",
    "    'doors': [], 'plants': [], 'redstone': [], 'light': [],\n",
    "    'other': []\n",
    "}\n",
    "\n",
    "for tok, block in tok2block.items():\n",
    "    if tok in AIR_TOKENS:\n",
    "        continue\n",
    "    block_lower = block.lower()\n",
    "    \n",
    "    categorized = False\n",
    "    if any(w in block_lower for w in ['oak', 'spruce', 'birch', 'jungle', 'acacia', 'dark_oak', 'mangrove', 'cherry', 'bamboo', 'crimson', 'warped']):\n",
    "        if 'stair' in block_lower:\n",
    "            BLOCK_CATEGORIES['stairs'].append(tok)\n",
    "        elif 'slab' in block_lower:\n",
    "            BLOCK_CATEGORIES['slabs'].append(tok)\n",
    "        elif 'fence' in block_lower:\n",
    "            BLOCK_CATEGORIES['fences'].append(tok)\n",
    "        elif 'door' in block_lower:\n",
    "            BLOCK_CATEGORIES['doors'].append(tok)\n",
    "        else:\n",
    "            BLOCK_CATEGORIES['wood'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['stone', 'cobble', 'brick', 'granite', 'diorite', 'andesite', 'deepslate', 'tuff']):\n",
    "        if 'stair' in block_lower:\n",
    "            BLOCK_CATEGORIES['stairs'].append(tok)\n",
    "        elif 'slab' in block_lower:\n",
    "            BLOCK_CATEGORIES['slabs'].append(tok)\n",
    "        elif 'wall' in block_lower:\n",
    "            BLOCK_CATEGORIES['walls'].append(tok)\n",
    "        else:\n",
    "            BLOCK_CATEGORIES['stone'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['iron', 'gold', 'copper', 'netherite']):\n",
    "        BLOCK_CATEGORIES['metal'].append(tok)\n",
    "        categorized = True\n",
    "    elif 'glass' in block_lower:\n",
    "        BLOCK_CATEGORIES['glass'].append(tok)\n",
    "        categorized = True\n",
    "    elif 'wool' in block_lower:\n",
    "        BLOCK_CATEGORIES['wool'].append(tok)\n",
    "        categorized = True\n",
    "    elif 'concrete' in block_lower:\n",
    "        BLOCK_CATEGORIES['concrete'].append(tok)\n",
    "        categorized = True\n",
    "    elif 'terracotta' in block_lower:\n",
    "        BLOCK_CATEGORIES['terracotta'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['redstone', 'piston', 'observer', 'comparator', 'repeater', 'lever', 'button']):\n",
    "        BLOCK_CATEGORIES['redstone'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['torch', 'lantern', 'lamp', 'glowstone', 'sea_lantern', 'shroomlight']):\n",
    "        BLOCK_CATEGORIES['light'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['flower', 'grass', 'fern', 'leaves', 'sapling', 'vine', 'moss']):\n",
    "        BLOCK_CATEGORIES['plants'].append(tok)\n",
    "        categorized = True\n",
    "    \n",
    "    if not categorized:\n",
    "        BLOCK_CATEGORIES['other'].append(tok)\n",
    "\n",
    "print(\"\\nBlock categories:\")\n",
    "for cat, toks in BLOCK_CATEGORIES.items():\n",
    "    print(f\"  {cat}: {len(toks)} blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Load Embeddings\n",
    "# ============================================================\n",
    "\n",
    "# V1: Skip-gram embeddings\n",
    "v1_embeddings = np.load(V1_EMBEDDINGS_PATH).astype(np.float32)\n",
    "print(f\"V1: {v1_embeddings.shape}\")\n",
    "\n",
    "# V3: Compositional embeddings\n",
    "v3_embeddings = np.load(V3_EMBEDDINGS_PATH).astype(np.float32)\n",
    "print(f\"V3: {v3_embeddings.shape}\")\n",
    "\n",
    "# Random (will create fresh per seed)\n",
    "v1_std = v1_embeddings.std()\n",
    "print(f\"V1 std (for random init): {v1_std:.4f}\")\n",
    "\n",
    "EMBEDDINGS_BASE = {\n",
    "    'V1': {'embeddings': v1_embeddings, 'dim': 32},\n",
    "    'V3': {'embeddings': v3_embeddings, 'dim': 40},\n",
    "}\n",
    "\n",
    "print(f\"\\nEmbeddings to test: {list(EMBEDDINGS_BASE.keys())} + Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Dataset\n",
    "# ============================================================\n",
    "\n",
    "class VQVAEDataset(Dataset):\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        if not self.h5_files:\n",
    "            raise ValueError(f\"No H5 files in {data_dir}\")\n",
    "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.h5_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_files[idx], 'r') as f:\n",
    "            key = list(f.keys())[0]\n",
    "            structure = f[key][:].astype(np.int64)\n",
    "        return torch.from_numpy(structure).long(), idx\n",
    "\n",
    "train_dataset = VQVAEDataset(DATA_DIR)\n",
    "val_dataset = VQVAEDataset(VAL_DIR)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Enhanced VQ-VAE with Diagnostics\n",
    "# ============================================================\n",
    "\n",
    "class ResidualBlock3D(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(channels)\n",
    "        self.bn2 = nn.BatchNorm3d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_codes, latent_dim, commitment_cost=0.5, ema_decay=0.99, dead_threshold=2):\n",
    "        super().__init__()\n",
    "        self.num_codes = num_codes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.ema_decay = ema_decay\n",
    "        self.dead_threshold = dead_threshold\n",
    "        \n",
    "        self.register_buffer('codebook', torch.randn(num_codes, latent_dim))\n",
    "        self.codebook.data.uniform_(-1/num_codes, 1/num_codes)\n",
    "        self.register_buffer('ema_cluster_size', torch.zeros(num_codes))\n",
    "        self.register_buffer('ema_embed_sum', torch.zeros(num_codes, latent_dim))\n",
    "        self.register_buffer('code_usage', torch.zeros(num_codes))\n",
    "        self.register_buffer('code_usage_total', torch.zeros(num_codes))\n",
    "    \n",
    "    def reset_epoch_stats(self):\n",
    "        self.code_usage.zero_()\n",
    "    \n",
    "    def get_usage_fraction(self):\n",
    "        return (self.code_usage > 0).float().mean().item()\n",
    "    \n",
    "    def get_perplexity(self):\n",
    "        if self.code_usage.sum() == 0:\n",
    "            return 0.0\n",
    "        probs = self.code_usage / self.code_usage.sum()\n",
    "        probs = probs[probs > 0]\n",
    "        entropy = -(probs * probs.log()).sum()\n",
    "        return entropy.exp().item()\n",
    "    \n",
    "    def get_entropy(self):\n",
    "        if self.code_usage.sum() == 0:\n",
    "            return 0.0\n",
    "        probs = self.code_usage / self.code_usage.sum()\n",
    "        probs = probs[probs > 0]\n",
    "        return -(probs * probs.log()).sum().item()\n",
    "    \n",
    "    def forward(self, z_e):\n",
    "        z_e_perm = z_e.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        flat = z_e_perm.view(-1, self.latent_dim)\n",
    "        \n",
    "        # Cast to float32 for codebook operations (AMP fix)\n",
    "        flat_f32 = flat.float()\n",
    "        codebook_f32 = self.codebook.float()\n",
    "        \n",
    "        d = (flat_f32.pow(2).sum(1, keepdim=True) \n",
    "             + codebook_f32.pow(2).sum(1) \n",
    "             - 2 * flat_f32 @ codebook_f32.t())\n",
    "        \n",
    "        indices = d.argmin(dim=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in indices.unique():\n",
    "                count = (indices == idx).sum()\n",
    "                self.code_usage[idx] += count\n",
    "                self.code_usage_total[idx] += count\n",
    "        \n",
    "        z_q_flat = self.codebook[indices]\n",
    "        z_q_perm = z_q_flat.view(z_e_perm.shape)\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                encodings = F.one_hot(indices, self.num_codes).float()\n",
    "                batch_size = encodings.sum(0)\n",
    "                \n",
    "                self.ema_cluster_size = self.ema_decay * self.ema_cluster_size + (1 - self.ema_decay) * batch_size\n",
    "                batch_sum = encodings.t() @ flat_f32\n",
    "                self.ema_embed_sum = self.ema_decay * self.ema_embed_sum + (1 - self.ema_decay) * batch_sum\n",
    "                \n",
    "                n = self.ema_cluster_size.sum()\n",
    "                smoothed = (self.ema_cluster_size + 1e-5) / (n + self.num_codes * 1e-5) * n\n",
    "                self.codebook.data = self.ema_embed_sum / smoothed.unsqueeze(1)\n",
    "                \n",
    "                # Dead code reset - use float32\n",
    "                dead = batch_size < self.dead_threshold\n",
    "                if dead.any() and flat_f32.size(0) > 0:\n",
    "                    n_dead = dead.sum().item()\n",
    "                    rand_idx = torch.randint(0, flat_f32.size(0), (n_dead,), device=flat_f32.device)\n",
    "                    self.codebook.data[dead] = flat_f32[rand_idx]\n",
    "                    self.ema_cluster_size[dead] = 1\n",
    "                    self.ema_embed_sum[dead] = flat_f32[rand_idx]\n",
    "        \n",
    "        commitment_loss = F.mse_loss(z_e_perm, z_q_perm.detach())\n",
    "        vq_loss = self.commitment_cost * commitment_loss\n",
    "        \n",
    "        z_q_st = z_e_perm + (z_q_perm - z_e_perm).detach()\n",
    "        z_q = z_q_st.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        \n",
    "        return z_q, vq_loss, indices.view(z_e_perm.shape[:-1])\n",
    "\n",
    "\n",
    "class DiagnosticVQVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dims, latent_dim, num_codes,\n",
    "                 commitment_cost, ema_decay, dead_threshold, pretrained_emb):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.block_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.block_emb.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
    "        self.block_emb.weight.requires_grad = False\n",
    "        \n",
    "        enc = []\n",
    "        in_ch = emb_dim\n",
    "        for h in hidden_dims:\n",
    "            enc.extend([\n",
    "                nn.Conv3d(in_ch, h, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h),\n",
    "                nn.ReLU(inplace=True),\n",
    "                ResidualBlock3D(h),\n",
    "            ])\n",
    "            in_ch = h\n",
    "        enc.append(nn.Conv3d(in_ch, latent_dim, 3, padding=1))\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "        \n",
    "        self.quantizer = VectorQuantizerEMA(num_codes, latent_dim, commitment_cost, ema_decay, dead_threshold)\n",
    "        \n",
    "        dec = []\n",
    "        in_ch = latent_dim\n",
    "        for h in reversed(hidden_dims):\n",
    "            dec.extend([\n",
    "                ResidualBlock3D(in_ch),\n",
    "                nn.ConvTranspose3d(in_ch, h, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ])\n",
    "            in_ch = h\n",
    "        dec.append(nn.Conv3d(in_ch, vocab_size, 3, padding=1))\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "    \n",
    "    def forward(self, block_ids):\n",
    "        x = self.block_emb(block_ids).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, vq_loss, indices = self.quantizer(z_e)\n",
    "        logits = self.decoder(z_q)\n",
    "        return {'logits': logits, 'vq_loss': vq_loss, 'indices': indices, 'z_e': z_e}\n",
    "    \n",
    "    def compute_loss_with_diagnostics(self, block_ids, air_tokens, structure_weight):\n",
    "        out = self(block_ids)\n",
    "        \n",
    "        logits = out['logits'].permute(0, 2, 3, 4, 1).contiguous()\n",
    "        logits_flat = logits.view(-1, self.vocab_size)\n",
    "        targets_flat = block_ids.view(-1)\n",
    "        \n",
    "        ce = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
    "        \n",
    "        air_dev = air_tokens.to(targets_flat.device)\n",
    "        is_air = torch.isin(targets_flat, air_dev)\n",
    "        is_struct = ~is_air\n",
    "        \n",
    "        weights = torch.ones_like(ce)\n",
    "        weights[is_struct] = structure_weight\n",
    "        recon_loss = (weights * ce).sum() / weights.sum()\n",
    "        \n",
    "        total_loss = recon_loss + out['vq_loss']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds_flat = logits_flat.argmax(-1)\n",
    "            correct = (preds_flat == targets_flat).float()\n",
    "            \n",
    "            acc = correct.mean()\n",
    "            air_acc = correct[is_air].mean() if is_air.any() else torch.tensor(0.0)\n",
    "            struct_acc = correct[is_struct].mean() if is_struct.any() else torch.tensor(0.0)\n",
    "            air_pct = is_air.float().mean()\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'vq_loss': out['vq_loss'],\n",
    "            'accuracy': acc,\n",
    "            'air_accuracy': air_acc,\n",
    "            'struct_accuracy': struct_acc,\n",
    "            'air_pct': air_pct,\n",
    "            'predictions': preds_flat,\n",
    "            'targets': targets_flat,\n",
    "            'is_structure': is_struct,\n",
    "            'indices': out['indices'],\n",
    "        }\n",
    "\n",
    "print(\"DiagnosticVQVAE defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Diagnostic Tracking Classes\n",
    "# ============================================================\n",
    "\n",
    "class PerBlockTracker:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.correct = np.zeros(vocab_size)\n",
    "        self.total = np.zeros(vocab_size)\n",
    "    \n",
    "    def update(self, preds, targets):\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        targets_np = targets.cpu().numpy()\n",
    "        for t in np.unique(targets_np):\n",
    "            mask = targets_np == t\n",
    "            self.total[t] += mask.sum()\n",
    "            self.correct[t] += (preds_np[mask] == t).sum()\n",
    "    \n",
    "    def get_accuracy(self, min_count=10):\n",
    "        accs = {}\n",
    "        for t in range(len(self.total)):\n",
    "            if self.total[t] >= min_count:\n",
    "                accs[t] = self.correct[t] / self.total[t]\n",
    "        return accs\n",
    "    \n",
    "    def get_top_bottom(self, tok2block, k=20, min_count=10):\n",
    "        accs = self.get_accuracy(min_count)\n",
    "        sorted_toks = sorted(accs.keys(), key=lambda t: accs[t], reverse=True)\n",
    "        \n",
    "        top = [(tok2block.get(t, f'tok_{t}'), accs[t], int(self.total[t])) for t in sorted_toks[:k]]\n",
    "        bottom = [(tok2block.get(t, f'tok_{t}'), accs[t], int(self.total[t])) for t in sorted_toks[-k:]]\n",
    "        return top, bottom\n",
    "\n",
    "\n",
    "class ConfusionTracker:\n",
    "    def __init__(self, max_track=1000):\n",
    "        self.confusions = Counter()\n",
    "        self.max_track = max_track\n",
    "    \n",
    "    def update(self, preds, targets, is_structure):\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        targets_np = targets.cpu().numpy()\n",
    "        is_struct_np = is_structure.cpu().numpy()\n",
    "        \n",
    "        wrong = (preds_np != targets_np) & is_struct_np\n",
    "        for t, p in zip(targets_np[wrong], preds_np[wrong]):\n",
    "            self.confusions[(int(t), int(p))] += 1\n",
    "    \n",
    "    def get_top_confusions(self, tok2block, k=20):\n",
    "        top = self.confusions.most_common(k)\n",
    "        result = []\n",
    "        for (t, p), count in top:\n",
    "            true_name = tok2block.get(t, f'tok_{t}')\n",
    "            pred_name = tok2block.get(p, f'tok_{p}')\n",
    "            result.append((true_name, pred_name, count))\n",
    "        return result\n",
    "\n",
    "\n",
    "class GradientTracker:\n",
    "    def __init__(self):\n",
    "        self.encoder_norms = []\n",
    "        self.decoder_norms = []\n",
    "    \n",
    "    def compute_norms(self, model):\n",
    "        enc_norm = 0.0\n",
    "        dec_norm = 0.0\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                norm = param.grad.norm().item()\n",
    "                if 'encoder' in name:\n",
    "                    enc_norm += norm ** 2\n",
    "                elif 'decoder' in name:\n",
    "                    dec_norm += norm ** 2\n",
    "        \n",
    "        self.encoder_norms.append(enc_norm ** 0.5)\n",
    "        self.decoder_norms.append(dec_norm ** 0.5)\n",
    "\n",
    "print(\"Diagnostic trackers defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Training Functions\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch_diagnostic(model, loader, optimizer, scaler, device, air_tokens, \n",
    "                           structure_weight, grad_tracker=None):\n",
    "    model.train()\n",
    "    model.quantizer.reset_epoch_stats()\n",
    "    \n",
    "    metrics = {'loss': 0, 'recon': 0, 'vq': 0, 'acc': 0, 'air_acc': 0, 'struct_acc': 0, 'air_pct': 0}\n",
    "    n = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (batch, _) in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss_with_diagnostics(batch, air_tokens, structure_weight)\n",
    "            loss = out['loss'] / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            if grad_tracker:\n",
    "                grad_tracker.compute_norms(model)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['recon'] += out['recon_loss'].item()\n",
    "        metrics['vq'] += out['vq_loss'].item()\n",
    "        metrics['acc'] += out['accuracy'].item()\n",
    "        metrics['air_acc'] += out['air_accuracy'].item()\n",
    "        metrics['struct_acc'] += out['struct_accuracy'].item()\n",
    "        metrics['air_pct'] += out['air_pct'].item()\n",
    "        n += 1\n",
    "    \n",
    "    if len(loader) % GRAD_ACCUM_STEPS != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    metrics['codebook_usage'] = model.quantizer.get_usage_fraction()\n",
    "    metrics['perplexity'] = model.quantizer.get_perplexity()\n",
    "    metrics['entropy'] = model.quantizer.get_entropy()\n",
    "    \n",
    "    return {k: v/n if k not in ['codebook_usage', 'perplexity', 'entropy'] else v for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_diagnostic(model, loader, device, air_tokens, structure_weight,\n",
    "                        block_tracker=None, confusion_tracker=None):\n",
    "    model.eval()\n",
    "    model.quantizer.reset_epoch_stats()\n",
    "    \n",
    "    metrics = {'loss': 0, 'recon': 0, 'acc': 0, 'air_acc': 0, 'struct_acc': 0, 'air_pct': 0}\n",
    "    n = 0\n",
    "    \n",
    "    for batch, _ in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss_with_diagnostics(batch, air_tokens, structure_weight)\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['recon'] += out['recon_loss'].item()\n",
    "        metrics['acc'] += out['accuracy'].item()\n",
    "        metrics['air_acc'] += out['air_accuracy'].item()\n",
    "        metrics['struct_acc'] += out['struct_accuracy'].item()\n",
    "        metrics['air_pct'] += out['air_pct'].item()\n",
    "        n += 1\n",
    "        \n",
    "        if block_tracker:\n",
    "            block_tracker.update(out['predictions'], out['targets'])\n",
    "        if confusion_tracker:\n",
    "            confusion_tracker.update(out['predictions'], out['targets'], out['is_structure'])\n",
    "    \n",
    "    metrics['codebook_usage'] = model.quantizer.get_usage_fraction()\n",
    "    metrics['perplexity'] = model.quantizer.get_perplexity()\n",
    "    metrics['entropy'] = model.quantizer.get_entropy()\n",
    "    \n",
    "    return {k: v/n if k not in ['codebook_usage', 'perplexity', 'entropy'] else v for k, v in metrics.items()}\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Visualization Functions\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_reconstruction_samples(model, dataset, device, air_tokens, n_samples=5):\n",
    "    model.eval()\n",
    "    samples = []\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), n_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        original, _ = dataset[idx]\n",
    "        original = original.unsqueeze(0).to(device)\n",
    "        \n",
    "        out = model(original)\n",
    "        recon = out['logits'].argmax(dim=1).squeeze(0)\n",
    "        \n",
    "        orig_flat = original.view(-1)\n",
    "        recon_flat = recon.view(-1)\n",
    "        \n",
    "        air_dev = air_tokens.to(device)\n",
    "        is_struct = ~torch.isin(orig_flat, air_dev)\n",
    "        \n",
    "        overall_acc = (orig_flat == recon_flat).float().mean().item()\n",
    "        struct_acc = (orig_flat[is_struct] == recon_flat[is_struct]).float().mean().item() if is_struct.any() else 0\n",
    "        \n",
    "        samples.append({\n",
    "            'original': original.squeeze(0).cpu().numpy(),\n",
    "            'reconstructed': recon.cpu().numpy(),\n",
    "            'overall_acc': overall_acc,\n",
    "            'struct_acc': struct_acc,\n",
    "            'idx': idx,\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def visualize_slice(original, reconstructed, z_slice, tok2block, air_tokens, ax_orig, ax_recon):\n",
    "    orig_slice = original[:, :, z_slice]\n",
    "    recon_slice = reconstructed[:, :, z_slice]\n",
    "    \n",
    "    def get_color(tok):\n",
    "        if tok in air_tokens:\n",
    "            return [1, 1, 1]\n",
    "        np.random.seed(tok)\n",
    "        return np.random.rand(3)\n",
    "    \n",
    "    orig_rgb = np.zeros((*orig_slice.shape, 3))\n",
    "    recon_rgb = np.zeros((*recon_slice.shape, 3))\n",
    "    \n",
    "    for i in range(orig_slice.shape[0]):\n",
    "        for j in range(orig_slice.shape[1]):\n",
    "            orig_rgb[i, j] = get_color(orig_slice[i, j])\n",
    "            recon_rgb[i, j] = get_color(recon_slice[i, j])\n",
    "    \n",
    "    ax_orig.imshow(orig_rgb)\n",
    "    ax_orig.set_title('Original')\n",
    "    ax_orig.axis('off')\n",
    "    \n",
    "    ax_recon.imshow(recon_rgb)\n",
    "    ax_recon.set_title('Reconstructed')\n",
    "    ax_recon.axis('off')\n",
    "\n",
    "\n",
    "def compute_category_accuracy(block_tracker, categories, air_tokens):\n",
    "    per_block = block_tracker.get_accuracy(min_count=5)\n",
    "    \n",
    "    cat_acc = {}\n",
    "    for cat, toks in categories.items():\n",
    "        valid_toks = [t for t in toks if t in per_block and t not in air_tokens]\n",
    "        if valid_toks:\n",
    "            cat_acc[cat] = np.mean([per_block[t] for t in valid_toks])\n",
    "    \n",
    "    return cat_acc\n",
    "\n",
    "print(\"Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Main Experiment Runner\n",
    "# ============================================================\n",
    "\n",
    "def run_full_experiment(name, embeddings, emb_dim, seed, air_tokens, train_ds, val_ds):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{name} (seed={seed}, dim={emb_dim})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, generator=g)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    model = DiagnosticVQVAE(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        emb_dim=emb_dim,\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        num_codes=NUM_CODEBOOK_ENTRIES,\n",
    "        commitment_cost=COMMITMENT_COST,\n",
    "        ema_decay=EMA_DECAY,\n",
    "        dead_threshold=DEAD_CODE_THRESHOLD,\n",
    "        pretrained_emb=embeddings,\n",
    "    ).to(device)\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable:,}\")\n",
    "    \n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "    \n",
    "    grad_tracker = GradientTracker()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_recon': [], 'train_vq': [],\n",
    "        'train_acc': [], 'train_air_acc': [], 'train_struct_acc': [],\n",
    "        'train_codebook_usage': [], 'train_perplexity': [], 'train_entropy': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_air_acc': [], 'val_struct_acc': [],\n",
    "        'val_codebook_usage': [], 'val_perplexity': [], 'val_entropy': [],\n",
    "        'encoder_grad_norm': [], 'decoder_grad_norm': [],\n",
    "    }\n",
    "    \n",
    "    best_struct_acc = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_m = train_epoch_diagnostic(model, train_loader, optimizer, scaler, device,\n",
    "                                         air_tokens, STRUCTURE_WEIGHT, grad_tracker)\n",
    "        val_m = validate_diagnostic(model, val_loader, device, air_tokens, STRUCTURE_WEIGHT)\n",
    "        \n",
    "        history['train_loss'].append(train_m['loss'])\n",
    "        history['train_recon'].append(train_m['recon'])\n",
    "        history['train_vq'].append(train_m['vq'])\n",
    "        history['train_acc'].append(train_m['acc'])\n",
    "        history['train_air_acc'].append(train_m['air_acc'])\n",
    "        history['train_struct_acc'].append(train_m['struct_acc'])\n",
    "        history['train_codebook_usage'].append(train_m['codebook_usage'])\n",
    "        history['train_perplexity'].append(train_m['perplexity'])\n",
    "        history['train_entropy'].append(train_m['entropy'])\n",
    "        history['val_loss'].append(val_m['loss'])\n",
    "        history['val_acc'].append(val_m['acc'])\n",
    "        history['val_air_acc'].append(val_m['air_acc'])\n",
    "        history['val_struct_acc'].append(val_m['struct_acc'])\n",
    "        history['val_codebook_usage'].append(val_m['codebook_usage'])\n",
    "        history['val_perplexity'].append(val_m['perplexity'])\n",
    "        history['val_entropy'].append(val_m['entropy'])\n",
    "        \n",
    "        if grad_tracker.encoder_norms:\n",
    "            history['encoder_grad_norm'].append(np.mean(grad_tracker.encoder_norms[-100:]))\n",
    "            history['decoder_grad_norm'].append(np.mean(grad_tracker.decoder_norms[-100:]))\n",
    "        \n",
    "        if val_m['struct_acc'] > best_struct_acc:\n",
    "            best_struct_acc = val_m['struct_acc']\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d} | Struct: {train_m['struct_acc']:.1%} | \"\n",
    "              f\"Val: {val_m['struct_acc']:.1%} | Perp: {train_m['perplexity']:.0f} | \"\n",
    "              f\"CB: {train_m['codebook_usage']:.1%}\")\n",
    "    \n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    print(\"\\nFinal detailed validation...\")\n",
    "    block_tracker = PerBlockTracker(VOCAB_SIZE)\n",
    "    confusion_tracker = ConfusionTracker()\n",
    "    \n",
    "    final_val = validate_diagnostic(model, val_loader, device, air_tokens, STRUCTURE_WEIGHT,\n",
    "                                    block_tracker, confusion_tracker)\n",
    "    \n",
    "    top_blocks, bottom_blocks = block_tracker.get_top_bottom(tok2block, k=TOP_K_BLOCKS)\n",
    "    top_confusions = confusion_tracker.get_top_confusions(tok2block, k=20)\n",
    "    category_acc = compute_category_accuracy(block_tracker, BLOCK_CATEGORIES, AIR_TOKENS)\n",
    "    \n",
    "    samples = get_reconstruction_samples(model, val_ds, device, air_tokens, NUM_RECONSTRUCTION_SAMPLES)\n",
    "    \n",
    "    results = {\n",
    "        'name': name,\n",
    "        'seed': seed,\n",
    "        'emb_dim': emb_dim,\n",
    "        'best_struct_acc': best_struct_acc,\n",
    "        'final_struct_acc': final_val['struct_acc'],\n",
    "        'final_val_loss': final_val['loss'],\n",
    "        'final_perplexity': final_val['perplexity'],\n",
    "        'final_codebook_usage': final_val['codebook_usage'],\n",
    "        'training_time': train_time,\n",
    "        'history': history,\n",
    "        'top_blocks': top_blocks,\n",
    "        'bottom_blocks': bottom_blocks,\n",
    "        'top_confusions': top_confusions,\n",
    "        'category_accuracy': category_acc,\n",
    "        'reconstruction_samples': samples,\n",
    "    }\n",
    "    \n",
    "    del model, optimizer, scaler\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Experiment runner defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Run All Experiments\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VQ-VAE EMBEDDING VALIDATION - COLAB\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Seeds: {BASE_SEEDS}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Embeddings: V1, V3, Random\")\n",
    "print()\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for emb_name, emb_data in EMBEDDINGS_BASE.items():\n",
    "    all_results[emb_name] = []\n",
    "    for seed in BASE_SEEDS:\n",
    "        result = run_full_experiment(\n",
    "            name=emb_name,\n",
    "            embeddings=emb_data['embeddings'],\n",
    "            emb_dim=emb_data['dim'],\n",
    "            seed=seed,\n",
    "            air_tokens=AIR_TOKENS_TENSOR,\n",
    "            train_ds=train_dataset,\n",
    "            val_ds=val_dataset,\n",
    "        )\n",
    "        all_results[emb_name].append(result)\n",
    "\n",
    "all_results['Random'] = []\n",
    "for seed in BASE_SEEDS:\n",
    "    np.random.seed(seed)\n",
    "    rand_emb = np.random.randn(VOCAB_SIZE, 32).astype(np.float32) * v1_std\n",
    "    \n",
    "    result = run_full_experiment(\n",
    "        name='Random',\n",
    "        embeddings=rand_emb,\n",
    "        emb_dim=32,\n",
    "        seed=seed,\n",
    "        air_tokens=AIR_TOKENS_TENSOR,\n",
    "        train_ds=train_dataset,\n",
    "        val_ds=val_dataset,\n",
    "    )\n",
    "    all_results['Random'].append(result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Results Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_stats = {}\n",
    "for name, runs in all_results.items():\n",
    "    struct_accs = [r['best_struct_acc'] for r in runs]\n",
    "    perplexities = [r['final_perplexity'] for r in runs]\n",
    "    cb_usages = [r['final_codebook_usage'] for r in runs]\n",
    "    \n",
    "    summary_stats[name] = {\n",
    "        'struct_acc_mean': np.mean(struct_accs),\n",
    "        'struct_acc_std': np.std(struct_accs),\n",
    "        'struct_acc_all': struct_accs,\n",
    "        'perplexity_mean': np.mean(perplexities),\n",
    "        'cb_usage_mean': np.mean(cb_usages),\n",
    "    }\n",
    "\n",
    "sorted_names = sorted(summary_stats.keys(), key=lambda x: summary_stats[x]['struct_acc_mean'], reverse=True)\n",
    "\n",
    "print(\"\\n{:<10} {:>15} {:>15} {:>15}\".format(\"Embedding\", \"Struct Acc\", \"Perplexity\", \"CB Usage\"))\n",
    "print(\"-\"*60)\n",
    "\n",
    "for name in sorted_names:\n",
    "    s = summary_stats[name]\n",
    "    print(\"{:<10} {:>15.1%} {:>15.0f} {:>15.1%}\".format(\n",
    "        name, s['struct_acc_mean'], s['perplexity_mean'], s['cb_usage_mean']\n",
    "    ))\n",
    "\n",
    "winner = sorted_names[0]\n",
    "print(f\"\\nWINNER: {winner} with {summary_stats[winner]['struct_acc_mean']:.1%} structure accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Save Results\n",
    "# ============================================================\n",
    "\n",
    "# Save summary\n",
    "summary_output = {}\n",
    "for emb_name, emb_stats in summary_stats.items():\n",
    "    summary_output[emb_name] = {\n",
    "        'struct_acc_mean': float(emb_stats['struct_acc_mean']),\n",
    "        'struct_acc_std': float(emb_stats['struct_acc_std']),\n",
    "        'perplexity_mean': float(emb_stats['perplexity_mean']),\n",
    "        'cb_usage_mean': float(emb_stats['cb_usage_mean']),\n",
    "    }\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/validation_summary.json\", 'w') as f:\n",
    "    json.dump(summary_output, f, indent=2)\n",
    "\n",
    "# Save detailed results\n",
    "detailed_output = {}\n",
    "for emb_name, runs in all_results.items():\n",
    "    detailed_output[emb_name] = []\n",
    "    for run in runs:\n",
    "        run_data = {\n",
    "            'seed': run['seed'],\n",
    "            'best_struct_acc': float(run['best_struct_acc']),\n",
    "            'final_struct_acc': float(run['final_struct_acc']),\n",
    "            'training_time': float(run['training_time']),\n",
    "            'history': {k: [float(x) for x in v] for k, v in run['history'].items()},\n",
    "            'category_accuracy': {k: float(v) for k, v in run['category_accuracy'].items()},\n",
    "        }\n",
    "        detailed_output[emb_name].append(run_data)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/validation_detailed.json\", 'w') as f:\n",
    "    json.dump(detailed_output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}/\")\n",
    "print(f\"  - validation_summary.json\")\n",
    "print(f\"  - validation_detailed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: Training Curves Plot\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = {'V1': 'blue', 'V3': 'purple', 'Random': 'red'}\n",
    "names = ['V1', 'V3', 'Random']\n",
    "\n",
    "# Structure accuracy\n",
    "ax = axes[0, 0]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][0]['history']['val_struct_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Validation Structure Accuracy', fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax = axes[0, 1]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][0]['history']['train_loss'], label=name, color=colors[name])\n",
    "ax.set_title('Training Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Codebook usage\n",
    "ax = axes[1, 0]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][0]['history']['train_codebook_usage'], label=name, color=colors[name])\n",
    "ax.set_title('Codebook Usage')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Final comparison bar chart\n",
    "ax = axes[1, 1]\n",
    "means = [summary_stats[name]['struct_acc_mean'] for name in names]\n",
    "bar_colors = [colors[name] for name in names]\n",
    "bars = ax.bar(names, means, color=bar_colors, edgecolor='black')\n",
    "ax.set_title('Final Structure Accuracy')\n",
    "ax.set_ylabel('Accuracy')\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{mean:.1%}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to {OUTPUT_DIR}/training_curves.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}