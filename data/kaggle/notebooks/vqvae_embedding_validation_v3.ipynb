{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE Embedding Validation V3 - Comprehensive Diagnostics\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. **Determine which embedding is best** for VQ-VAE structure reconstruction\n",
    "2. **Understand WHY** one embedding outperforms another\n",
    "3. **Identify failure modes** - what's breaking and how to fix it\n",
    "4. **Guide future improvements** - what aspects of embeddings/VQ-VAE matter most\n",
    "\n",
    "## Embeddings Tested\n",
    "\n",
    "| Embedding | Dim | Description |\n",
    "|-----------|-----|-------------|\n",
    "| V1 | 32 | Skip-gram (co-occurrence based) |\n",
    "| V3 | 40 | Compositional (material + shape + properties) |\n",
    "| Random | 32 | Random baseline |\n",
    "\n",
    "**Note:** V2 (hybrid) dropped - consistently worst performer in previous runs.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **Epochs**: 12\n",
    "- **Seeds**: 1 (single run per embedding, ~5 hours total)\n",
    "- **Structure weight**: 10x (to counter 80% air imbalance)\n",
    "\n",
    "## Improvements over V2 Validation\n",
    "\n",
    "| Category | V2 Validation | V3 (this notebook) |\n",
    "|----------|---------------|--------------------|\n",
    "| **VQ-VAE** | Broken config | **EMA codebook, dead code reset** |\n",
    "| **Per-block analysis** | None | **Top/bottom blocks, confusion matrix** |\n",
    "| **Latent space** | Usage only | **Perplexity, entropy, code analysis** |\n",
    "| **Visual** | None | **Reconstruction samples** |\n",
    "| **Training dynamics** | Basic | **Gradient norms, loss components** |\n",
    "| **Block-type breakdown** | None | **Accuracy by block category** |\n",
    "\n",
    "## Key Questions to Answer\n",
    "\n",
    "1. Does V1 or V3 perform better?\n",
    "2. Which block types benefit most from good embeddings?\n",
    "3. What do the codebook entries represent?\n",
    "4. Are structures visually recognizable after reconstruction?\n",
    "5. What's the main failure mode - wrong block type, or wrong position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Configuration\n",
    "# ============================================================\n",
    "\n",
    "# === Data Paths ===\n",
    "DATA_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/train\"\n",
    "VAL_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/val\"\n",
    "VOCAB_PATH = \"/kaggle/input/minecraft-schematics/tok2block.json\"\n",
    "\n",
    "# Embeddings (V2 dropped - consistently worst performer)\n",
    "V1_EMBEDDINGS_PATH = \"/kaggle/input/minecraft-embeddings/block_embeddings.npy\"\n",
    "V3_EMBEDDINGS_PATH = \"/kaggle/input/minecraft-embeddings-v3/block_embeddings_v3.npy\"\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# === Model Architecture ===\n",
    "HIDDEN_DIMS = [64, 128, 256]\n",
    "LATENT_DIM = 256\n",
    "NUM_CODEBOOK_ENTRIES = 512\n",
    "\n",
    "# === VQ-VAE Settings ===\n",
    "COMMITMENT_COST = 0.5\n",
    "EMA_DECAY = 0.99\n",
    "DEAD_CODE_THRESHOLD = 2\n",
    "STRUCTURE_WEIGHT = 10.0\n",
    "\n",
    "# === Training ===\n",
    "EPOCHS = 12\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 3e-4\n",
    "USE_AMP = True\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# === Statistical Rigor ===\n",
    "NUM_SEEDS = 1  # Single seed to fit within Kaggle time limits\n",
    "BASE_SEEDS = [42]\n",
    "\n",
    "# === Diagnostics ===\n",
    "NUM_RECONSTRUCTION_SAMPLES = 5  # Visual samples to save\n",
    "TOP_K_BLOCKS = 20  # Track top/bottom K blocks by accuracy\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"V3 Validation Configuration:\")\n",
    "print(f\"  Embeddings: V1, V3, Random (V2 dropped)\")\n",
    "print(f\"  Seeds: {BASE_SEEDS} ({NUM_SEEDS} runs per embedding)\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Architecture: {HIDDEN_DIMS}\")\n",
    "print(f\"  Structure weight: {STRUCTURE_WEIGHT}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load Vocabulary and Categorize Blocks\n",
    "# ============================================================\n",
    "\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "block2tok = {v: k for k, v in tok2block.items()}\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Find air tokens\n",
    "AIR_TOKENS = set()\n",
    "for tok, block in tok2block.items():\n",
    "    if 'air' in block.lower() and 'stair' not in block.lower():\n",
    "        AIR_TOKENS.add(tok)\n",
    "        print(f\"  Air token: {tok} = {block}\")\n",
    "\n",
    "AIR_TOKENS_LIST = sorted(AIR_TOKENS)\n",
    "AIR_TOKENS_TENSOR = torch.tensor(AIR_TOKENS_LIST, dtype=torch.long)\n",
    "\n",
    "# Categorize blocks for analysis\n",
    "BLOCK_CATEGORIES = {\n",
    "    'wood': [], 'stone': [], 'metal': [], 'glass': [],\n",
    "    'wool': [], 'concrete': [], 'terracotta': [],\n",
    "    'stairs': [], 'slabs': [], 'walls': [], 'fences': [],\n",
    "    'doors': [], 'plants': [], 'redstone': [], 'light': [],\n",
    "    'other': []\n",
    "}\n",
    "\n",
    "for tok, block in tok2block.items():\n",
    "    if tok in AIR_TOKENS:\n",
    "        continue\n",
    "    block_lower = block.lower()\n",
    "    \n",
    "    categorized = False\n",
    "    if any(w in block_lower for w in ['oak', 'spruce', 'birch', 'jungle', 'acacia', 'dark_oak', 'mangrove', 'cherry', 'bamboo', 'crimson', 'warped']):\n",
    "        if 'stair' in block_lower:\n",
    "            BLOCK_CATEGORIES['stairs'].append(tok)\n",
    "        elif 'slab' in block_lower:\n",
    "            BLOCK_CATEGORIES['slabs'].append(tok)\n",
    "        elif 'fence' in block_lower:\n",
    "            BLOCK_CATEGORIES['fences'].append(tok)\n",
    "        elif 'door' in block_lower:\n",
    "            BLOCK_CATEGORIES['doors'].append(tok)\n",
    "        else:\n",
    "            BLOCK_CATEGORIES['wood'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['stone', 'cobble', 'brick', 'granite', 'diorite', 'andesite', 'deepslate', 'tuff']):\n",
    "        if 'stair' in block_lower:\n",
    "            BLOCK_CATEGORIES['stairs'].append(tok)\n",
    "        elif 'slab' in block_lower:\n",
    "            BLOCK_CATEGORIES['slabs'].append(tok)\n",
    "        elif 'wall' in block_lower:\n",
    "            BLOCK_CATEGORIES['walls'].append(tok)\n",
    "        else:\n",
    "            BLOCK_CATEGORIES['stone'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['iron', 'gold', 'copper', 'netherite']):\n",
    "        BLOCK_CATEGORIES['metal'].append(tok)\n",
    "        categorized = True\n",
    "    elif 'glass' in block_lower:\n",
    "        BLOCK_CATEGORIES['glass'].append(tok)\n",
    "        categorized = True\n",
    "    elif 'wool' in block_lower:\n",
    "        BLOCK_CATEGORIES['wool'].append(tok)\n",
    "        categorized = True\n",
    "    elif 'concrete' in block_lower:\n",
    "        BLOCK_CATEGORIES['concrete'].append(tok)\n",
    "        categorized = True\n",
    "    elif 'terracotta' in block_lower:\n",
    "        BLOCK_CATEGORIES['terracotta'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['redstone', 'piston', 'observer', 'comparator', 'repeater', 'lever', 'button']):\n",
    "        BLOCK_CATEGORIES['redstone'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['torch', 'lantern', 'lamp', 'glowstone', 'sea_lantern', 'shroomlight']):\n",
    "        BLOCK_CATEGORIES['light'].append(tok)\n",
    "        categorized = True\n",
    "    elif any(w in block_lower for w in ['flower', 'grass', 'fern', 'leaves', 'sapling', 'vine', 'moss']):\n",
    "        BLOCK_CATEGORIES['plants'].append(tok)\n",
    "        categorized = True\n",
    "    \n",
    "    if not categorized:\n",
    "        BLOCK_CATEGORIES['other'].append(tok)\n",
    "\n",
    "print(\"\\nBlock categories:\")\n",
    "for cat, toks in BLOCK_CATEGORIES.items():\n",
    "    print(f\"  {cat}: {len(toks)} blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Load Embeddings\n",
    "# ============================================================\n",
    "\n",
    "# V1: Skip-gram embeddings\n",
    "v1_embeddings = np.load(V1_EMBEDDINGS_PATH).astype(np.float32)\n",
    "print(f\"V1: {v1_embeddings.shape}\")\n",
    "\n",
    "# V3: Compositional embeddings\n",
    "v3_embeddings = np.load(V3_EMBEDDINGS_PATH).astype(np.float32)\n",
    "print(f\"V3: {v3_embeddings.shape}\")\n",
    "\n",
    "# Random (will create fresh per seed)\n",
    "v1_std = v1_embeddings.std()\n",
    "print(f\"V1 std (for random init): {v1_std:.4f}\")\n",
    "\n",
    "# V2 dropped - consistently worst performer in previous validation\n",
    "EMBEDDINGS_BASE = {\n",
    "    'V1': {'embeddings': v1_embeddings, 'dim': 32},\n",
    "    'V3': {'embeddings': v3_embeddings, 'dim': 40},\n",
    "}\n",
    "\n",
    "print(f\"\\nEmbeddings to test: {list(EMBEDDINGS_BASE.keys())} + Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Dataset\n",
    "# ============================================================\n",
    "\n",
    "class VQVAEDataset(Dataset):\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        if not self.h5_files:\n",
    "            raise ValueError(f\"No H5 files in {data_dir}\")\n",
    "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.h5_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_files[idx], 'r') as f:\n",
    "            key = list(f.keys())[0]\n",
    "            structure = f[key][:].astype(np.int64)\n",
    "        return torch.from_numpy(structure).long(), idx\n",
    "\n",
    "train_dataset = VQVAEDataset(DATA_DIR)\n",
    "val_dataset = VQVAEDataset(VAL_DIR)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Enhanced VQ-VAE with Diagnostics\n",
    "# ============================================================\n",
    "\n",
    "class ResidualBlock3D(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(channels)\n",
    "        self.bn2 = nn.BatchNorm3d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_codes, latent_dim, commitment_cost=0.5, ema_decay=0.99, dead_threshold=2):\n",
    "        super().__init__()\n",
    "        self.num_codes = num_codes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.ema_decay = ema_decay\n",
    "        self.dead_threshold = dead_threshold\n",
    "        \n",
    "        self.register_buffer('codebook', torch.randn(num_codes, latent_dim))\n",
    "        self.codebook.data.uniform_(-1/num_codes, 1/num_codes)\n",
    "        self.register_buffer('ema_cluster_size', torch.zeros(num_codes))\n",
    "        self.register_buffer('ema_embed_sum', torch.zeros(num_codes, latent_dim))\n",
    "        self.register_buffer('code_usage', torch.zeros(num_codes))\n",
    "        self.register_buffer('code_usage_total', torch.zeros(num_codes))\n",
    "    \n",
    "    def reset_epoch_stats(self):\n",
    "        self.code_usage.zero_()\n",
    "    \n",
    "    def get_usage_fraction(self):\n",
    "        return (self.code_usage > 0).float().mean().item()\n",
    "    \n",
    "    def get_perplexity(self):\n",
    "        \"\"\"Effective number of codes used (2^entropy).\"\"\"\n",
    "        if self.code_usage.sum() == 0:\n",
    "            return 0.0\n",
    "        probs = self.code_usage / self.code_usage.sum()\n",
    "        probs = probs[probs > 0]  # Avoid log(0)\n",
    "        entropy = -(probs * probs.log()).sum()\n",
    "        return entropy.exp().item()\n",
    "    \n",
    "    def get_entropy(self):\n",
    "        \"\"\"Entropy of code distribution (higher = more uniform).\"\"\"\n",
    "        if self.code_usage.sum() == 0:\n",
    "            return 0.0\n",
    "        probs = self.code_usage / self.code_usage.sum()\n",
    "        probs = probs[probs > 0]\n",
    "        return -(probs * probs.log()).sum().item()\n",
    "    \n",
    "    def forward(self, z_e):\n",
    "        z_e_perm = z_e.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        flat = z_e_perm.view(-1, self.latent_dim)\n",
    "        \n",
    "        # Cast to float32 for distance computation (avoid AMP half precision issues)\n",
    "        flat_f32 = flat.float()\n",
    "        codebook_f32 = self.codebook.float()\n",
    "        \n",
    "        d = (flat_f32.pow(2).sum(1, keepdim=True) \n",
    "             + codebook_f32.pow(2).sum(1) \n",
    "             - 2 * flat_f32 @ codebook_f32.t())\n",
    "        \n",
    "        indices = d.argmin(dim=1)\n",
    "        \n",
    "        # Track usage\n",
    "        with torch.no_grad():\n",
    "            for idx in indices.unique():\n",
    "                count = (indices == idx).sum()\n",
    "                self.code_usage[idx] += count\n",
    "                self.code_usage_total[idx] += count\n",
    "        \n",
    "        z_q_flat = self.codebook[indices]\n",
    "        z_q_perm = z_q_flat.view(z_e_perm.shape)\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                encodings = F.one_hot(indices, self.num_codes).float()\n",
    "                batch_size = encodings.sum(0)\n",
    "                \n",
    "                self.ema_cluster_size = self.ema_decay * self.ema_cluster_size + (1 - self.ema_decay) * batch_size\n",
    "                batch_sum = encodings.t() @ flat_f32  # Use float32\n",
    "                self.ema_embed_sum = self.ema_decay * self.ema_embed_sum + (1 - self.ema_decay) * batch_sum\n",
    "                \n",
    "                n = self.ema_cluster_size.sum()\n",
    "                smoothed = (self.ema_cluster_size + 1e-5) / (n + self.num_codes * 1e-5) * n\n",
    "                self.codebook.data = self.ema_embed_sum / smoothed.unsqueeze(1)\n",
    "                \n",
    "                # Dead code reset - use float32 to match codebook dtype\n",
    "                dead = batch_size < self.dead_threshold\n",
    "                if dead.any() and flat_f32.size(0) > 0:\n",
    "                    n_dead = dead.sum().item()\n",
    "                    rand_idx = torch.randint(0, flat_f32.size(0), (n_dead,), device=flat_f32.device)\n",
    "                    self.codebook.data[dead] = flat_f32[rand_idx]\n",
    "                    self.ema_cluster_size[dead] = 1\n",
    "                    self.ema_embed_sum[dead] = flat_f32[rand_idx]\n",
    "        \n",
    "        commitment_loss = F.mse_loss(z_e_perm, z_q_perm.detach())\n",
    "        vq_loss = self.commitment_cost * commitment_loss\n",
    "        \n",
    "        z_q_st = z_e_perm + (z_q_perm - z_e_perm).detach()\n",
    "        z_q = z_q_st.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        \n",
    "        return z_q, vq_loss, indices.view(z_e_perm.shape[:-1])\n",
    "\n",
    "\n",
    "class DiagnosticVQVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dims, latent_dim, num_codes,\n",
    "                 commitment_cost, ema_decay, dead_threshold, pretrained_emb):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Frozen embeddings\n",
    "        self.block_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.block_emb.weight.data.copy_(torch.from_numpy(pretrained_emb))\n",
    "        self.block_emb.weight.requires_grad = False\n",
    "        \n",
    "        # Encoder\n",
    "        enc = []\n",
    "        in_ch = emb_dim\n",
    "        for h in hidden_dims:\n",
    "            enc.extend([\n",
    "                nn.Conv3d(in_ch, h, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h),\n",
    "                nn.ReLU(inplace=True),\n",
    "                ResidualBlock3D(h),\n",
    "            ])\n",
    "            in_ch = h\n",
    "        enc.append(nn.Conv3d(in_ch, latent_dim, 3, padding=1))\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "        \n",
    "        # Quantizer\n",
    "        self.quantizer = VectorQuantizerEMA(num_codes, latent_dim, commitment_cost, ema_decay, dead_threshold)\n",
    "        \n",
    "        # Decoder\n",
    "        dec = []\n",
    "        in_ch = latent_dim\n",
    "        for h in reversed(hidden_dims):\n",
    "            dec.extend([\n",
    "                ResidualBlock3D(in_ch),\n",
    "                nn.ConvTranspose3d(in_ch, h, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ])\n",
    "            in_ch = h\n",
    "        dec.append(nn.Conv3d(in_ch, vocab_size, 3, padding=1))\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "    \n",
    "    def forward(self, block_ids):\n",
    "        x = self.block_emb(block_ids).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, vq_loss, indices = self.quantizer(z_e)\n",
    "        logits = self.decoder(z_q)\n",
    "        return {'logits': logits, 'vq_loss': vq_loss, 'indices': indices, 'z_e': z_e}\n",
    "    \n",
    "    def compute_loss_with_diagnostics(self, block_ids, air_tokens, structure_weight):\n",
    "        out = self(block_ids)\n",
    "        \n",
    "        logits = out['logits'].permute(0, 2, 3, 4, 1).contiguous()\n",
    "        logits_flat = logits.view(-1, self.vocab_size)\n",
    "        targets_flat = block_ids.view(-1)\n",
    "        \n",
    "        # Per-element CE\n",
    "        ce = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
    "        \n",
    "        # Structure weighting\n",
    "        air_dev = air_tokens.to(targets_flat.device)\n",
    "        is_air = torch.isin(targets_flat, air_dev)\n",
    "        is_struct = ~is_air\n",
    "        \n",
    "        weights = torch.ones_like(ce)\n",
    "        weights[is_struct] = structure_weight\n",
    "        recon_loss = (weights * ce).sum() / weights.sum()\n",
    "        \n",
    "        total_loss = recon_loss + out['vq_loss']\n",
    "        \n",
    "        # Detailed metrics\n",
    "        with torch.no_grad():\n",
    "            preds_flat = logits_flat.argmax(-1)\n",
    "            correct = (preds_flat == targets_flat).float()\n",
    "            \n",
    "            acc = correct.mean()\n",
    "            air_acc = correct[is_air].mean() if is_air.any() else torch.tensor(0.0)\n",
    "            struct_acc = correct[is_struct].mean() if is_struct.any() else torch.tensor(0.0)\n",
    "            air_pct = is_air.float().mean()\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'vq_loss': out['vq_loss'],\n",
    "            'accuracy': acc,\n",
    "            'air_accuracy': air_acc,\n",
    "            'struct_accuracy': struct_acc,\n",
    "            'air_pct': air_pct,\n",
    "            'predictions': preds_flat,\n",
    "            'targets': targets_flat,\n",
    "            'is_structure': is_struct,\n",
    "            'indices': out['indices'],\n",
    "        }\n",
    "\n",
    "print(\"DiagnosticVQVAE defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Diagnostic Tracking Classes\n",
    "# ============================================================\n",
    "\n",
    "class PerBlockTracker:\n",
    "    \"\"\"Track accuracy per block type.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        self.correct = np.zeros(vocab_size)\n",
    "        self.total = np.zeros(vocab_size)\n",
    "    \n",
    "    def update(self, preds, targets):\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        targets_np = targets.cpu().numpy()\n",
    "        for t in np.unique(targets_np):\n",
    "            mask = targets_np == t\n",
    "            self.total[t] += mask.sum()\n",
    "            self.correct[t] += (preds_np[mask] == t).sum()\n",
    "    \n",
    "    def get_accuracy(self, min_count=10):\n",
    "        \"\"\"Return dict of token -> accuracy for tokens with min_count samples.\"\"\"\n",
    "        accs = {}\n",
    "        for t in range(len(self.total)):\n",
    "            if self.total[t] >= min_count:\n",
    "                accs[t] = self.correct[t] / self.total[t]\n",
    "        return accs\n",
    "    \n",
    "    def get_top_bottom(self, tok2block, k=20, min_count=10):\n",
    "        \"\"\"Return top-k and bottom-k blocks by accuracy.\"\"\"\n",
    "        accs = self.get_accuracy(min_count)\n",
    "        sorted_toks = sorted(accs.keys(), key=lambda t: accs[t], reverse=True)\n",
    "        \n",
    "        top = [(tok2block.get(t, f'tok_{t}'), accs[t], int(self.total[t])) for t in sorted_toks[:k]]\n",
    "        bottom = [(tok2block.get(t, f'tok_{t}'), accs[t], int(self.total[t])) for t in sorted_toks[-k:]]\n",
    "        return top, bottom\n",
    "\n",
    "\n",
    "class ConfusionTracker:\n",
    "    \"\"\"Track what blocks get confused with what (for structure blocks only).\"\"\"\n",
    "    def __init__(self, max_track=1000):\n",
    "        self.confusions = Counter()  # (true, pred) -> count\n",
    "        self.max_track = max_track\n",
    "    \n",
    "    def update(self, preds, targets, is_structure):\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        targets_np = targets.cpu().numpy()\n",
    "        is_struct_np = is_structure.cpu().numpy()\n",
    "        \n",
    "        # Only track structure block confusions\n",
    "        wrong = (preds_np != targets_np) & is_struct_np\n",
    "        for t, p in zip(targets_np[wrong], preds_np[wrong]):\n",
    "            self.confusions[(int(t), int(p))] += 1\n",
    "    \n",
    "    def get_top_confusions(self, tok2block, k=20):\n",
    "        \"\"\"Return top-k most common confusions.\"\"\"\n",
    "        top = self.confusions.most_common(k)\n",
    "        result = []\n",
    "        for (t, p), count in top:\n",
    "            true_name = tok2block.get(t, f'tok_{t}')\n",
    "            pred_name = tok2block.get(p, f'tok_{p}')\n",
    "            result.append((true_name, pred_name, count))\n",
    "        return result\n",
    "\n",
    "\n",
    "class GradientTracker:\n",
    "    \"\"\"Track gradient norms for encoder vs decoder.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.encoder_norms = []\n",
    "        self.decoder_norms = []\n",
    "    \n",
    "    def compute_norms(self, model):\n",
    "        enc_norm = 0.0\n",
    "        dec_norm = 0.0\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                norm = param.grad.norm().item()\n",
    "                if 'encoder' in name:\n",
    "                    enc_norm += norm ** 2\n",
    "                elif 'decoder' in name:\n",
    "                    dec_norm += norm ** 2\n",
    "        \n",
    "        self.encoder_norms.append(enc_norm ** 0.5)\n",
    "        self.decoder_norms.append(dec_norm ** 0.5)\n",
    "\n",
    "\n",
    "print(\"Diagnostic trackers defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Training Functions with Diagnostics\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch_diagnostic(model, loader, optimizer, scaler, device, air_tokens, \n",
    "                           structure_weight, grad_tracker=None):\n",
    "    model.train()\n",
    "    model.quantizer.reset_epoch_stats()\n",
    "    \n",
    "    metrics = {'loss': 0, 'recon': 0, 'vq': 0, 'acc': 0, 'air_acc': 0, 'struct_acc': 0, 'air_pct': 0}\n",
    "    n = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (batch, _) in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss_with_diagnostics(batch, air_tokens, structure_weight)\n",
    "            loss = out['loss'] / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            if grad_tracker:\n",
    "                grad_tracker.compute_norms(model)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['recon'] += out['recon_loss'].item()\n",
    "        metrics['vq'] += out['vq_loss'].item()\n",
    "        metrics['acc'] += out['accuracy'].item()\n",
    "        metrics['air_acc'] += out['air_accuracy'].item()\n",
    "        metrics['struct_acc'] += out['struct_accuracy'].item()\n",
    "        metrics['air_pct'] += out['air_pct'].item()\n",
    "        n += 1\n",
    "    \n",
    "    if len(loader) % GRAD_ACCUM_STEPS != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    metrics['codebook_usage'] = model.quantizer.get_usage_fraction()\n",
    "    metrics['perplexity'] = model.quantizer.get_perplexity()\n",
    "    metrics['entropy'] = model.quantizer.get_entropy()\n",
    "    \n",
    "    return {k: v/n if k not in ['codebook_usage', 'perplexity', 'entropy'] else v for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_diagnostic(model, loader, device, air_tokens, structure_weight,\n",
    "                        block_tracker=None, confusion_tracker=None):\n",
    "    model.eval()\n",
    "    model.quantizer.reset_epoch_stats()\n",
    "    \n",
    "    metrics = {'loss': 0, 'recon': 0, 'acc': 0, 'air_acc': 0, 'struct_acc': 0, 'air_pct': 0}\n",
    "    n = 0\n",
    "    \n",
    "    for batch, _ in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss_with_diagnostics(batch, air_tokens, structure_weight)\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['recon'] += out['recon_loss'].item()\n",
    "        metrics['acc'] += out['accuracy'].item()\n",
    "        metrics['air_acc'] += out['air_accuracy'].item()\n",
    "        metrics['struct_acc'] += out['struct_accuracy'].item()\n",
    "        metrics['air_pct'] += out['air_pct'].item()\n",
    "        n += 1\n",
    "        \n",
    "        if block_tracker:\n",
    "            block_tracker.update(out['predictions'], out['targets'])\n",
    "        if confusion_tracker:\n",
    "            confusion_tracker.update(out['predictions'], out['targets'], out['is_structure'])\n",
    "    \n",
    "    metrics['codebook_usage'] = model.quantizer.get_usage_fraction()\n",
    "    metrics['perplexity'] = model.quantizer.get_perplexity()\n",
    "    metrics['entropy'] = model.quantizer.get_entropy()\n",
    "    \n",
    "    return {k: v/n if k not in ['codebook_usage', 'perplexity', 'entropy'] else v for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "print(\"Diagnostic training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Reconstruction Visualization\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_reconstruction_samples(model, dataset, device, air_tokens, n_samples=5):\n",
    "    \"\"\"Get original and reconstructed structures for visualization.\"\"\"\n",
    "    model.eval()\n",
    "    samples = []\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), n_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        original, _ = dataset[idx]\n",
    "        original = original.unsqueeze(0).to(device)\n",
    "        \n",
    "        out = model(original)\n",
    "        recon = out['logits'].argmax(dim=1).squeeze(0)\n",
    "        \n",
    "        # Compute per-structure accuracy\n",
    "        orig_flat = original.view(-1)\n",
    "        recon_flat = recon.view(-1)\n",
    "        \n",
    "        air_dev = air_tokens.to(device)\n",
    "        is_struct = ~torch.isin(orig_flat, air_dev)\n",
    "        \n",
    "        overall_acc = (orig_flat == recon_flat).float().mean().item()\n",
    "        struct_acc = (orig_flat[is_struct] == recon_flat[is_struct]).float().mean().item() if is_struct.any() else 0\n",
    "        \n",
    "        samples.append({\n",
    "            'original': original.squeeze(0).cpu().numpy(),\n",
    "            'reconstructed': recon.cpu().numpy(),\n",
    "            'overall_acc': overall_acc,\n",
    "            'struct_acc': struct_acc,\n",
    "            'idx': idx,\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def visualize_slice(original, reconstructed, z_slice, tok2block, air_tokens, ax_orig, ax_recon):\n",
    "    \"\"\"Visualize a single z-slice of original vs reconstructed.\"\"\"\n",
    "    orig_slice = original[:, :, z_slice]\n",
    "    recon_slice = reconstructed[:, :, z_slice]\n",
    "    \n",
    "    # Create color maps\n",
    "    def get_color(tok):\n",
    "        if tok in air_tokens:\n",
    "            return [1, 1, 1]  # White for air\n",
    "        # Hash-based coloring\n",
    "        np.random.seed(tok)\n",
    "        return np.random.rand(3)\n",
    "    \n",
    "    orig_rgb = np.zeros((*orig_slice.shape, 3))\n",
    "    recon_rgb = np.zeros((*recon_slice.shape, 3))\n",
    "    \n",
    "    for i in range(orig_slice.shape[0]):\n",
    "        for j in range(orig_slice.shape[1]):\n",
    "            orig_rgb[i, j] = get_color(orig_slice[i, j])\n",
    "            recon_rgb[i, j] = get_color(recon_slice[i, j])\n",
    "    \n",
    "    ax_orig.imshow(orig_rgb)\n",
    "    ax_orig.set_title('Original')\n",
    "    ax_orig.axis('off')\n",
    "    \n",
    "    ax_recon.imshow(recon_rgb)\n",
    "    ax_recon.set_title('Reconstructed')\n",
    "    ax_recon.axis('off')\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Category Accuracy Analysis\n",
    "# ============================================================\n",
    "\n",
    "def compute_category_accuracy(block_tracker, categories, air_tokens):\n",
    "    \"\"\"Compute accuracy per block category.\"\"\"\n",
    "    per_block = block_tracker.get_accuracy(min_count=5)\n",
    "    \n",
    "    cat_acc = {}\n",
    "    for cat, toks in categories.items():\n",
    "        valid_toks = [t for t in toks if t in per_block and t not in air_tokens]\n",
    "        if valid_toks:\n",
    "            cat_acc[cat] = np.mean([per_block[t] for t in valid_toks])\n",
    "    \n",
    "    return cat_acc\n",
    "\n",
    "\n",
    "print(\"Category analysis defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Main Experiment Runner\n",
    "# ============================================================\n",
    "\n",
    "def run_full_experiment(name, embeddings, emb_dim, seed, air_tokens, train_ds, val_ds):\n",
    "    \"\"\"Run one full experiment with all diagnostics.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{name} (seed={seed}, dim={emb_dim})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Set seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Data loaders (re-create with seed)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=True, generator=g)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    # Create model\n",
    "    model = DiagnosticVQVAE(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        emb_dim=emb_dim,\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        num_codes=NUM_CODEBOOK_ENTRIES,\n",
    "        commitment_cost=COMMITMENT_COST,\n",
    "        ema_decay=EMA_DECAY,\n",
    "        dead_threshold=DEAD_CODE_THRESHOLD,\n",
    "        pretrained_emb=embeddings,\n",
    "    ).to(device)\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable:,}\")\n",
    "    \n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "    \n",
    "    # Trackers\n",
    "    grad_tracker = GradientTracker()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_recon': [], 'train_vq': [],\n",
    "        'train_acc': [], 'train_air_acc': [], 'train_struct_acc': [],\n",
    "        'train_codebook_usage': [], 'train_perplexity': [], 'train_entropy': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_air_acc': [], 'val_struct_acc': [],\n",
    "        'val_codebook_usage': [], 'val_perplexity': [], 'val_entropy': [],\n",
    "        'encoder_grad_norm': [], 'decoder_grad_norm': [],\n",
    "    }\n",
    "    \n",
    "    best_struct_acc = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_m = train_epoch_diagnostic(model, train_loader, optimizer, scaler, device,\n",
    "                                         air_tokens, STRUCTURE_WEIGHT, grad_tracker)\n",
    "        val_m = validate_diagnostic(model, val_loader, device, air_tokens, STRUCTURE_WEIGHT)\n",
    "        \n",
    "        # Record\n",
    "        history['train_loss'].append(train_m['loss'])\n",
    "        history['train_recon'].append(train_m['recon'])\n",
    "        history['train_vq'].append(train_m['vq'])\n",
    "        history['train_acc'].append(train_m['acc'])\n",
    "        history['train_air_acc'].append(train_m['air_acc'])\n",
    "        history['train_struct_acc'].append(train_m['struct_acc'])\n",
    "        history['train_codebook_usage'].append(train_m['codebook_usage'])\n",
    "        history['train_perplexity'].append(train_m['perplexity'])\n",
    "        history['train_entropy'].append(train_m['entropy'])\n",
    "        history['val_loss'].append(val_m['loss'])\n",
    "        history['val_acc'].append(val_m['acc'])\n",
    "        history['val_air_acc'].append(val_m['air_acc'])\n",
    "        history['val_struct_acc'].append(val_m['struct_acc'])\n",
    "        history['val_codebook_usage'].append(val_m['codebook_usage'])\n",
    "        history['val_perplexity'].append(val_m['perplexity'])\n",
    "        history['val_entropy'].append(val_m['entropy'])\n",
    "        \n",
    "        # Gradient norms (average over accumulation steps)\n",
    "        if grad_tracker.encoder_norms:\n",
    "            history['encoder_grad_norm'].append(np.mean(grad_tracker.encoder_norms[-100:]))\n",
    "            history['decoder_grad_norm'].append(np.mean(grad_tracker.decoder_norms[-100:]))\n",
    "        \n",
    "        if val_m['struct_acc'] > best_struct_acc:\n",
    "            best_struct_acc = val_m['struct_acc']\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d} | Struct: {train_m['struct_acc']:.1%} | \"\n",
    "              f\"Val: {val_m['struct_acc']:.1%} | Perp: {train_m['perplexity']:.0f} | \"\n",
    "              f\"CB: {train_m['codebook_usage']:.1%}\")\n",
    "    \n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Final detailed validation\n",
    "    print(\"\\nFinal detailed validation...\")\n",
    "    block_tracker = PerBlockTracker(VOCAB_SIZE)\n",
    "    confusion_tracker = ConfusionTracker()\n",
    "    \n",
    "    final_val = validate_diagnostic(model, val_loader, device, air_tokens, STRUCTURE_WEIGHT,\n",
    "                                    block_tracker, confusion_tracker)\n",
    "    \n",
    "    # Get per-block and confusion analysis\n",
    "    top_blocks, bottom_blocks = block_tracker.get_top_bottom(tok2block, k=TOP_K_BLOCKS)\n",
    "    top_confusions = confusion_tracker.get_top_confusions(tok2block, k=20)\n",
    "    category_acc = compute_category_accuracy(block_tracker, BLOCK_CATEGORIES, AIR_TOKENS)\n",
    "    \n",
    "    # Get reconstruction samples\n",
    "    samples = get_reconstruction_samples(model, val_ds, device, air_tokens, NUM_RECONSTRUCTION_SAMPLES)\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'seed': seed,\n",
    "        'emb_dim': emb_dim,\n",
    "        'best_struct_acc': best_struct_acc,\n",
    "        'final_struct_acc': final_val['struct_acc'],\n",
    "        'final_val_loss': final_val['loss'],\n",
    "        'final_perplexity': final_val['perplexity'],\n",
    "        'final_codebook_usage': final_val['codebook_usage'],\n",
    "        'training_time': train_time,\n",
    "        'history': history,\n",
    "        'top_blocks': top_blocks,\n",
    "        'bottom_blocks': bottom_blocks,\n",
    "        'top_confusions': top_confusions,\n",
    "        'category_accuracy': category_acc,\n",
    "        'reconstruction_samples': samples,\n",
    "    }\n",
    "    \n",
    "    del model, optimizer, scaler\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Main experiment runner defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Run All Experiments\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VQ-VAE EMBEDDING VALIDATION V3 - COMPREHENSIVE DIAGNOSTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Seeds: {BASE_SEEDS}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Embeddings: V1, V3, Random (V2 dropped)\")\n",
    "print()\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# Run each embedding with multiple seeds\n",
    "for emb_name, emb_data in EMBEDDINGS_BASE.items():\n",
    "    all_results[emb_name] = []\n",
    "    for seed in BASE_SEEDS:\n",
    "        result = run_full_experiment(\n",
    "            name=emb_name,\n",
    "            embeddings=emb_data['embeddings'],\n",
    "            emb_dim=emb_data['dim'],\n",
    "            seed=seed,\n",
    "            air_tokens=AIR_TOKENS_TENSOR,\n",
    "            train_ds=train_dataset,\n",
    "            val_ds=val_dataset,\n",
    "        )\n",
    "        all_results[emb_name].append(result)\n",
    "\n",
    "# Random embeddings (fresh random per seed)\n",
    "all_results['Random'] = []\n",
    "for seed in BASE_SEEDS:\n",
    "    np.random.seed(seed)\n",
    "    rand_emb = np.random.randn(VOCAB_SIZE, 32).astype(np.float32) * v1_std\n",
    "    \n",
    "    result = run_full_experiment(\n",
    "        name='Random',\n",
    "        embeddings=rand_emb,\n",
    "        emb_dim=32,\n",
    "        seed=seed,\n",
    "        air_tokens=AIR_TOKENS_TENSOR,\n",
    "        train_ds=train_dataset,\n",
    "        val_ds=val_dataset,\n",
    "    )\n",
    "    all_results['Random'].append(result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Statistical Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute mean and std for each embedding\n",
    "summary_stats = {}\n",
    "for name, runs in all_results.items():\n",
    "    struct_accs = [r['best_struct_acc'] for r in runs]\n",
    "    perplexities = [r['final_perplexity'] for r in runs]\n",
    "    cb_usages = [r['final_codebook_usage'] for r in runs]\n",
    "    \n",
    "    summary_stats[name] = {\n",
    "        'struct_acc_mean': np.mean(struct_accs),\n",
    "        'struct_acc_std': np.std(struct_accs),\n",
    "        'struct_acc_all': struct_accs,\n",
    "        'perplexity_mean': np.mean(perplexities),\n",
    "        'perplexity_std': np.std(perplexities),\n",
    "        'cb_usage_mean': np.mean(cb_usages),\n",
    "        'cb_usage_std': np.std(cb_usages),\n",
    "    }\n",
    "\n",
    "# Print results table\n",
    "print(\"\\n{:<10} {:>18} {:>18} {:>18}\".format(\n",
    "    \"Embedding\", \"Struct Acc\", \"Perplexity\", \"CB Usage\"\n",
    "))\n",
    "print(\"-\"*70)\n",
    "\n",
    "sorted_names = sorted(summary_stats.keys(), key=lambda x: summary_stats[x]['struct_acc_mean'], reverse=True)\n",
    "\n",
    "for name in sorted_names:\n",
    "    s = summary_stats[name]\n",
    "    if NUM_SEEDS > 1:\n",
    "        print(\"{:<10} {:>8.1%} +/- {:>5.1%}   {:>6.0f} +/- {:>5.0f}   {:>6.1%} +/- {:>4.1%}\".format(\n",
    "            name,\n",
    "            s['struct_acc_mean'], s['struct_acc_std'],\n",
    "            s['perplexity_mean'], s['perplexity_std'],\n",
    "            s['cb_usage_mean'], s['cb_usage_std'],\n",
    "        ))\n",
    "    else:\n",
    "        print(\"{:<10} {:>15.1%}   {:>12.0f}   {:>12.1%}\".format(\n",
    "            name,\n",
    "            s['struct_acc_mean'],\n",
    "            s['perplexity_mean'],\n",
    "            s['cb_usage_mean'],\n",
    "        ))\n",
    "\n",
    "# Statistical significance tests (only if multiple seeds)\n",
    "if NUM_SEEDS > 1:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATISTICAL SIGNIFICANCE (t-tests)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    names = ['V1', 'V3', 'Random']\n",
    "    for i, n1 in enumerate(names):\n",
    "        for n2 in names[i+1:]:\n",
    "            acc1 = summary_stats[n1]['struct_acc_all']\n",
    "            acc2 = summary_stats[n2]['struct_acc_all']\n",
    "            \n",
    "            t_stat, p_val = stats.ttest_ind(acc1, acc2)\n",
    "            \n",
    "            sig = \"***\" if p_val < 0.01 else \"**\" if p_val < 0.05 else \"*\" if p_val < 0.1 else \"\"\n",
    "            \n",
    "            diff = summary_stats[n1]['struct_acc_mean'] - summary_stats[n2]['struct_acc_mean']\n",
    "            print(f\"{n1} vs {n2}: diff={diff:+.1%}, p={p_val:.3f} {sig}\")\n",
    "else:\n",
    "    print(\"\\n(Statistical significance tests skipped - need multiple seeds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: Per-Block Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-BLOCK ANALYSIS (from best run of each embedding)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get best run for each embedding\n",
    "best_runs = {}\n",
    "for name, runs in all_results.items():\n",
    "    best_idx = np.argmax([r['best_struct_acc'] for r in runs])\n",
    "    best_runs[name] = runs[best_idx]\n",
    "\n",
    "# Top and bottom blocks for winner\n",
    "winner = sorted_names[0]\n",
    "print(f\"\\n=== {winner} (Best Embedding) ===\")\n",
    "print(f\"\\nTop {TOP_K_BLOCKS} easiest blocks to reconstruct:\")\n",
    "for block, acc, count in best_runs[winner]['top_blocks']:\n",
    "    print(f\"  {acc:>6.1%} ({count:>5} samples): {block}\")\n",
    "\n",
    "print(f\"\\nBottom {TOP_K_BLOCKS} hardest blocks to reconstruct:\")\n",
    "for block, acc, count in best_runs[winner]['bottom_blocks']:\n",
    "    print(f\"  {acc:>6.1%} ({count:>5} samples): {block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Confusion Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP CONFUSIONS (What gets mistaken for what?)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name in ['V1', 'V3', 'Random']:\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    for true_block, pred_block, count in best_runs[name]['top_confusions'][:10]:\n",
    "        print(f\"  {count:>5}x: {true_block[:30]:<30} -> {pred_block[:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 16: Category Accuracy Comparison\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACCURACY BY BLOCK CATEGORY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Gather category accuracies\n",
    "cat_data = {}\n",
    "for name in ['V1', 'V3', 'Random']:\n",
    "    cat_data[name] = best_runs[name]['category_accuracy']\n",
    "\n",
    "# Print table\n",
    "all_cats = sorted(set().union(*[set(d.keys()) for d in cat_data.values()]))\n",
    "\n",
    "print(\"\\n{:<15} {:>10} {:>10} {:>10}\".format(\"Category\", \"V1\", \"V3\", \"Random\"))\n",
    "print(\"-\"*50)\n",
    "\n",
    "for cat in all_cats:\n",
    "    row = [cat]\n",
    "    for name in ['V1', 'V3', 'Random']:\n",
    "        if cat in cat_data[name]:\n",
    "            row.append(f\"{cat_data[name][cat]:.1%}\")\n",
    "        else:\n",
    "            row.append(\"-\")\n",
    "    print(\"{:<15} {:>10} {:>10} {:>10}\".format(*row))\n",
    "\n",
    "# Plot category comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(all_cats))\n",
    "width = 0.25\n",
    "colors = {'V1': 'blue', 'V3': 'purple', 'Random': 'red'}\n",
    "\n",
    "for i, name in enumerate(['V1', 'V3', 'Random']):\n",
    "    vals = [cat_data[name].get(cat, 0) for cat in all_cats]\n",
    "    ax.bar(x + i*width, vals, width, label=name, color=colors[name], alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Reconstruction Accuracy by Block Category')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(all_cats, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/category_accuracy.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 17: Training Dynamics Plots\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
    "\n",
    "colors = {'V1': 'blue', 'V3': 'purple', 'Random': 'red'}\n",
    "names = ['V1', 'V3', 'Random']\n",
    "\n",
    "# Use first seed for clarity\n",
    "run_idx = 0\n",
    "\n",
    "# Row 1: Loss components\n",
    "ax = axes[0, 0]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][run_idx]['history']['train_loss'], label=name, color=colors[name])\n",
    "ax.set_title('Total Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][run_idx]['history']['train_recon'], label=name, color=colors[name])\n",
    "ax.set_title('Reconstruction Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[0, 2]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][run_idx]['history']['train_vq'], label=name, color=colors[name])\n",
    "ax.set_title('VQ Loss (Commitment)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[0, 3]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][run_idx]['history']['val_struct_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('STRUCTURE ACCURACY (KEY)', fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Codebook health\n",
    "ax = axes[1, 0]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][run_idx]['history']['train_codebook_usage'], label=name, color=colors[name])\n",
    "ax.axhline(y=0.3, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_title('Codebook Usage %')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][run_idx]['history']['train_perplexity'], label=name, color=colors[name])\n",
    "ax.set_title('Codebook Perplexity (effective # codes)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1, 2]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name][run_idx]['history']['train_entropy'], label=name, color=colors[name])\n",
    "ax.set_title('Code Distribution Entropy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence speed\n",
    "ax = axes[1, 3]\n",
    "for name in names:\n",
    "    # Normalize to show relative convergence\n",
    "    hist = all_results[name][run_idx]['history']['val_struct_acc']\n",
    "    final = hist[-1]\n",
    "    normalized = [h/final if final > 0 else 0 for h in hist]\n",
    "    ax.plot(normalized, label=name, color=colors[name])\n",
    "ax.set_title('Convergence Speed (normalized)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('% of final accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 3: Gradient analysis and train/val gap\n",
    "ax = axes[2, 0]\n",
    "for name in names:\n",
    "    if all_results[name][run_idx]['history']['encoder_grad_norm']:\n",
    "        ax.plot(all_results[name][run_idx]['history']['encoder_grad_norm'], label=name, color=colors[name])\n",
    "ax.set_title('Encoder Gradient Norm')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[2, 1]\n",
    "for name in names:\n",
    "    if all_results[name][run_idx]['history']['decoder_grad_norm']:\n",
    "        ax.plot(all_results[name][run_idx]['history']['decoder_grad_norm'], label=name, color=colors[name])\n",
    "ax.set_title('Decoder Gradient Norm')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Train/val gap (overfitting indicator)\n",
    "ax = axes[2, 2]\n",
    "for name in names:\n",
    "    train = all_results[name][run_idx]['history']['train_struct_acc']\n",
    "    val = all_results[name][run_idx]['history']['val_struct_acc']\n",
    "    gap = [t - v for t, v in zip(train, val)]\n",
    "    ax.plot(gap, label=name, color=colors[name])\n",
    "ax.axhline(y=0, color='gray', linestyle='--')\n",
    "ax.set_title('Train-Val Gap (overfitting indicator)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Final bar chart with error bars\n",
    "ax = axes[2, 3]\n",
    "means = [summary_stats[name]['struct_acc_mean'] for name in names]\n",
    "stds = [summary_stats[name]['struct_acc_std'] for name in names]\n",
    "bar_colors = [colors[name] for name in names]\n",
    "\n",
    "bars = ax.bar(names, means, yerr=stds, capsize=5, color=bar_colors, edgecolor='black')\n",
    "ax.set_title('Final Structure Accuracy (mean +/- std)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{mean:.1%}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_dynamics.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 18: Reconstruction Visualization\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECONSTRUCTION SAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize reconstructions from best embedding\n",
    "winner = sorted_names[0]\n",
    "samples = best_runs[winner]['reconstruction_samples']\n",
    "\n",
    "fig, axes = plt.subplots(len(samples), 6, figsize=(18, 3*len(samples)))\n",
    "\n",
    "for row, sample in enumerate(samples):\n",
    "    orig = sample['original']\n",
    "    recon = sample['reconstructed']\n",
    "    \n",
    "    # Show 3 z-slices\n",
    "    for col, z in enumerate([8, 16, 24]):\n",
    "        if z < orig.shape[2]:\n",
    "            visualize_slice(orig, recon, z, tok2block, AIR_TOKENS,\n",
    "                           axes[row, col*2], axes[row, col*2+1])\n",
    "    \n",
    "    axes[row, 0].set_ylabel(f\"Sample {row+1}\\nStruct: {sample['struct_acc']:.1%}\", fontsize=10)\n",
    "\n",
    "plt.suptitle(f'Reconstruction Samples ({winner})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/reconstruction_samples.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 19: Save All Results\n",
    "# ============================================================\n",
    "\n",
    "# Save summary statistics\n",
    "summary_output = {}\n",
    "for emb_name, emb_stats in summary_stats.items():  # Renamed to avoid shadowing scipy.stats\n",
    "    summary_output[emb_name] = {\n",
    "        'struct_acc_mean': float(emb_stats['struct_acc_mean']),\n",
    "        'struct_acc_std': float(emb_stats['struct_acc_std']),\n",
    "        'struct_acc_all': [float(x) for x in emb_stats['struct_acc_all']],\n",
    "        'perplexity_mean': float(emb_stats['perplexity_mean']),\n",
    "        'perplexity_std': float(emb_stats['perplexity_std']),\n",
    "        'cb_usage_mean': float(emb_stats['cb_usage_mean']),\n",
    "        'cb_usage_std': float(emb_stats['cb_usage_std']),\n",
    "    }\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/validation_v3_summary.json\", 'w') as f:\n",
    "    json.dump(summary_output, f, indent=2)\n",
    "\n",
    "# Save detailed results (without reconstruction samples - too large)\n",
    "detailed_output = {}\n",
    "for emb_name, runs in all_results.items():\n",
    "    detailed_output[emb_name] = []\n",
    "    for run in runs:\n",
    "        run_data = {\n",
    "            'seed': run['seed'],\n",
    "            'best_struct_acc': float(run['best_struct_acc']),\n",
    "            'final_struct_acc': float(run['final_struct_acc']),\n",
    "            'final_perplexity': float(run['final_perplexity']),\n",
    "            'final_codebook_usage': float(run['final_codebook_usage']),\n",
    "            'training_time': float(run['training_time']),\n",
    "            'history': {k: [float(x) for x in v] for k, v in run['history'].items()},\n",
    "            'top_blocks': run['top_blocks'],\n",
    "            'bottom_blocks': run['bottom_blocks'],\n",
    "            'top_confusions': run['top_confusions'],\n",
    "            'category_accuracy': {k: float(v) for k, v in run['category_accuracy'].items()},\n",
    "        }\n",
    "        detailed_output[emb_name].append(run_data)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/validation_v3_detailed.json\", 'w') as f:\n",
    "    json.dump(detailed_output, f, indent=2)\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'seeds': BASE_SEEDS,\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'hidden_dims': HIDDEN_DIMS,\n",
    "    'latent_dim': LATENT_DIM,\n",
    "    'num_codes': NUM_CODEBOOK_ENTRIES,\n",
    "    'commitment_cost': COMMITMENT_COST,\n",
    "    'ema_decay': EMA_DECAY,\n",
    "    'structure_weight': STRUCTURE_WEIGHT,\n",
    "    'air_tokens': AIR_TOKENS_LIST,\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/validation_v3_config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(f\"  - {OUTPUT_DIR}/validation_v3_summary.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/validation_v3_detailed.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/validation_v3_config.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/category_accuracy.png\")\n",
    "print(f\"  - {OUTPUT_DIR}/training_dynamics.png\")\n",
    "print(f\"  - {OUTPUT_DIR}/reconstruction_samples.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 20: Final Conclusions\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL CONCLUSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. RANKING (by structure accuracy):\")\n",
    "for i, name in enumerate(sorted_names, 1):\n",
    "    s = summary_stats[name]\n",
    "    if NUM_SEEDS > 1:\n",
    "        print(f\"   {i}. {name}: {s['struct_acc_mean']:.1%} +/- {s['struct_acc_std']:.1%}\")\n",
    "    else:\n",
    "        print(f\"   {i}. {name}: {s['struct_acc_mean']:.1%}\")\n",
    "\n",
    "winner = sorted_names[0]\n",
    "random_mean = summary_stats['Random']['struct_acc_mean']\n",
    "winner_mean = summary_stats[winner]['struct_acc_mean']\n",
    "improvement = (winner_mean - random_mean) / random_mean * 100\n",
    "\n",
    "print(f\"\\n2. WINNER: {winner}\")\n",
    "print(f\"   Improvement over random: {improvement:+.1f}%\")\n",
    "\n",
    "if NUM_SEEDS > 1:\n",
    "    print(\"\\n3. STATISTICAL SIGNIFICANCE:\")\n",
    "    v1_accs = summary_stats['V1']['struct_acc_all']\n",
    "    v3_accs = summary_stats['V3']['struct_acc_all']\n",
    "    rand_accs = summary_stats['Random']['struct_acc_all']\n",
    "\n",
    "    _, p_v1_random = stats.ttest_ind(v1_accs, rand_accs)\n",
    "    _, p_v1_v3 = stats.ttest_ind(v1_accs, v3_accs)\n",
    "    _, p_v3_random = stats.ttest_ind(v3_accs, rand_accs)\n",
    "\n",
    "    print(f\"   V1 vs Random: p={p_v1_random:.4f} {'(significant)' if p_v1_random < 0.05 else '(not significant)'}\")\n",
    "    print(f\"   V3 vs Random: p={p_v3_random:.4f} {'(significant)' if p_v3_random < 0.05 else '(not significant)'}\")\n",
    "    print(f\"   V1 vs V3: p={p_v1_v3:.4f} {'(significant)' if p_v1_v3 < 0.05 else '(not significant)'}\")\n",
    "else:\n",
    "    print(\"\\n3. STATISTICAL SIGNIFICANCE:\")\n",
    "    print(\"   (Skipped - need multiple seeds for t-tests)\")\n",
    "\n",
    "names = ['V1', 'V3', 'Random']\n",
    "\n",
    "print(\"\\n4. CODEBOOK HEALTH:\")\n",
    "for name in names:\n",
    "    usage = summary_stats[name]['cb_usage_mean']\n",
    "    perp = summary_stats[name]['perplexity_mean']\n",
    "    status = \"Healthy\" if usage > 0.3 else \"WARNING: Possible collapse\"\n",
    "    print(f\"   {name}: {usage:.1%} usage, {perp:.0f} effective codes - {status}\")\n",
    "\n",
    "print(\"\\n5. CATEGORY INSIGHTS:\")\n",
    "# Find categories where V1 significantly beats Random\n",
    "v1_cats = best_runs['V1']['category_accuracy']\n",
    "rand_cats = best_runs['Random']['category_accuracy']\n",
    "best_cats = []\n",
    "for cat in v1_cats:\n",
    "    if cat in rand_cats:\n",
    "        diff = v1_cats[cat] - rand_cats[cat]\n",
    "        if diff > 0.05:\n",
    "            best_cats.append((cat, diff))\n",
    "\n",
    "best_cats.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"   Categories where embeddings help most:\")\n",
    "for cat, diff in best_cats[:5]:\n",
    "    print(f\"   - {cat}: +{diff:.1%}\")\n",
    "\n",
    "print(\"\\n6. MAIN FAILURE MODES:\")\n",
    "print(\"   Most common confusions (from winner):\")\n",
    "for true_b, pred_b, count in best_runs[winner]['top_confusions'][:5]:\n",
    "    print(f\"   - {true_b[:25]} -> {pred_b[:25]} ({count}x)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "v1_mean = summary_stats['V1']['struct_acc_mean']\n",
    "v3_mean = summary_stats['V3']['struct_acc_mean']\n",
    "\n",
    "if v1_mean > v3_mean:\n",
    "    diff_pct = (v1_mean - v3_mean) / v3_mean * 100\n",
    "    print(f\"\\n-> V1 outperformed V3 by {diff_pct:.1f}%\")\n",
    "    if diff_pct > 5:\n",
    "        print(\"   Recommendation: Use V1 embeddings for VQ-VAE\")\n",
    "    else:\n",
    "        print(\"   Difference is small. Consider V3 for interpretability benefits.\")\n",
    "else:\n",
    "    diff_pct = (v3_mean - v1_mean) / v1_mean * 100\n",
    "    print(f\"\\n-> V3 outperformed V1 by {diff_pct:.1f}%\")\n",
    "    if diff_pct > 5:\n",
    "        print(\"   Recommendation: Use V3 compositional embeddings for VQ-VAE\")\n",
    "    else:\n",
    "        print(\"   Difference is small. V3 has interpretability benefits.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
