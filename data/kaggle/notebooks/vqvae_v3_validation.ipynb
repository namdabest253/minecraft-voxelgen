{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE Embedding Validation: V3 Only\n",
    "\n",
    "## Purpose\n",
    "Validate Block2Vec V3 (compositional) embeddings for VQ-VAE structure reconstruction.\n",
    "\n",
    "## Previous Results (from V2 validation)\n",
    "- V1: **44.3%** structure accuracy\n",
    "- V2: **43.0%** structure accuracy  \n",
    "- Random: **39.0%** structure accuracy\n",
    "\n",
    "**Key Question**: Can V3 compositional embeddings beat V1's 44.3%?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Configuration (V3 ONLY)\n",
    "# ============================================================\n",
    "\n",
    "# === Data Paths ===\n",
    "DATA_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/train\"\n",
    "VAL_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/val\"\n",
    "VOCAB_PATH = \"/kaggle/input/minecraft-schematics/tok2block.json\"\n",
    "\n",
    "# V3 embeddings path\n",
    "V3_EMBEDDINGS_PATH = \"/kaggle/input/block2vec-v3/block_embeddings_v3.npy\"\n",
    "\n",
    "# Previous validation results (V1, V2, Random) for comparison plotting\n",
    "PREVIOUS_RESULTS_PATH = \"/kaggle/input/block2vec-v3/embedding_validation_full.json\"\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# === Mini Model Architecture (same as V2 validation) ===\n",
    "BLOCK_EMBEDDING_DIM = 40  # V3 compositional: 16+16+8=40\n",
    "HIDDEN_DIMS = [32, 64, 128]\n",
    "LATENT_DIM = 128\n",
    "NUM_CODEBOOK_ENTRIES = 512\n",
    "COMMITMENT_COST = 0.25\n",
    "\n",
    "# === Training ===\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 3e-4\n",
    "USE_AMP = True\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# === Other ===\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"V3 Validation Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load Vocabulary and V3 Embeddings\n",
    "# ============================================================\n",
    "\n",
    "# Load vocabulary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE} block types\")\n",
    "\n",
    "# ============================================================\n",
    "# Find ALL air tokens (not just token 0!)\n",
    "# ============================================================\n",
    "AIR_TOKENS = set()\n",
    "for tok, block in tok2block.items():\n",
    "    block_lower = block.lower()\n",
    "    if 'air' in block_lower and 'stair' not in block_lower:\n",
    "        AIR_TOKENS.add(tok)\n",
    "        print(f\"  Found air token: {tok} = {block}\")\n",
    "\n",
    "AIR_TOKENS_TENSOR = torch.tensor(sorted(AIR_TOKENS), dtype=torch.long)\n",
    "print()\n",
    "print(f\"Air tokens: {AIR_TOKENS_TENSOR.tolist()}\")\n",
    "\n",
    "# Load V3 embeddings - keep as numpy (model will convert)\n",
    "v3_embeddings = np.load(V3_EMBEDDINGS_PATH)\n",
    "print()\n",
    "print(f\"V3 embeddings shape: {v3_embeddings.shape}\")\n",
    "\n",
    "# V3 ONLY - keep as numpy arrays\n",
    "EMBEDDINGS = {\n",
    "    \"V3\": v3_embeddings,\n",
    "}\n",
    "\n",
    "print()\n",
    "print(f\"Embeddings to test: {list(EMBEDDINGS.keys())}\")\n",
    "print(\"Note: Comparing against previous results - V1: 44.3%, V2: 43.0%, Random: 39.0%\")\n",
    "\n",
    "# Save air tokens info\n",
    "air_info = {\n",
    "    \"air_tokens\": sorted(AIR_TOKENS),\n",
    "    \"note\": \"These tokens were excluded from structure accuracy calculation\"\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/air_tokens_used.json\", 'w') as f:\n",
    "    json.dump(air_info, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Dataset\n",
    "# ============================================================\n",
    "\n",
    "class VQVAEDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, seed: int = 42):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        if not self.h5_files:\n",
    "            raise ValueError(f\"No H5 files found in {data_dir}\")\n",
    "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.h5_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_files[idx], 'r') as f:\n",
    "            key = list(f.keys())[0]\n",
    "            structure = f[key][:].astype(np.int64)\n",
    "        return torch.from_numpy(structure).long()\n",
    "\n",
    "\n",
    "train_dataset = VQVAEDataset(DATA_DIR, seed=SEED)\n",
    "val_dataset = VQVAEDataset(VAL_DIR, seed=SEED)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Mini VQ-VAE Model\n",
    "# ============================================================\n",
    "\n",
    "class ResidualBlock3D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(channels)\n",
    "        self.bn2 = nn.BatchNorm3d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class MiniVQVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dims, latent_dim, \n",
    "                 num_codes, commitment_cost, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_codes = num_codes\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "        # Block embeddings (frozen)\n",
    "        self.block_emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.block_emb.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.block_emb.weight.requires_grad = False\n",
    "        \n",
    "        # Encoder\n",
    "        enc_layers = []\n",
    "        in_ch = embedding_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            enc_layers.extend([\n",
    "                nn.Conv3d(in_ch, h_dim, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                ResidualBlock3D(h_dim),\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        enc_layers.append(nn.Conv3d(in_ch, latent_dim, 3, padding=1))\n",
    "        self.encoder = nn.Sequential(*enc_layers)\n",
    "        \n",
    "        # Codebook\n",
    "        self.codebook = nn.Embedding(num_codes, latent_dim)\n",
    "        self.codebook.weight.data.uniform_(-1/num_codes, 1/num_codes)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_layers = []\n",
    "        in_ch = latent_dim\n",
    "        for h_dim in reversed(hidden_dims):\n",
    "            dec_layers.extend([\n",
    "                ResidualBlock3D(in_ch),\n",
    "                nn.ConvTranspose3d(in_ch, h_dim, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        dec_layers.append(nn.Conv3d(in_ch, vocab_size, 3, padding=1))\n",
    "        self.decoder = nn.Sequential(*dec_layers)\n",
    "    \n",
    "    def quantize(self, z_e):\n",
    "        # z_e: [B, C, D, H, W]\n",
    "        z_e_perm = z_e.permute(0, 2, 3, 4, 1).contiguous()  # [B, D, H, W, C]\n",
    "        flat = z_e_perm.view(-1, self.latent_dim)\n",
    "        \n",
    "        # Distances\n",
    "        d = (flat**2).sum(1, keepdim=True) + \\\n",
    "            (self.codebook.weight**2).sum(1) - \\\n",
    "            2 * flat @ self.codebook.weight.t()\n",
    "        \n",
    "        indices = d.argmin(1)\n",
    "        z_q_flat = self.codebook(indices)\n",
    "        z_q_perm = z_q_flat.view(z_e_perm.shape)\n",
    "        \n",
    "        # Losses\n",
    "        codebook_loss = F.mse_loss(z_q_perm, z_e_perm.detach())\n",
    "        commit_loss = F.mse_loss(z_e_perm, z_q_perm.detach())\n",
    "        vq_loss = codebook_loss + self.commitment_cost * commit_loss\n",
    "        \n",
    "        # Straight-through\n",
    "        z_q_st = z_e_perm + (z_q_perm - z_e_perm).detach()\n",
    "        z_q = z_q_st.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        \n",
    "        return z_q, vq_loss, indices.view(z_e_perm.shape[:-1])\n",
    "    \n",
    "    def forward(self, block_ids):\n",
    "        # Embed\n",
    "        x = self.block_emb(block_ids)  # [B, 32, 32, 32, emb]\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()  # [B, emb, 32, 32, 32]\n",
    "        \n",
    "        # Encode\n",
    "        z_e = self.encoder(x)\n",
    "        \n",
    "        # Quantize\n",
    "        z_q, vq_loss, indices = self.quantize(z_e)\n",
    "        \n",
    "        # Decode\n",
    "        logits = self.decoder(z_q)\n",
    "        \n",
    "        return {'logits': logits, 'vq_loss': vq_loss, 'indices': indices}\n",
    "    \n",
    "    def compute_loss(self, block_ids, air_tokens_tensor):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics.\n",
    "        \n",
    "        BUG FIX: Now uses air_tokens_tensor to correctly identify air blocks.\n",
    "        Previous version used `block_ids != 0`, but token 0 is UNKNOWN_BLOCK!\n",
    "        Air tokens are: 19 (air), 164 (cave_air), 932 (void_air)\n",
    "        \"\"\"\n",
    "        out = self(block_ids)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        logits = out['logits'].permute(0, 2, 3, 4, 1).contiguous()\n",
    "        recon_loss = F.cross_entropy(logits.view(-1, self.vocab_size), block_ids.view(-1))\n",
    "        \n",
    "        total_loss = recon_loss + out['vq_loss']\n",
    "        \n",
    "        # Accuracy metrics\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(-1)\n",
    "            targets_flat = block_ids.view(-1)\n",
    "            preds_flat = preds.view(-1)\n",
    "            \n",
    "            # Overall accuracy\n",
    "            correct = (preds_flat == targets_flat).float()\n",
    "            acc = correct.mean()\n",
    "            \n",
    "            # Move air tokens to same device\n",
    "            air_tokens_device = air_tokens_tensor.to(targets_flat.device)\n",
    "            \n",
    "            # Find air and non-air blocks using torch.isin\n",
    "            is_air = torch.isin(targets_flat, air_tokens_device)\n",
    "            is_structure = ~is_air\n",
    "            \n",
    "            # Air accuracy\n",
    "            if is_air.sum() > 0:\n",
    "                air_acc = correct[is_air].mean()\n",
    "            else:\n",
    "                air_acc = torch.tensor(0.0, device=block_ids.device)\n",
    "            \n",
    "            # Structure accuracy (non-air) - THE KEY METRIC!\n",
    "            if is_structure.sum() > 0:\n",
    "                struct_acc = correct[is_structure].mean()\n",
    "            else:\n",
    "                struct_acc = torch.tensor(0.0, device=block_ids.device)\n",
    "            \n",
    "            # Track air percentage for sanity check\n",
    "            air_pct = is_air.float().mean()\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'vq_loss': out['vq_loss'],\n",
    "            'accuracy': acc,\n",
    "            'air_accuracy': air_acc,\n",
    "            'struct_accuracy': struct_acc,\n",
    "            'air_percentage': air_pct,\n",
    "            'indices': out['indices'],\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"MiniVQVAE defined!\")\n",
    "print(\"BUG FIX: compute_loss now correctly identifies all air tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Training Functions\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device, air_tokens_tensor):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    metrics = {'loss': 0, 'recon': 0, 'vq': 0, 'acc': 0, 'air_acc': 0, 'struct_acc': 0, 'air_pct': 0}\n",
    "    n = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss(batch, air_tokens_tensor)\n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = out['loss'] / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Step optimizer every GRAD_ACCUM_STEPS batches\n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['recon'] += out['recon_loss'].item()\n",
    "        metrics['vq'] += out['vq_loss'].item()\n",
    "        metrics['acc'] += out['accuracy'].item()\n",
    "        metrics['air_acc'] += out['air_accuracy'].item()\n",
    "        metrics['struct_acc'] += out['struct_accuracy'].item()\n",
    "        metrics['air_pct'] += out['air_percentage'].item()\n",
    "        n += 1\n",
    "    \n",
    "    # Handle remaining gradients if loader length not divisible by GRAD_ACCUM_STEPS\n",
    "    if len(loader) % GRAD_ACCUM_STEPS != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return {k: v/n for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device, air_tokens_tensor):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    metrics = {'loss': 0, 'recon': 0, 'acc': 0, 'air_acc': 0, 'struct_acc': 0, 'air_pct': 0}\n",
    "    n = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss(batch, air_tokens_tensor)\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['recon'] += out['recon_loss'].item()\n",
    "        metrics['acc'] += out['accuracy'].item()\n",
    "        metrics['air_acc'] += out['air_accuracy'].item()\n",
    "        metrics['struct_acc'] += out['struct_accuracy'].item()\n",
    "        metrics['air_pct'] += out['air_percentage'].item()\n",
    "        n += 1\n",
    "    \n",
    "    return {k: v/n for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Run Experiment for One Embedding Type\n",
    "# ============================================================\n",
    "\n",
    "def run_experiment(name, embeddings, air_tokens_tensor):\n",
    "    \"\"\"Train and evaluate VQ-VAE with given embeddings.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with {name} embeddings\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Air tokens: {air_tokens_tensor.tolist()}\")\n",
    "    \n",
    "    # Clear GPU memory from previous experiment\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    # Create model\n",
    "    model = MiniVQVAE(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=BLOCK_EMBEDDING_DIM,\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        num_codes=NUM_CODEBOOK_ENTRIES,\n",
    "        commitment_cost=COMMITMENT_COST,\n",
    "        pretrained_embeddings=embeddings,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count params\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    \n",
    "    # Print GPU memory status\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"GPU memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "    \n",
    "    # Training loop - now tracking air_acc and air_pct\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_air_acc': [], 'train_struct_acc': [], 'train_air_pct': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_air_acc': [], 'val_struct_acc': [], 'val_air_pct': [],\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_metrics = train_epoch(model, train_loader, optimizer, scaler, device, air_tokens_tensor)\n",
    "        val_metrics = validate(model, val_loader, device, air_tokens_tensor)\n",
    "        \n",
    "        history['train_loss'].append(train_metrics['loss'])\n",
    "        history['train_acc'].append(train_metrics['acc'])\n",
    "        history['train_air_acc'].append(train_metrics['air_acc'])\n",
    "        history['train_struct_acc'].append(train_metrics['struct_acc'])\n",
    "        history['train_air_pct'].append(train_metrics['air_pct'])\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['acc'])\n",
    "        history['val_air_acc'].append(val_metrics['air_acc'])\n",
    "        history['val_struct_acc'].append(val_metrics['struct_acc'])\n",
    "        history['val_air_pct'].append(val_metrics['air_pct'])\n",
    "        \n",
    "        # Show all metrics including structure accuracy\n",
    "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
    "              f\"Loss: {train_metrics['loss']:.3f} | \"\n",
    "              f\"Acc: {train_metrics['acc']:.1%} | \"\n",
    "              f\"Struct: {train_metrics['struct_acc']:.1%} | \"\n",
    "              f\"Air: {train_metrics['air_acc']:.1%} | \"\n",
    "              f\"Val Struct: {val_metrics['struct_acc']:.1%} | \"\n",
    "              f\"Air%: {val_metrics['air_pct']:.1%}\")\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training time: {train_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Sanity check: struct_acc should differ from overall acc\n",
    "    if abs(history['val_acc'][-1] - history['val_struct_acc'][-1]) < 0.001:\n",
    "        print(\"⚠️  WARNING: Overall and Structure accuracy are nearly identical!\")\n",
    "        print(\"    Air detection may still be broken.\")\n",
    "    else:\n",
    "        print(f\"✓ Structure accuracy ({history['val_struct_acc'][-1]:.1%}) differs from overall ({history['val_acc'][-1]:.1%})\")\n",
    "    \n",
    "    # Final metrics\n",
    "    final_metrics = {\n",
    "        'name': name,\n",
    "        'final_train_loss': history['train_loss'][-1],\n",
    "        'final_val_loss': history['val_loss'][-1],\n",
    "        'final_train_acc': history['train_acc'][-1],\n",
    "        'final_val_acc': history['val_acc'][-1],\n",
    "        'final_train_struct_acc': history['train_struct_acc'][-1],\n",
    "        'final_val_struct_acc': history['val_struct_acc'][-1],\n",
    "        'final_train_air_acc': history['train_air_acc'][-1],\n",
    "        'final_val_air_acc': history['val_air_acc'][-1],\n",
    "        'avg_air_pct': np.mean(history['val_air_pct']),\n",
    "        'best_val_loss': min(history['val_loss']),\n",
    "        'best_val_acc': max(history['val_acc']),\n",
    "        'best_val_struct_acc': max(history['val_struct_acc']),\n",
    "        'training_time': train_time,\n",
    "        'history': history,\n",
    "    }\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, optimizer, scaler\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "print(\"Experiment function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Run All Experiments\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 0: VQ-VAE EMBEDDING VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAir tokens being used: {AIR_TOKENS_TENSOR.tolist()}\")\n",
    "print(\"These will be EXCLUDED from structure accuracy calculation.\\n\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for name, embeddings in EMBEDDINGS.items():\n",
    "    results = run_experiment(name, embeddings, AIR_TOKENS_TENSOR)\n",
    "    all_results[name] = results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Load Previous Results and Compare\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING PREVIOUS RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load previous results (V1, V2, Random) from JSON file\n",
    "with open(PREVIOUS_RESULTS_PATH, 'r') as f:\n",
    "    previous_results = json.load(f)\n",
    "\n",
    "print(f\"Loaded previous results: {list(previous_results.keys())}\")\n",
    "\n",
    "# Combine all results\n",
    "all_results_combined = {**previous_results, 'V3': all_results['V3']}\n",
    "\n",
    "# Extract best metrics from previous results\n",
    "for name in ['V1', 'V2', 'Random']:\n",
    "    hist = previous_results[name]['history']\n",
    "    previous_results[name]['best_val_loss'] = min(hist['val_loss'])\n",
    "    previous_results[name]['best_val_acc'] = max(hist['val_acc'])\n",
    "    previous_results[name]['best_val_struct_acc'] = max(hist['val_struct_acc'])\n",
    "    print(f\"  {name}: {previous_results[name]['best_val_struct_acc']:.1%} structure accuracy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show air percentage\n",
    "avg_air_pct = all_results['V3']['avg_air_pct']\n",
    "print(f\"\\nAverage air percentage in data: {avg_air_pct:.1%}\")\n",
    "print(\"This is why STRUCTURE accuracy (non-air) is the key metric!\\n\")\n",
    "\n",
    "# Create comparison table\n",
    "print(\"{:<10} {:>12} {:>12} {:>12} {:>12}\".format(\n",
    "    \"Embeddings\", \"Val Loss\", \"Overall Acc\", \"STRUCT Acc\", \"Time\"))\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Print all results\n",
    "for name in ['V3', 'V1', 'V2', 'Random']:\n",
    "    if name == 'V3':\n",
    "        r = all_results['V3']\n",
    "        time_str = f\"{r['training_time']/60:.1f}m\"\n",
    "    else:\n",
    "        r = previous_results[name]\n",
    "        time_str = f\"{r['training_time']/60:.1f}m\"\n",
    "    \n",
    "    print(\"{:<10} {:>12.4f} {:>12.1%} {:>12.1%} {:>12}\".format(\n",
    "        name + (\" (NEW)\" if name == 'V3' else \"\"),\n",
    "        r['best_val_loss'],\n",
    "        r['best_val_acc'],\n",
    "        r['best_val_struct_acc'],\n",
    "        time_str\n",
    "    ))\n",
    "\n",
    "# Calculate V3 improvement\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"V3 IMPROVEMENT OVER BASELINES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "v3_struct = all_results['V3']['best_val_struct_acc']\n",
    "v1_struct = previous_results['V1']['best_val_struct_acc']\n",
    "v2_struct = previous_results['V2']['best_val_struct_acc']\n",
    "random_struct = previous_results['Random']['best_val_struct_acc']\n",
    "\n",
    "v3_vs_random = (v3_struct - random_struct) / random_struct * 100\n",
    "v3_vs_v1 = (v3_struct - v1_struct) / v1_struct * 100\n",
    "v3_vs_v2 = (v3_struct - v2_struct) / v2_struct * 100\n",
    "\n",
    "print(f\"\\nV3 Structure Accuracy: {v3_struct:.1%}\")\n",
    "print(f\"  vs Random ({random_struct:.1%}): {v3_vs_random:+.1f}%\")\n",
    "print(f\"  vs V1 ({v1_struct:.1%}):     {v3_vs_v1:+.1f}%\")\n",
    "print(f\"  vs V2 ({v2_struct:.1%}):     {v3_vs_v2:+.1f}%\")\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if v3_struct > v1_struct:\n",
    "    print(f\"\\n✓ V3 ({v3_struct:.1%}) BEATS V1 ({v1_struct:.1%})!\")\n",
    "    print(\"  Compositional embeddings are the best approach.\")\n",
    "    print(\"  Use V3 embeddings for VQ-VAE training.\")\n",
    "elif v3_struct > v2_struct:\n",
    "    print(f\"\\n~ V3 ({v3_struct:.1%}) beats V2 ({v2_struct:.1%}) but not V1 ({v1_struct:.1%}).\")\n",
    "    print(\"  V1 skip-gram is still the best. Consider hybrid approach.\")\n",
    "else:\n",
    "    print(f\"\\n✗ V3 ({v3_struct:.1%}) is worse than V1 and V2.\")\n",
    "    print(\"  Compositional approach didn't help. Stick with V1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Plot All Results (V1, V2, V3, Random)\n",
    "# ============================================================\n",
    "\n",
    "# Plot comparison of all embedding types\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "colors = {'V1': 'blue', 'V2': 'green', 'V3': 'purple', 'Random': 'red'}\n",
    "names = ['V1', 'V2', 'V3', 'Random']\n",
    "\n",
    "# Validation Loss\n",
    "ax = axes[0, 0]\n",
    "for name in names:\n",
    "    if name == 'V3':\n",
    "        hist = all_results['V3']['history']\n",
    "    else:\n",
    "        hist = previous_results[name]['history']\n",
    "    ax.plot(hist['val_loss'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Validation Loss by Embedding Type', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overall Accuracy  \n",
    "ax = axes[0, 1]\n",
    "for name in names:\n",
    "    if name == 'V3':\n",
    "        hist = all_results['V3']['history']\n",
    "    else:\n",
    "        hist = previous_results[name]['history']\n",
    "    ax.plot(hist['val_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Overall Accuracy (includes ~80% air)', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Structure Accuracy (KEY METRIC)\n",
    "ax = axes[0, 2]\n",
    "for name in names:\n",
    "    if name == 'V3':\n",
    "        hist = all_results['V3']['history']\n",
    "    else:\n",
    "        hist = previous_results[name]['history']\n",
    "    ax.plot(hist['val_struct_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('STRUCTURE Accuracy (KEY METRIC)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Structure Accuracy (non-air)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Air Accuracy\n",
    "ax = axes[1, 0]\n",
    "for name in names:\n",
    "    if name == 'V3':\n",
    "        hist = all_results['V3']['history']\n",
    "    else:\n",
    "        hist = previous_results[name]['history']\n",
    "    ax.plot(hist['val_air_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Air Block Accuracy', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Air Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Air Percentage (sanity check)\n",
    "ax = axes[1, 1]\n",
    "for name in names:\n",
    "    if name == 'V3':\n",
    "        hist = all_results['V3']['history']\n",
    "    else:\n",
    "        hist = previous_results[name]['history']\n",
    "    ax.plot(hist['val_air_pct'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Air Block % in Data (~should be constant)', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Air Percentage')\n",
    "ax.set_ylim(0.78, 0.82)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart comparison\n",
    "ax = axes[1, 2]\n",
    "struct_accs = []\n",
    "for name in names:\n",
    "    if name == 'V3':\n",
    "        struct_accs.append(all_results['V3']['best_val_struct_acc'])\n",
    "    else:\n",
    "        struct_accs.append(previous_results[name]['best_val_struct_acc'])\n",
    "bar_colors = [colors[n] for n in names]\n",
    "bars = ax.bar(names, struct_accs, color=bar_colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_title('Best Structure Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim(0, 0.55)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc, name in zip(bars, struct_accs, names):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, height + 0.01, \n",
    "            f'{acc:.1%}', ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/embedding_comparison_all.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved plot to {OUTPUT_DIR}/embedding_comparison_all.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Save Final Results\n",
    "# ============================================================\n",
    "\n",
    "# Get V3 results from all_results\n",
    "v3_result = all_results['V3']\n",
    "\n",
    "# Save V3 summary\n",
    "v3_summary = {\n",
    "    \"V3\": {\n",
    "        \"final_val_loss\": v3_result[\"history\"][\"val_loss\"][-1],\n",
    "        \"final_val_acc\": v3_result[\"history\"][\"val_acc\"][-1],\n",
    "        \"final_val_struct_acc\": v3_result[\"history\"][\"val_struct_acc\"][-1],\n",
    "        \"final_val_air_acc\": v3_result[\"history\"][\"val_air_acc\"][-1],\n",
    "        \"best_val_struct_acc\": v3_result[\"best_val_struct_acc\"],\n",
    "        \"training_time_minutes\": v3_result[\"training_time\"] / 60,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/v3_validation_summary.json\", 'w') as f:\n",
    "    json.dump(v3_summary, f, indent=2)\n",
    "\n",
    "# Save full history (convert numpy to lists for JSON)\n",
    "v3_full = {\n",
    "    'V3': {\n",
    "        'history': {k: [float(x) for x in v] for k, v in v3_result['history'].items()},\n",
    "        'training_time': v3_result['training_time'],\n",
    "        'best_val_loss': float(v3_result['best_val_loss']),\n",
    "        'best_val_acc': float(v3_result['best_val_acc']),\n",
    "        'best_val_struct_acc': float(v3_result['best_val_struct_acc']),\n",
    "    }\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/v3_validation_full.json\", 'w') as f:\n",
    "    json.dump(v3_full, f, indent=2)\n",
    "\n",
    "# Print comparison using loaded previous results (not hardcoded)\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL COMPARISON - Structure Accuracy (non-air blocks)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  V1:     {previous_results['V1']['best_val_struct_acc']:.1%}  (previous)\")\n",
    "print(f\"  V2:     {previous_results['V2']['best_val_struct_acc']:.1%}  (previous)\")\n",
    "print(f\"  V3:     {v3_result['best_val_struct_acc']:.1%}  ← NEW (best)\")\n",
    "print(f\"  Random: {previous_results['Random']['best_val_struct_acc']:.1%}  (previous)\")\n",
    "print()\n",
    "\n",
    "# Use BEST accuracy, not last epoch accuracy\n",
    "v3_best = v3_result['best_val_struct_acc']\n",
    "v1_best = previous_results['V1']['best_val_struct_acc']\n",
    "v2_best = previous_results['V2']['best_val_struct_acc']\n",
    "\n",
    "if v3_best > v1_best:\n",
    "    print(\"✓ V3 BEATS V1! Use V3 embeddings for VQ-VAE.\")\n",
    "elif v3_best > v2_best:\n",
    "    print(\"~ V3 is between V1 and V2. Could use either V1 or V3.\")\n",
    "else:\n",
    "    print(\"✗ V3 is worse than V1. Stick with V1 embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Final Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 0 COMPLETE: VQ-VAE EMBEDDING VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nQuestion: Do Block2Vec embeddings help VQ-VAE reconstruct STRUCTURES?\")\n",
    "print(f\"\\nAir tokens excluded: {sorted(AIR_TOKENS)}\")\n",
    "print(f\"Average air percentage: {avg_air_pct:.1%}\")\n",
    "print(\"\\nNOTE: Overall accuracy is ~{:.0%} just from predicting air correctly.\".format(avg_air_pct))\n",
    "print(\"      STRUCTURE accuracy is the true measure of reconstruction quality!\")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(\"\\n{:<12} {:>12} {:>15} {:>15} {:>12}\".format(\n",
    "    \"\", \"Val Loss\", \"Overall Acc\", \"★STRUCT Acc★\", \"Air Acc\"))\n",
    "print(\"-\"*70)\n",
    "\n",
    "# All results\n",
    "for name in ['V3', 'V1', 'V2', 'Random']:\n",
    "    if name == 'V3':\n",
    "        r = all_results['V3']\n",
    "        label = \"V3 (NEW)\"\n",
    "    else:\n",
    "        r = previous_results[name]\n",
    "        label = name\n",
    "    print(\"{:<12} {:>12.4f} {:>15.1%} {:>15.1%} {:>12.1%}\".format(\n",
    "        label,\n",
    "        r['best_val_loss'],\n",
    "        r['best_val_acc'],\n",
    "        r['best_val_struct_acc'],\n",
    "        r['history']['val_air_acc'][-1]\n",
    "    ))\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "v3_struct = all_results['V3']['best_val_struct_acc']\n",
    "v1_struct = previous_results['V1']['best_val_struct_acc']\n",
    "v2_struct = previous_results['V2']['best_val_struct_acc']\n",
    "random_struct = previous_results['Random']['best_val_struct_acc']\n",
    "\n",
    "v3_vs_random = (v3_struct - random_struct) / random_struct * 100\n",
    "v3_vs_v1 = (v3_struct - v1_struct) / v1_struct * 100\n",
    "\n",
    "print(f\"\\nV3 Structure Accuracy: {v3_struct:.1%}\")\n",
    "print(f\"  Improvement over Random ({random_struct:.1%}): {v3_vs_random:+.1f}%\")\n",
    "print(f\"  Comparison to V1 ({v1_struct:.1%}):        {v3_vs_v1:+.1f}%\")\n",
    "\n",
    "if v3_struct > v1_struct:\n",
    "    print(\"\\n✓ V3 BEATS V1! Compositional embeddings are the best!\")\n",
    "elif v3_struct > v2_struct:\n",
    "    print(\"\\n~ V3 is between V1 and V2. V1 still slightly better.\")\n",
    "else:\n",
    "    print(\"\\n✗ V3 underperforms. Stick with V1 embeddings.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Files saved:\")\n",
    "print(f\"  - {OUTPUT_DIR}/embedding_comparison_all.png\")\n",
    "print(f\"  - {OUTPUT_DIR}/v3_validation_summary.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/v3_validation_full.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/air_tokens_used.json\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
