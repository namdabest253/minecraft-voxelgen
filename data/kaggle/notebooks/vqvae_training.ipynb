{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE: Learning to Compress Minecraft Structures\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook trains a **Vector Quantized Variational AutoEncoder (VQ-VAE)** to learn a compressed representation of Minecraft structures. After training, the model can:\n",
    "\n",
    "1. **Encode** any 32×32×32 structure into a small grid of discrete codes\n",
    "2. **Decode** those codes back into a full structure\n",
    "\n",
    "This compressed representation will later be used for text-to-structure generation.\n",
    "\n",
    "## The Pipeline So Far\n",
    "\n",
    "1. **Phase 1 (Done)**: Prepare training data - 4,462 Minecraft builds as 3D arrays\n",
    "2. **Phase 2 (Done)**: Train Block2Vec - learned 32-dim embeddings for each block type\n",
    "3. **Phase 3 (This Notebook)**: Train VQ-VAE - learn to compress/decompress structures\n",
    "4. **Phase 4**: Connect text descriptions to the VQ-VAE\n",
    "5. **Phase 5**: Generate new structures from text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding AutoEncoders\n",
    "\n",
    "## What is an AutoEncoder?\n",
    "\n",
    "An **autoencoder** is a neural network that learns to compress and reconstruct data:\n",
    "\n",
    "```\n",
    "Input → [Encoder] → Latent Code → [Decoder] → Reconstructed Input\n",
    "  ↑                                                    ↓\n",
    "  └──────────── Should be as similar as possible ──────┘\n",
    "```\n",
    "\n",
    "For Minecraft structures:\n",
    "- **Input**: 32×32×32 blocks (32,768 positions)\n",
    "- **Latent Code**: 4×4×4 = 64 vectors (much smaller!)\n",
    "- **Output**: Reconstructed 32×32×32 structure\n",
    "\n",
    "## Why Compress?\n",
    "\n",
    "1. **Efficient Generation**: Generating 64 codes is much easier than 32,768 blocks\n",
    "2. **Learning Patterns**: The encoder learns to recognize building patterns\n",
    "3. **Denoising**: Reconstruction smooths out noise and inconsistencies\n",
    "\n",
    "## The Bottleneck Forces Learning\n",
    "\n",
    "The latent code is MUCH smaller than the input. This forces the encoder to:\n",
    "- Identify what's important\n",
    "- Discard irrelevant details\n",
    "- Learn efficient representations\n",
    "\n",
    "If we just copied the input, we wouldn't learn anything useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: The \"VQ\" in VQ-VAE - Vector Quantization\n",
    "\n",
    "## Continuous vs Discrete\n",
    "\n",
    "A regular autoencoder has **continuous** latent codes - any real number is allowed.\n",
    "\n",
    "VQ-VAE uses **discrete** codes - each position must pick from a fixed set of options.\n",
    "\n",
    "Think of it like this:\n",
    "- **Continuous**: Any shade of color (infinite options)\n",
    "- **Discrete**: Pick from a palette of 512 specific colors\n",
    "\n",
    "## The Codebook\n",
    "\n",
    "VQ-VAE maintains a **codebook** - a learned lookup table of vectors:\n",
    "\n",
    "```\n",
    "Codebook (512 vectors, each 256-dimensional):\n",
    "┌─────────────────────────────────────┐\n",
    "│ Code 0:   [0.2, -0.1, 0.5, ...]     │ → Maybe represents \"stone walls\"\n",
    "│ Code 1:   [0.8,  0.3, -0.2, ...]    │ → Maybe represents \"wooden floors\"\n",
    "│ Code 2:   [-0.4, 0.7, 0.1, ...]     │ → Maybe represents \"glass windows\"\n",
    "│ ...                                  │\n",
    "│ Code 511: [0.1, 0.0, 0.3, ...]      │ → Maybe represents \"empty air\"\n",
    "└─────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## How Quantization Works\n",
    "\n",
    "1. **Encoder outputs** a continuous vector at each position\n",
    "2. **Find nearest** codebook entry for each position\n",
    "3. **Replace** encoder output with the codebook entry\n",
    "4. **Decoder** receives the codebook vectors\n",
    "\n",
    "```\n",
    "Encoder output: [0.21, -0.08, 0.52, ...]  (continuous)\n",
    "                        ↓\n",
    "              Find nearest in codebook\n",
    "                        ↓\n",
    "Quantized:      [0.2, -0.1, 0.5, ...]     (Code 0)\n",
    "                        ↓\n",
    "                    Decoder\n",
    "```\n",
    "\n",
    "## Why Discrete is Better for Generation\n",
    "\n",
    "1. **Finite possibilities**: With 512 codes at 64 positions, there are 512^64 possible structures (still huge, but manageable)\n",
    "2. **No \"blurry\" outputs**: Discrete codes produce sharp, distinct outputs\n",
    "3. **Easy to sample**: Just pick codes 0-511 at each position\n",
    "4. **Works with language models**: Text models naturally output discrete tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: The Straight-Through Estimator\n",
    "\n",
    "## The Problem: Argmin is Not Differentiable\n",
    "\n",
    "Finding the nearest codebook entry uses `argmin` (which index has minimum distance?)\n",
    "\n",
    "But `argmin` has **zero gradient** everywhere! Neural networks learn through gradients.\n",
    "\n",
    "```\n",
    "Forward:  Encoder output → argmin → Codebook entry → Decoder\n",
    "Backward: How should encoder change? No gradient flows through argmin!\n",
    "```\n",
    "\n",
    "## The Solution: Copy the Gradients\n",
    "\n",
    "We use a clever trick called the **straight-through estimator**:\n",
    "\n",
    "- **Forward pass**: Use the quantized codebook vector\n",
    "- **Backward pass**: Pretend quantization didn't happen, copy gradients directly\n",
    "\n",
    "In code:\n",
    "```python\n",
    "# Forward: z_q is used (quantized)\n",
    "# Backward: gradients go to z_e (as if z_q = z_e)\n",
    "z_q = z_e + (z_q - z_e).detach()\n",
    "```\n",
    "\n",
    "The `.detach()` stops gradients through `(z_q - z_e)`, so:\n",
    "- Forward: `z_e + (z_q - z_e) = z_q` ✓\n",
    "- Backward: Only `z_e` receives gradients ✓\n",
    "\n",
    "## Why This Works\n",
    "\n",
    "The encoder learns to output vectors CLOSE to codebook entries.\n",
    "The decoder provides feedback: \"If you'd output this slightly different vector, the reconstruction would be better.\"\n",
    "The encoder adjusts, and over time, encoder outputs cluster around good codebook entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: The Three VQ-VAE Losses\n",
    "\n",
    "## 1. Reconstruction Loss\n",
    "\n",
    "**Goal**: The output should match the input.\n",
    "\n",
    "For each position, we predict which block should be there (out of 3,717 options).\n",
    "This is **cross-entropy loss** - the standard loss for classification.\n",
    "\n",
    "```\n",
    "True block: oak_planks (token 153)\n",
    "Model predicts: [0.01, 0.02, ..., 0.85 (token 153), ..., 0.01]\n",
    "Loss = -log(0.85) = 0.16  (low loss = good!)\n",
    "\n",
    "If model predicted 0.01 for token 153:\n",
    "Loss = -log(0.01) = 4.6  (high loss = bad!)\n",
    "```\n",
    "\n",
    "## 2. Codebook Loss\n",
    "\n",
    "**Goal**: Move codebook vectors toward encoder outputs.\n",
    "\n",
    "```python\n",
    "codebook_loss = MSE(codebook_vectors, encoder_output.detach())\n",
    "```\n",
    "\n",
    "The `.detach()` means: \"Update the codebook, but don't change the encoder.\"\n",
    "\n",
    "This makes codebook vectors migrate toward where the encoder is pointing.\n",
    "\n",
    "## 3. Commitment Loss\n",
    "\n",
    "**Goal**: Keep encoder outputs close to codebook vectors.\n",
    "\n",
    "```python\n",
    "commitment_loss = MSE(encoder_output, codebook_vectors.detach())\n",
    "```\n",
    "\n",
    "This prevents the encoder from wandering too far from any codebook entry.\n",
    "\n",
    "## Total Loss\n",
    "\n",
    "```\n",
    "total_loss = reconstruction_loss + codebook_loss + β × commitment_loss\n",
    "```\n",
    "\n",
    "Where β (beta) is typically 0.25. The commitment loss is weighted lower because we want the codebook to \"follow\" the encoder more than vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Architecture for Minecraft\n",
    "\n",
    "## The Full Pipeline\n",
    "\n",
    "```\n",
    "Input: 32×32×32 block IDs\n",
    "        ↓\n",
    "Block Embedding: Convert each ID to 32-dim vector (using Block2Vec!)\n",
    "        ↓\n",
    "Shape: 32×32×32×32 (spatial × embedding_dim)\n",
    "        ↓\n",
    "Encoder: 3D Convolutions that downsample\n",
    "  - 32→16→8→4 in each spatial dimension\n",
    "  - 32→64→128→256 channels\n",
    "        ↓\n",
    "Shape: 4×4×4×256\n",
    "        ↓\n",
    "Vector Quantizer: Each of 64 positions picks from 512 codebook entries\n",
    "        ↓\n",
    "Shape: 4×4×4×256 (same, but now discrete)\n",
    "        ↓\n",
    "Decoder: 3D Transposed Convolutions that upsample\n",
    "  - 4→8→16→32 in each spatial dimension\n",
    "  - 256→128→64→3717 channels\n",
    "        ↓\n",
    "Output: 32×32×32×3717 (logits for each block type at each position)\n",
    "```\n",
    "\n",
    "## Compression Ratio\n",
    "\n",
    "- **Input**: 32×32×32 = 32,768 positions, each needs log2(3717) ≈ 12 bits\n",
    "- **Latent**: 4×4×4 = 64 positions, each needs log2(512) = 9 bits\n",
    "- **Compression**: 32,768 × 12 / (64 × 9) ≈ **680:1**\n",
    "\n",
    "That's a massive compression! The model must learn efficient patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Let's Start Coding!\n",
    "\n",
    "Now that you understand the concepts, let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 2: Configuration\n# ============================================================\n\n# === Data Paths ===\nDATA_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/train\"\nVAL_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/val\"\nVOCAB_PATH = \"/kaggle/input/minecraft-schematics/tok2block.json\"  # At root level!\nEMBEDDINGS_PATH = \"/kaggle/input/block2vec-embeddings/block_embeddings.npy\"  # From Phase 2!\nOUTPUT_DIR = \"/kaggle/working\"\n\n# === Model Architecture ===\nBLOCK_EMBEDDING_DIM = 32    # Must match Block2Vec embeddings\nHIDDEN_DIMS = [64, 128, 256]  # Encoder channel progression\nLATENT_DIM = 256            # Codebook vector dimension\nNUM_CODEBOOK_ENTRIES = 512  # Number of codes in codebook\nCOMMITMENT_COST = 0.25      # Beta for commitment loss\n\n# === Training Hyperparameters ===\nEPOCHS = 25\nBATCH_SIZE = 8              # Reduced for memory (using mixed precision)\nLEARNING_RATE = 1e-4        # Lower LR for stable training\nWEIGHT_DECAY = 1e-5\nUSE_AMP = True              # Automatic Mixed Precision (fp16) - saves memory!\n\n# === Other ===\nSEED = 42\nNUM_WORKERS = 2             # Parallel data loading\n\nprint(\"Configuration loaded!\")\nprint(f\"  Latent grid: 4×4×4 = 64 positions\")\nprint(f\"  Codebook: {NUM_CODEBOOK_ENTRIES} entries × {LATENT_DIM} dims\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Mixed Precision (AMP): {USE_AMP}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load Vocabulary and Pre-trained Embeddings\n",
    "# ============================================================\n",
    "\n",
    "# Load vocabulary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE} block types\")\n",
    "\n",
    "# Load pre-trained Block2Vec embeddings\n",
    "pretrained_embeddings = np.load(EMBEDDINGS_PATH)\n",
    "print(f\"Loaded embeddings: {pretrained_embeddings.shape}\")\n",
    "\n",
    "# Verify dimensions match\n",
    "assert pretrained_embeddings.shape == (VOCAB_SIZE, BLOCK_EMBEDDING_DIM), \\\n",
    "    f\"Embedding shape mismatch! Expected ({VOCAB_SIZE}, {BLOCK_EMBEDDING_DIM})\"\n",
    "\n",
    "print(\"\\nEmbeddings will be used to convert block IDs → 32-dim vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: The Dataset\n",
    "\n",
    "For VQ-VAE, we simply load complete 32×32×32 structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Dataset Class\n",
    "# ============================================================\n",
    "\n",
    "class VQVAEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that loads complete 32×32×32 Minecraft structures.\n",
    "    \n",
    "    Unlike Block2VecDataset (which yielded individual block pairs),\n",
    "    this returns whole structures for reconstruction learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        augment: bool = False,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.augment = augment\n",
    "        self.rng = random.Random(seed)\n",
    "        \n",
    "        # Find all H5 files\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        if not self.h5_files:\n",
    "            raise ValueError(f\"No H5 files found in {data_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.h5_files)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        \"\"\"Load and return a single structure.\"\"\"\n",
    "        h5_path = self.h5_files[idx]\n",
    "        \n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            key = list(f.keys())[0]\n",
    "            structure = f[key][:]\n",
    "        \n",
    "        structure = structure.astype(np.int64)\n",
    "        \n",
    "        # Optional augmentation: random rotations around Y axis\n",
    "        if self.augment:\n",
    "            k = self.rng.randint(0, 3)  # 0, 90, 180, or 270 degrees\n",
    "            if k > 0:\n",
    "                structure = np.rot90(structure, k=k, axes=(0, 2))\n",
    "            \n",
    "            # Random horizontal flip\n",
    "            if self.rng.random() > 0.5:\n",
    "                structure = np.flip(structure, axis=2)\n",
    "            \n",
    "            structure = np.ascontiguousarray(structure)\n",
    "        \n",
    "        return torch.from_numpy(structure).long()\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VQVAEDataset(DATA_DIR, augment=True, seed=SEED)\n",
    "val_dataset = VQVAEDataset(VAL_DIR, augment=False, seed=SEED)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_dataset)} structures\")\n",
    "print(f\"Val: {len(val_dataset)} structures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Create DataLoaders\n",
    "# ============================================================\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Val batches per epoch: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: The VQ-VAE Model\n",
    "\n",
    "Now let's define the neural network. This is more complex than Block2Vec!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Vector Quantizer\n",
    "# ============================================================\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantization layer with straight-through gradient estimator.\n",
    "    \n",
    "    This is the heart of VQ-VAE - it discretizes the latent space.\n",
    "    \n",
    "    Args:\n",
    "        num_embeddings: Number of codebook entries (K=512)\n",
    "        embedding_dim: Dimension of each codebook vector (D=256)\n",
    "        commitment_cost: Weight for commitment loss (beta=0.25)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int = 512,\n",
    "        embedding_dim: int = 256,\n",
    "        commitment_cost: float = 0.25,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "        # The codebook: K vectors of dimension D\n",
    "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        \n",
    "        # Initialize with small uniform values\n",
    "        self.codebook.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)\n",
    "    \n",
    "    def forward(self, z_e: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Quantize encoder outputs to nearest codebook entries.\n",
    "        \n",
    "        Args:\n",
    "            z_e: Encoder output [batch, channels, depth, height, width]\n",
    "                 channels should equal embedding_dim\n",
    "        \n",
    "        Returns:\n",
    "            z_q: Quantized output (same shape as z_e)\n",
    "            vq_loss: Codebook + commitment loss\n",
    "            indices: Which codebook entry was selected [batch, D, H, W]\n",
    "        \"\"\"\n",
    "        # z_e shape: [B, C, D, H, W] where C = embedding_dim\n",
    "        \n",
    "        # Reshape to [B*D*H*W, C] for distance calculation\n",
    "        z_e_permuted = z_e.permute(0, 2, 3, 4, 1).contiguous()  # [B, D, H, W, C]\n",
    "        flat_z_e = z_e_permuted.view(-1, self.embedding_dim)    # [B*D*H*W, C]\n",
    "        \n",
    "        # Compute distances using the identity:\n",
    "        # ||z - c||² = ||z||² + ||c||² - 2*z·c\n",
    "        z_e_sq = (flat_z_e ** 2).sum(dim=1, keepdim=True)          # [N, 1]\n",
    "        codebook_sq = (self.codebook.weight ** 2).sum(dim=1).unsqueeze(0)  # [1, K]\n",
    "        dot_product = torch.mm(flat_z_e, self.codebook.weight.t())  # [N, K]\n",
    "        distances = z_e_sq + codebook_sq - 2 * dot_product          # [N, K]\n",
    "        \n",
    "        # Find nearest codebook entry\n",
    "        indices = distances.argmin(dim=1)  # [N]\n",
    "        \n",
    "        # Look up codebook vectors\n",
    "        z_q_flat = self.codebook(indices)  # [N, C]\n",
    "        z_q_permuted = z_q_flat.view(z_e_permuted.shape)  # [B, D, H, W, C]\n",
    "        \n",
    "        # === Compute VQ Loss ===\n",
    "        # Codebook loss: move codebook toward encoder (stop gradient on z_e)\n",
    "        codebook_loss = F.mse_loss(z_q_permuted, z_e_permuted.detach())\n",
    "        \n",
    "        # Commitment loss: keep encoder close to codebook (stop gradient on z_q)\n",
    "        commitment_loss = F.mse_loss(z_e_permuted, z_q_permuted.detach())\n",
    "        \n",
    "        vq_loss = codebook_loss + self.commitment_cost * commitment_loss\n",
    "        \n",
    "        # === Straight-Through Estimator ===\n",
    "        # Forward: use z_q | Backward: gradients flow to z_e\n",
    "        z_q_st = z_e_permuted + (z_q_permuted - z_e_permuted).detach()\n",
    "        \n",
    "        # Permute back to [B, C, D, H, W]\n",
    "        z_q = z_q_st.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        \n",
    "        # Reshape indices to spatial grid\n",
    "        indices = indices.view(z_e_permuted.shape[:-1])  # [B, D, H, W]\n",
    "        \n",
    "        return z_q, vq_loss, indices\n",
    "\n",
    "\n",
    "print(\"VectorQuantizer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Residual Block\n",
    "# ============================================================\n",
    "\n",
    "class ResidualBlock3D(nn.Module):\n",
    "    \"\"\"\n",
    "    3D Residual block for encoder/decoder.\n",
    "    \n",
    "    Residual connections help gradients flow through deep networks:\n",
    "    output = conv(conv(input)) + input\n",
    "    \n",
    "    If the convolutions learn nothing, the output = input (identity).\n",
    "    This makes training easier for deep networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(in_channels)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        \n",
    "        # If dimensions change, need a 1x1 conv for the skip connection\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = self.skip(x)\n",
    "        \n",
    "        out = self.bn1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        return out + identity  # The residual connection!\n",
    "\n",
    "\n",
    "print(\"ResidualBlock3D defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Encoder\n",
    "# ============================================================\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    3D CNN Encoder that compresses 32×32×32 to 4×4×4.\n",
    "    \n",
    "    Uses strided convolutions to downsample by 2× at each layer.\n",
    "    \n",
    "    Architecture:\n",
    "        32×32×32 (32ch) → 16×16×16 (64ch) → 8×8×8 (128ch) → 4×4×4 (256ch)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 32,\n",
    "        hidden_dims: List[int] = None,\n",
    "        latent_dim: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [64, 128, 256]\n",
    "        \n",
    "        layers = []\n",
    "        current_channels = in_channels\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                # Strided conv to downsample 2×\n",
    "                nn.Conv3d(current_channels, hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                # Residual block for more capacity\n",
    "                ResidualBlock3D(hidden_dim, hidden_dim),\n",
    "            ])\n",
    "            current_channels = hidden_dim\n",
    "        \n",
    "        # Final projection to latent dimension\n",
    "        layers.append(nn.Conv3d(current_channels, latent_dim, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "print(\"Encoder defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Decoder\n",
    "# ============================================================\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    3D CNN Decoder that expands 4×4×4 back to 32×32×32.\n",
    "    \n",
    "    Uses transposed convolutions to upsample by 2× at each layer.\n",
    "    \n",
    "    Architecture:\n",
    "        4×4×4 (256ch) → 8×8×8 (128ch) → 16×16×16 (64ch) → 32×32×32 (vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 256,\n",
    "        hidden_dims: List[int] = None,\n",
    "        num_blocks: int = 3717,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [256, 128, 64]\n",
    "        \n",
    "        layers = []\n",
    "        current_channels = latent_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                ResidualBlock3D(current_channels, current_channels),\n",
    "                # Transposed conv to upsample 2×\n",
    "                nn.ConvTranspose3d(current_channels, hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ])\n",
    "            current_channels = hidden_dim\n",
    "        \n",
    "        # Final prediction layer: output logits for each block type\n",
    "        layers.append(nn.Conv3d(current_channels, num_blocks, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z_q: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(z_q)\n",
    "\n",
    "\n",
    "print(\"Decoder defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Full VQ-VAE Model\n",
    "# ============================================================\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantized Variational AutoEncoder for Minecraft structures.\n",
    "    \n",
    "    Full pipeline:\n",
    "        1. Embed input blocks using pre-trained Block2Vec\n",
    "        2. Encode to compressed latent grid\n",
    "        3. Quantize each position to nearest codebook entry\n",
    "        4. Decode to predict block at each position\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        block_embedding_dim: int,\n",
    "        hidden_dims: List[int],\n",
    "        latent_dim: int,\n",
    "        num_codebook_entries: int,\n",
    "        commitment_cost: float,\n",
    "        pretrained_embeddings: np.ndarray,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_codebook_entries = num_codebook_entries\n",
    "        \n",
    "        # Block embedding layer (using pre-trained Block2Vec!)\n",
    "        self.block_embeddings = nn.Embedding(vocab_size, block_embedding_dim)\n",
    "        self.block_embeddings.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.block_embeddings.weight.requires_grad = False  # Freeze - already trained!\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            in_channels=block_embedding_dim,\n",
    "            hidden_dims=hidden_dims,\n",
    "            latent_dim=latent_dim,\n",
    "        )\n",
    "        \n",
    "        # Vector Quantizer\n",
    "        self.quantizer = VectorQuantizer(\n",
    "            num_embeddings=num_codebook_entries,\n",
    "            embedding_dim=latent_dim,\n",
    "            commitment_cost=commitment_cost,\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            latent_dim=latent_dim,\n",
    "            hidden_dims=list(reversed(hidden_dims)),\n",
    "            num_blocks=vocab_size,\n",
    "        )\n",
    "    \n",
    "    def forward(self, block_ids: torch.Tensor) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Full forward pass.\n",
    "        \n",
    "        Args:\n",
    "            block_ids: Block token IDs [batch, 32, 32, 32]\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with logits, vq_loss, indices, etc.\n",
    "        \"\"\"\n",
    "        # Step 1: Embed blocks\n",
    "        # [B, 32, 32, 32] → [B, 32, 32, 32, 32] (last dim is embedding)\n",
    "        embedded = self.block_embeddings(block_ids)\n",
    "        \n",
    "        # Permute to channel-first: [B, 32, 32, 32, 32] → [B, 32, 32, 32, 32]\n",
    "        # Wait, that looks the same! Let me clarify:\n",
    "        # Input:  [B, D, H, W, C] where D=H=W=32, C=32\n",
    "        # Output: [B, C, D, H, W]\n",
    "        embedded = embedded.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        \n",
    "        # Step 2: Encode\n",
    "        z_e = self.encoder(embedded)  # [B, 256, 4, 4, 4]\n",
    "        \n",
    "        # Step 3: Quantize\n",
    "        z_q, vq_loss, indices = self.quantizer(z_e)\n",
    "        \n",
    "        # Step 4: Decode\n",
    "        logits = self.decoder(z_q)  # [B, vocab_size, 32, 32, 32]\n",
    "        \n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"vq_loss\": vq_loss,\n",
    "            \"indices\": indices,\n",
    "            \"z_e\": z_e,\n",
    "            \"z_q\": z_q,\n",
    "        }\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        block_ids: torch.Tensor,\n",
    "        ignore_index: int = -100,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute training loss.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with loss, reconstruction_loss, vq_loss, accuracy\n",
    "        \"\"\"\n",
    "        outputs = self(block_ids)\n",
    "        \n",
    "        # Reconstruction loss (cross-entropy)\n",
    "        # logits: [B, vocab_size, 32, 32, 32] → [B, 32, 32, 32, vocab_size]\n",
    "        logits = outputs[\"logits\"].permute(0, 2, 3, 4, 1).contiguous()\n",
    "        \n",
    "        # Flatten for cross-entropy\n",
    "        logits_flat = logits.view(-1, self.vocab_size)\n",
    "        targets_flat = block_ids.view(-1)\n",
    "        \n",
    "        reconstruction_loss = F.cross_entropy(\n",
    "            logits_flat,\n",
    "            targets_flat,\n",
    "            ignore_index=ignore_index,\n",
    "        )\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + outputs[\"vq_loss\"]\n",
    "        \n",
    "        # Compute accuracy\n",
    "        with torch.no_grad():\n",
    "            predictions = logits_flat.argmax(dim=1)\n",
    "            if ignore_index >= 0:\n",
    "                mask = targets_flat != ignore_index\n",
    "                correct = (predictions[mask] == targets_flat[mask]).float().sum()\n",
    "                total = mask.sum()\n",
    "            else:\n",
    "                correct = (predictions == targets_flat).float().sum()\n",
    "                total = targets_flat.numel()\n",
    "            accuracy = correct / total if total > 0 else torch.tensor(0.0)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"vq_loss\": outputs[\"vq_loss\"],\n",
    "            \"accuracy\": accuracy,\n",
    "            \"indices\": outputs[\"indices\"],\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"VQVAE model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Create Model\n",
    "# ============================================================\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Create model\n",
    "model = VQVAE(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    block_embedding_dim=BLOCK_EMBEDDING_DIM,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    num_codebook_entries=NUM_CODEBOOK_ENTRIES,\n",
    "    commitment_cost=COMMITMENT_COST,\n",
    "    pretrained_embeddings=pretrained_embeddings,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model on {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen (Block2Vec): {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Test Forward Pass\n",
    "# ============================================================\n",
    "\n",
    "# Quick test to make sure everything works\n",
    "print(\"Testing forward pass...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Create a dummy batch\n",
    "    test_batch = torch.randint(0, VOCAB_SIZE, (2, 32, 32, 32)).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model.compute_loss(test_batch)\n",
    "    \n",
    "    print(f\"  Input shape: {test_batch.shape}\")\n",
    "    print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
    "    print(f\"  Reconstruction loss: {outputs['reconstruction_loss'].item():.4f}\")\n",
    "    print(f\"  VQ loss: {outputs['vq_loss'].item():.4f}\")\n",
    "    print(f\"  Accuracy: {outputs['accuracy'].item():.4f}\")\n",
    "    print(f\"  Indices shape: {outputs['indices'].shape}\")\n",
    "\n",
    "print(\"\\nForward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 13: Create Optimizer, Scheduler, and Scaler\n# ============================================================\n\n# AdamW optimizer\noptimizer = optim.AdamW(\n    filter(lambda p: p.requires_grad, model.parameters()),  # Only trainable params\n    lr=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n)\n\n# Learning rate scheduler: reduce LR when validation loss plateaus\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,      # Multiply LR by 0.5 when triggered\n    patience=5,       # Wait 5 epochs before reducing\n)\n\n# GradScaler for mixed precision training\n# This scales gradients to prevent underflow in fp16\nscaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n\nprint(f\"Optimizer: AdamW (lr={LEARNING_RATE})\")\nprint(f\"Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")\nprint(f\"Mixed Precision: {'Enabled' if USE_AMP else 'Disabled'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 14: Training Loop (with Mixed Precision)\n# ============================================================\n\ndef train_epoch(model, loader, optimizer, scaler, device, use_amp=True):\n    \"\"\"Train for one epoch with optional mixed precision.\"\"\"\n    model.train()\n    total_loss = 0\n    total_recon = 0\n    total_vq = 0\n    total_acc = 0\n    num_batches = 0\n    \n    # Track codebook usage\n    all_indices = []\n    \n    for batch in tqdm(loader, desc=\"Training\", leave=False):\n        batch = batch.to(device)\n        \n        # Forward pass with automatic mixed precision\n        with torch.amp.autocast('cuda', enabled=use_amp):\n            outputs = model.compute_loss(batch)\n            loss = outputs[\"loss\"]\n        \n        # Backward pass with gradient scaling\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        \n        # Unscale gradients for clipping\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        # Optimizer step with scaler\n        scaler.step(optimizer)\n        scaler.update()\n        \n        # Track metrics (these are already fp32)\n        total_loss += loss.item()\n        total_recon += outputs[\"reconstruction_loss\"].item()\n        total_vq += outputs[\"vq_loss\"].item()\n        total_acc += outputs[\"accuracy\"].item()\n        num_batches += 1\n        \n        # Track codebook usage\n        all_indices.append(outputs[\"indices\"].cpu())\n    \n    # Compute codebook utilization\n    all_indices = torch.cat([idx.view(-1) for idx in all_indices])\n    unique_codes = len(torch.unique(all_indices))\n    \n    return {\n        \"loss\": total_loss / num_batches,\n        \"recon_loss\": total_recon / num_batches,\n        \"vq_loss\": total_vq / num_batches,\n        \"accuracy\": total_acc / num_batches,\n        \"codebook_usage\": unique_codes / NUM_CODEBOOK_ENTRIES,\n    }\n\n\n@torch.no_grad()\ndef validate(model, loader, device, use_amp=True):\n    \"\"\"Validate the model with optional mixed precision.\"\"\"\n    model.eval()\n    total_loss = 0\n    total_recon = 0\n    total_acc = 0\n    num_batches = 0\n    \n    for batch in tqdm(loader, desc=\"Validating\", leave=False):\n        batch = batch.to(device)\n        \n        with torch.amp.autocast('cuda', enabled=use_amp):\n            outputs = model.compute_loss(batch)\n        \n        total_loss += outputs[\"loss\"].item()\n        total_recon += outputs[\"reconstruction_loss\"].item()\n        total_acc += outputs[\"accuracy\"].item()\n        num_batches += 1\n    \n    return {\n        \"loss\": total_loss / num_batches,\n        \"recon_loss\": total_recon / num_batches,\n        \"accuracy\": total_acc / num_batches,\n    }\n\n\nprint(\"Training functions defined (with mixed precision support)!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 15: Main Training Loop\n# ============================================================\n\nprint(\"=\" * 60)\nprint(\"Starting Training\")\nprint(\"=\" * 60)\n\n# Track metrics\nhistory = {\n    \"train_loss\": [],\n    \"train_recon\": [],\n    \"train_vq\": [],\n    \"train_acc\": [],\n    \"val_loss\": [],\n    \"val_recon\": [],\n    \"val_acc\": [],\n    \"codebook_usage\": [],\n    \"lr\": [],\n}\n\nbest_val_loss = float(\"inf\")\nstart_time = time.time()\n\nfor epoch in range(EPOCHS):\n    epoch_start = time.time()\n    \n    # Train (with mixed precision)\n    train_metrics = train_epoch(model, train_loader, optimizer, scaler, device, use_amp=USE_AMP)\n    \n    # Validate (with mixed precision)\n    val_metrics = validate(model, val_loader, device, use_amp=USE_AMP)\n    \n    # Update scheduler\n    scheduler.step(val_metrics[\"loss\"])\n    current_lr = optimizer.param_groups[0][\"lr\"]\n    \n    # Track history\n    history[\"train_loss\"].append(train_metrics[\"loss\"])\n    history[\"train_recon\"].append(train_metrics[\"recon_loss\"])\n    history[\"train_vq\"].append(train_metrics[\"vq_loss\"])\n    history[\"train_acc\"].append(train_metrics[\"accuracy\"])\n    history[\"val_loss\"].append(val_metrics[\"loss\"])\n    history[\"val_recon\"].append(val_metrics[\"recon_loss\"])\n    history[\"val_acc\"].append(val_metrics[\"accuracy\"])\n    history[\"codebook_usage\"].append(train_metrics[\"codebook_usage\"])\n    history[\"lr\"].append(current_lr)\n    \n    # Save best model\n    if val_metrics[\"loss\"] < best_val_loss:\n        best_val_loss = val_metrics[\"loss\"]\n        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/vqvae_best.pt\")\n    \n    # Print progress\n    epoch_time = time.time() - epoch_start\n    print(\n        f\"Epoch {epoch+1:3d}/{EPOCHS} | \"\n        f\"Train: {train_metrics['loss']:.4f} (acc {train_metrics['accuracy']:.3f}) | \"\n        f\"Val: {val_metrics['loss']:.4f} (acc {val_metrics['accuracy']:.3f}) | \"\n        f\"CB: {train_metrics['codebook_usage']:.1%} | \"\n        f\"LR: {current_lr:.2e} | \"\n        f\"{epoch_time:.0f}s\"\n    )\n\ntotal_time = time.time() - start_time\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"Training complete in {total_time/60:.1f} minutes\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 16: Save Results\n",
    "# ============================================================\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), f\"{OUTPUT_DIR}/vqvae_final.pt\")\n",
    "print(f\"Final model saved to {OUTPUT_DIR}/vqvae_final.pt\")\n",
    "\n",
    "# Save codebook\n",
    "codebook = model.quantizer.codebook.weight.data.cpu().numpy()\n",
    "np.save(f\"{OUTPUT_DIR}/codebook.npy\", codebook)\n",
    "print(f\"Codebook saved: {codebook.shape}\")\n",
    "\n",
    "# Save training history\n",
    "with open(f\"{OUTPUT_DIR}/training_history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(\"Training history saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 10: Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 17: Plot Training Curves\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history[\"train_loss\"], label=\"Train\", linewidth=2)\n",
    "ax.plot(history[\"val_loss\"], label=\"Val\", linewidth=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Total Loss\")\n",
    "ax.set_title(\"Training and Validation Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history[\"train_acc\"], label=\"Train\", linewidth=2)\n",
    "ax.plot(history[\"val_acc\"], label=\"Val\", linewidth=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Block Prediction Accuracy\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction vs VQ Loss\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history[\"train_recon\"], label=\"Reconstruction\", linewidth=2)\n",
    "ax.plot(history[\"train_vq\"], label=\"VQ\", linewidth=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Loss Components (Training)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Codebook Usage\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history[\"codebook_usage\"], linewidth=2, color=\"green\")\n",
    "ax.axhline(y=1.0, color=\"red\", linestyle=\"--\", label=\"100% usage\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Fraction Used\")\n",
    "ax.set_title(\"Codebook Utilization\")\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal metrics:\")\n",
    "print(f\"  Train accuracy: {history['train_acc'][-1]:.3f}\")\n",
    "print(f\"  Val accuracy: {history['val_acc'][-1]:.3f}\")\n",
    "print(f\"  Codebook usage: {history['codebook_usage'][-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 18: Visualize Reconstructions\n",
    "# ============================================================\n",
    "\n",
    "def visualize_reconstruction(model, dataset, device, idx=0):\n",
    "    \"\"\"Visualize original vs reconstructed structure.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a sample\n",
    "    original = dataset[idx].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(original)\n",
    "        reconstructed = outputs[\"logits\"].argmax(dim=1)\n",
    "    \n",
    "    original = original.cpu().numpy()[0]\n",
    "    reconstructed = reconstructed.cpu().numpy()[0]\n",
    "    \n",
    "    # Compare center slices\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # X-slice (center)\n",
    "    slice_idx = 16\n",
    "    \n",
    "    axes[0, 0].imshow(original[slice_idx, :, :], cmap='tab20')\n",
    "    axes[0, 0].set_title(f'Original (X slice {slice_idx})')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(reconstructed[slice_idx, :, :], cmap='tab20')\n",
    "    axes[1, 0].set_title(f'Reconstructed (X slice {slice_idx})')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Y-slice (center)\n",
    "    axes[0, 1].imshow(original[:, slice_idx, :], cmap='tab20')\n",
    "    axes[0, 1].set_title(f'Original (Y slice {slice_idx})')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(reconstructed[:, slice_idx, :], cmap='tab20')\n",
    "    axes[1, 1].set_title(f'Reconstructed (Y slice {slice_idx})')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Z-slice (center)\n",
    "    axes[0, 2].imshow(original[:, :, slice_idx], cmap='tab20')\n",
    "    axes[0, 2].set_title(f'Original (Z slice {slice_idx})')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(reconstructed[:, :, slice_idx], cmap='tab20')\n",
    "    axes[1, 2].set_title(f'Reconstructed (Z slice {slice_idx})')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    # Compute accuracy for this sample\n",
    "    accuracy = (original == reconstructed).mean()\n",
    "    plt.suptitle(f'Reconstruction Accuracy: {accuracy:.1%}', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/reconstruction_{idx}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Visualize a few samples\n",
    "print(\"Visualizing reconstructions...\")\n",
    "for i in range(3):\n",
    "    acc = visualize_reconstruction(model, val_dataset, device, idx=i)\n",
    "    print(f\"Sample {i}: {acc:.1%} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 19: Analyze Codebook Usage\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def analyze_codebook(model, loader, device):\n",
    "    \"\"\"Analyze how the codebook is being used.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_indices = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Analyzing codebook\"):\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch)\n",
    "        all_indices.append(outputs[\"indices\"].cpu().view(-1))\n",
    "    \n",
    "    all_indices = torch.cat(all_indices)\n",
    "    \n",
    "    # Count usage of each code\n",
    "    usage = torch.bincount(all_indices, minlength=NUM_CODEBOOK_ENTRIES)\n",
    "    usage = usage.float() / usage.sum()\n",
    "    \n",
    "    return usage.numpy()\n",
    "\n",
    "\n",
    "# Analyze\n",
    "codebook_usage = analyze_codebook(model, val_loader, device)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "ax.bar(range(NUM_CODEBOOK_ENTRIES), sorted(codebook_usage, reverse=True))\n",
    "ax.set_xlabel(\"Codebook Entry (sorted by usage)\")\n",
    "ax.set_ylabel(\"Usage Frequency\")\n",
    "ax.set_title(\"Codebook Usage Distribution\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "# Stats\n",
    "ax = axes[1]\n",
    "used_codes = (codebook_usage > 0).sum()\n",
    "top10_usage = sorted(codebook_usage, reverse=True)[:10]\n",
    "\n",
    "stats_text = f\"\"\"\n",
    "Codebook Statistics:\n",
    "\n",
    "Total codes: {NUM_CODEBOOK_ENTRIES}\n",
    "Used codes: {used_codes} ({used_codes/NUM_CODEBOOK_ENTRIES:.1%})\n",
    "Dead codes: {NUM_CODEBOOK_ENTRIES - used_codes}\n",
    "\n",
    "Top 10 codes account for: {sum(top10_usage):.1%}\n",
    "\n",
    "Max usage: {max(codebook_usage):.3%}\n",
    "Min usage (non-zero): {min(u for u in codebook_usage if u > 0):.6%}\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.5, stats_text, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='center', fontfamily='monospace')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/codebook_analysis.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 11: Summary and Next Steps\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "1. **AutoEncoders** compress and reconstruct data through a bottleneck\n",
    "2. **Vector Quantization** forces the latent space to be discrete (finite options)\n",
    "3. **The Codebook** learns patterns that appear in the training data\n",
    "4. **Straight-Through Estimator** allows gradients to flow despite discrete quantization\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "With the VQ-VAE trained, we can now:\n",
    "\n",
    "1. **Encode** any structure into 64 discrete codes (4×4×4 grid)\n",
    "2. **Decode** those codes back to a full structure\n",
    "\n",
    "In Phase 4, we'll train a model to predict these codes from text descriptions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 20: Final Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VQ-VAE TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Input: 32×32×32 block structure\")\n",
    "print(f\"  Latent: 4×4×4 = 64 discrete codes\")\n",
    "print(f\"  Codebook: {NUM_CODEBOOK_ENTRIES} entries × {LATENT_DIM} dims\")\n",
    "print(f\"  Compression: ~680:1\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Final train accuracy: {history['train_acc'][-1]:.3f}\")\n",
    "print(f\"  Final val accuracy: {history['val_acc'][-1]:.3f}\")\n",
    "print(f\"  Codebook usage: {history['codebook_usage'][-1]:.1%}\")\n",
    "\n",
    "print(f\"\\nOutput files in {OUTPUT_DIR}:\")\n",
    "print(f\"  - vqvae_best.pt (best validation loss)\")\n",
    "print(f\"  - vqvae_final.pt (final epoch)\")\n",
    "print(f\"  - codebook.npy\")\n",
    "print(f\"  - training_history.json\")\n",
    "print(f\"  - training_curves.png\")\n",
    "print(f\"  - reconstruction_*.png\")\n",
    "print(f\"  - codebook_analysis.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}