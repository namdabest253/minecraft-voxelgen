{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Block2Vec V2: Hybrid Skip-gram + CBOW Training\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "This notebook trains an **improved version** of Block2Vec that fixes the problems we discovered in V1. After training, blocks that are **semantically similar** (like all types of planks) will have similar embeddings, not just blocks that happen to appear next to each other.\n",
        "\n",
        "## Why V2?\n",
        "\n",
        "V1 had serious problems that we only discovered after evaluating the trained embeddings:\n",
        "\n",
        "| Problem | V1 Result | What It Means |\n",
        "|---------|-----------|---------------|\n",
        "| Category coherence | 20.4% | oak_planks neighbors weren't other planks |\n",
        "| Block state consistency | 0.486 | oak_stairs[facing=north] vs [facing=south] had LOW similarity |\n",
        "| Analogy accuracy | 11.1% | \"oak is to oak_log as spruce is to ?\" failed |\n",
        "\n",
        "The core issue: **Skip-gram only learns co-occurrence, not semantic similarity.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Understanding V1's Failure\n",
        "\n",
        "## What Skip-gram Actually Learned\n",
        "\n",
        "Skip-gram learns: \"Blocks that appear NEAR each other should have SIMILAR embeddings.\"\n",
        "\n",
        "This sounds reasonable, but consider:\n",
        "\n",
        "```\n",
        "In an oak house:\n",
        "  oak_planks is surrounded by: oak_stairs, oak_log, glass, torch\n",
        "  \n",
        "In a spruce house (a different build):\n",
        "  spruce_planks is surrounded by: spruce_stairs, spruce_log, glass, torch\n",
        "```\n",
        "\n",
        "**The problem:** `oak_planks` and `spruce_planks` never appear NEAR each other (they're in different builds), so skip-gram doesn't learn that they're similar!\n",
        "\n",
        "Instead, skip-gram learned:\n",
        "- `oak_planks` ≈ `oak_stairs` (they co-occur in oak builds)\n",
        "- `spruce_planks` ≈ `spruce_stairs` (they co-occur in spruce builds)\n",
        "- `oak_planks` ≉ `spruce_planks` (they never appear together!)\n",
        "\n",
        "## The Block State Disaster\n",
        "\n",
        "V1 treated every block state as a separate token:\n",
        "\n",
        "```\n",
        "Token 1234: minecraft:oak_stairs[facing=north,half=bottom]\n",
        "Token 1235: minecraft:oak_stairs[facing=south,half=bottom]\n",
        "Token 1236: minecraft:oak_stairs[facing=east,half=bottom]\n",
        "...\n",
        "```\n",
        "\n",
        "These are ALL oak stairs, but because `[facing=north]` appears on the NORTH side of buildings and `[facing=south]` appears on the SOUTH side, they have **completely different context neighborhoods**!\n",
        "\n",
        "Result: Some variants of the SAME block had **negative similarity** to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: The Two Types of Similarity\n",
        "\n",
        "## Type 1: Co-occurrence Similarity\n",
        "\n",
        "**\"Blocks that appear NEAR each other are similar.\"**\n",
        "\n",
        "This is what Skip-gram directly learns:\n",
        "\n",
        "```\n",
        "Training pair: (oak_log, oak_leaves)\n",
        "Result: oak_log embedding ≈ oak_leaves embedding\n",
        "```\n",
        "\n",
        "Because they appear together in trees, skip-gram pushes their embeddings closer.\n",
        "\n",
        "**Good for:** Learning that torches go on walls, chests go near crafting tables, ores cluster in caves.\n",
        "\n",
        "**Bad for:** Learning that oak_planks ≈ spruce_planks (they never appear together).\n",
        "\n",
        "## Type 2: Distributional Similarity\n",
        "\n",
        "**\"Blocks with similar NEIGHBORS are similar.\"**\n",
        "\n",
        "This is a different concept:\n",
        "\n",
        "```\n",
        "oak_log's typical neighbors:   {oak_leaves, air, dirt, grass}\n",
        "birch_log's typical neighbors: {birch_leaves, air, dirt, grass}\n",
        "```\n",
        "\n",
        "Notice: Both logs have **similar neighbor distributions** (leaves, air, dirt, grass). Even though `oak_log` and `birch_log` never appear NEAR each other, they appear in **similar contexts**.\n",
        "\n",
        "Distributional similarity says: If two blocks have similar neighbors, they should have similar embeddings.\n",
        "\n",
        "**Good for:** Learning that oak_planks ≈ spruce_planks (both have similar neighbor patterns).\n",
        "\n",
        "## The Key Insight\n",
        "\n",
        "We need BOTH types of similarity:\n",
        "- Co-occurrence: torch ≈ wall (they go together)\n",
        "- Distributional: oak_planks ≈ spruce_planks (similar contexts)\n",
        "\n",
        "V1 only had co-occurrence. V2 adds distributional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: CBOW - Learning Distributional Similarity\n",
        "\n",
        "## Skip-gram vs CBOW\n",
        "\n",
        "Skip-gram and CBOW are opposites:\n",
        "\n",
        "```\n",
        "SKIP-GRAM: center → predict context\n",
        "  Input:  oak_planks\n",
        "  Output: Predict [oak_stairs, glass, torch, air, ...]\n",
        "  \n",
        "CBOW: context → predict center\n",
        "  Input:  [oak_stairs, glass, torch, air, ...]\n",
        "  Output: Predict oak_planks\n",
        "```\n",
        "\n",
        "## Why CBOW Captures Distributional Similarity\n",
        "\n",
        "Consider two training examples:\n",
        "\n",
        "```\n",
        "Example 1 (oak house):\n",
        "  Context: [oak_stairs, glass, torch, air]\n",
        "  Center:  oak_planks\n",
        "  \n",
        "Example 2 (spruce house):\n",
        "  Context: [spruce_stairs, glass, torch, air]\n",
        "  Center:  spruce_planks\n",
        "```\n",
        "\n",
        "CBOW averages the context embeddings and predicts the center:\n",
        "\n",
        "```\n",
        "avg([oak_stairs, glass, torch, air]) → oak_planks\n",
        "avg([spruce_stairs, glass, torch, air]) → spruce_planks\n",
        "```\n",
        "\n",
        "If `glass`, `torch`, and `air` are the same in both, and `oak_stairs` ≈ `spruce_stairs` (both are stairs), then:\n",
        "\n",
        "```\n",
        "avg(context1) ≈ avg(context2)\n",
        "```\n",
        "\n",
        "Since similar context averages predict both `oak_planks` and `spruce_planks`, CBOW learns that:\n",
        "\n",
        "```\n",
        "oak_planks ≈ spruce_planks\n",
        "```\n",
        "\n",
        "**This is exactly what we want!**\n",
        "\n",
        "## The Math Behind CBOW\n",
        "\n",
        "1. Take all context blocks: `[ctx1, ctx2, ctx3, ...]`\n",
        "2. Look up their embeddings: `[emb1, emb2, emb3, ...]`\n",
        "3. Average them: `context_avg = mean([emb1, emb2, emb3, ...])`\n",
        "4. Use `context_avg` to predict the center block\n",
        "5. Loss = how wrong was our prediction?\n",
        "\n",
        "Blocks that appear in similar contexts get pulled together because they need to be predicted from similar context averages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: The Hybrid Approach\n",
        "\n",
        "## Why Not Just Use CBOW?\n",
        "\n",
        "CBOW has its own weakness: it loses **specific** co-occurrence information.\n",
        "\n",
        "Skip-gram knows: \"torch often appears next to wall\" (specific pair)\n",
        "CBOW knows: \"torch appears in wall-like contexts\" (averaged out)\n",
        "\n",
        "For structure generation, we want both:\n",
        "- Distributional: All planks are similar (for material substitution)\n",
        "- Co-occurrence: Torches go on walls (for structural patterns)\n",
        "\n",
        "## The Hybrid Loss\n",
        "\n",
        "V2 uses both losses together:\n",
        "\n",
        "```python\n",
        "total_loss = alpha * skipgram_loss + beta * cbow_loss\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `alpha = 1.0` (skip-gram weight)\n",
        "- `beta = 1.0` (CBOW weight)\n",
        "\n",
        "Both losses update the SAME embedding matrices, so the embeddings learn from both signals.\n",
        "\n",
        "## What the Model Learns\n",
        "\n",
        "After hybrid training:\n",
        "\n",
        "| Similarity | Source | Example |\n",
        "|------------|--------|--------|\n",
        "| oak_planks ≈ spruce_planks | CBOW | Similar contexts |\n",
        "| oak_planks ≈ oak_stairs | Skip-gram | Co-occur in builds |\n",
        "| diamond_ore ≈ emerald_ore | Both | Same caves + similar contexts |\n",
        "\n",
        "The embeddings become richer, capturing multiple types of relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Block State Collapsing\n",
        "\n",
        "## The Problem\n",
        "\n",
        "V1 vocabulary: 3,717 tokens\n",
        "V2 vocabulary: 1,007 tokens\n",
        "\n",
        "Where did 2,710 tokens go?\n",
        "\n",
        "```\n",
        "V1 had separate tokens for:\n",
        "  oak_stairs[facing=north,half=bottom,shape=straight]  → Token 29\n",
        "  oak_stairs[facing=east,half=bottom,shape=straight]   → Token 30\n",
        "  oak_stairs[facing=south,half=bottom,shape=straight]  → Token 31\n",
        "  oak_stairs[facing=west,half=bottom,shape=straight]   → Token 32\n",
        "  ... (40 total variants!)\n",
        "\n",
        "V2 collapses all to:\n",
        "  oak_stairs → Token 42\n",
        "```\n",
        "\n",
        "## Why Collapse?\n",
        "\n",
        "1. **They're the same block:** `oak_stairs[facing=north]` and `oak_stairs[facing=south]` are both oak stairs.\n",
        "\n",
        "2. **States are orientation, not identity:** The `facing` property is about which way the stairs point, not what material they are.\n",
        "\n",
        "3. **Different contexts:** North-facing stairs appear on north sides, south-facing on south sides. Skip-gram sees these as completely different blocks with different neighbors.\n",
        "\n",
        "## What We Preserve\n",
        "\n",
        "Some states ARE semantically meaningful:\n",
        "\n",
        "| Block | State | Why Keep It |\n",
        "|-------|-------|-------------|\n",
        "| water | level=0 vs level=7 | Source vs flowing water |\n",
        "| bed | part=head vs part=foot | Different parts of multi-block |\n",
        "| door | half=upper vs half=lower | Different parts of door |\n",
        "| snow | layers=1..8 | Height matters |\n",
        "\n",
        "We keep these because they represent fundamentally different things, not just orientation.\n",
        "\n",
        "## The Mapping\n",
        "\n",
        "We create a mapping from original tokens to collapsed tokens:\n",
        "\n",
        "```python\n",
        "original_to_collapsed = {\n",
        "    29: 42,   # oak_stairs[facing=north] → oak_stairs\n",
        "    30: 42,   # oak_stairs[facing=east]  → oak_stairs\n",
        "    31: 42,   # oak_stairs[facing=south] → oak_stairs\n",
        "    32: 42,   # oak_stairs[facing=west]  → oak_stairs\n",
        "    ...\n",
        "}\n",
        "```\n",
        "\n",
        "During training, we convert all builds to use collapsed tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 6: New Tracking Metrics\n",
        "\n",
        "V1 only tracked total loss. V2 tracks much more to help diagnose problems.\n",
        "\n",
        "## Loss Components\n",
        "\n",
        "```python\n",
        "history = {\n",
        "    \"total_loss\": [...],      # Combined loss\n",
        "    \"skipgram_loss\": [...],   # Skip-gram component\n",
        "    \"cbow_loss\": [...],       # CBOW component\n",
        "}\n",
        "```\n",
        "\n",
        "**Why track separately?**\n",
        "\n",
        "If skip-gram loss drops to 0.5 but CBOW stays at 2.0, something is wrong with CBOW. We can tune `alpha` and `beta` to rebalance.\n",
        "\n",
        "## Gradient Norms\n",
        "\n",
        "```python\n",
        "history = {\n",
        "    \"sg_grad_norm\": [...],    # Skip-gram gradient magnitude\n",
        "    \"cbow_grad_norm\": [...],  # CBOW gradient magnitude\n",
        "}\n",
        "```\n",
        "\n",
        "**Why track gradients?**\n",
        "\n",
        "Gradients tell us how much each loss is actually updating the model. If CBOW gradients are 10x larger than skip-gram, CBOW will dominate training regardless of loss weights.\n",
        "\n",
        "**What to look for:**\n",
        "- Balanced gradients: SG and CBOW norms are similar\n",
        "- Imbalanced: One is 10x the other → adjust weights\n",
        "\n",
        "## Category Coherence\n",
        "\n",
        "```python\n",
        "history = {\n",
        "    \"category_coherence\": [\n",
        "        {\"epoch\": 1, \"avg\": 0.25, \"per_cat\": {...}},\n",
        "        {\"epoch\": 5, \"avg\": 0.35, \"per_cat\": {...}},\n",
        "        ...\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Why track coherence during training?**\n",
        "\n",
        "V1 peaked at epoch 10 and started overfitting by epoch 20. We didn't know because we only looked at loss. By tracking coherence during training, we can see:\n",
        "\n",
        "- Epoch 5: coherence 25% (improving)\n",
        "- Epoch 10: coherence 45% (best!)\n",
        "- Epoch 15: coherence 40% (overfitting!)\n",
        "\n",
        "Now we know to use the epoch 10 checkpoint.\n",
        "\n",
        "## Loss Ratio\n",
        "\n",
        "We plot `skipgram_loss / cbow_loss` over time.\n",
        "\n",
        "- Ratio ≈ 1.0: Balanced learning\n",
        "- Ratio >> 1.0: Skip-gram not learning as fast\n",
        "- Ratio << 1.0: CBOW not learning as fast\n",
        "\n",
        "This helps diagnose if the hybrid is working or if one component is dominating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 7: Early Stopping and Checkpoints\n",
        "\n",
        "## The Overfitting Problem\n",
        "\n",
        "V1 trained for 50 epochs. Looking at the loss curve:\n",
        "\n",
        "```\n",
        "Epoch 1:  loss = 1.957\n",
        "Epoch 10: loss = 1.596  ← Best!\n",
        "Epoch 20: loss = 1.594  \n",
        "Epoch 50: loss = 1.596  ← Slightly worse than epoch 10!\n",
        "```\n",
        "\n",
        "The loss stopped improving after epoch 10, but we trained for 40 more epochs! This is **overfitting**: the model memorizes training data instead of learning general patterns.\n",
        "\n",
        "## V2 Solution: Early Stopping\n",
        "\n",
        "```python\n",
        "EPOCHS = 25        # Max epochs\n",
        "PATIENCE = 5       # Stop if no improvement for 5 epochs\n",
        "```\n",
        "\n",
        "How it works:\n",
        "1. Track the best loss seen so far\n",
        "2. If current loss is better, save the model and reset patience\n",
        "3. If current loss is worse, increment patience counter\n",
        "4. If patience counter reaches 5, stop training\n",
        "\n",
        "Example:\n",
        "```\n",
        "Epoch 1:  loss=2.0, best=2.0, patience=0 (new best!)\n",
        "Epoch 2:  loss=1.8, best=1.8, patience=0 (new best!)\n",
        "Epoch 3:  loss=1.9, best=1.8, patience=1 (worse)\n",
        "Epoch 4:  loss=1.7, best=1.7, patience=0 (new best!)\n",
        "Epoch 5:  loss=1.75, best=1.7, patience=1\n",
        "Epoch 6:  loss=1.76, best=1.7, patience=2\n",
        "Epoch 7:  loss=1.77, best=1.7, patience=3\n",
        "Epoch 8:  loss=1.78, best=1.7, patience=4\n",
        "Epoch 9:  loss=1.79, best=1.7, patience=5 → STOP!\n",
        "```\n",
        "\n",
        "We save the epoch 4 model (best loss) and stop at epoch 9.\n",
        "\n",
        "## Checkpoints\n",
        "\n",
        "We also save checkpoints every 5 epochs:\n",
        "\n",
        "```\n",
        "block2vec_v2_epoch5.pt\n",
        "block2vec_v2_epoch10.pt\n",
        "block2vec_v2_epoch15.pt\n",
        "block2vec_v2_best.pt  ← Best by loss\n",
        "```\n",
        "\n",
        "**Why keep checkpoints?**\n",
        "\n",
        "Sometimes the \"best loss\" model isn't the best for downstream tasks. If epoch 10 has the best category coherence but epoch 15 has the best loss, we might want epoch 10. Checkpoints let us try different epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 8: What Could Still Go Wrong\n",
        "\n",
        "Even with all these improvements, V2 might not work perfectly. Here are potential issues we're watching for:\n",
        "\n",
        "## Issue 1: Loss Weight Imbalance\n",
        "\n",
        "If `beta` (CBOW weight) is too high, we might lose co-occurrence learning. If `alpha` is too high, we don't fix the problem.\n",
        "\n",
        "**How to detect:** Look at the loss ratio plot. If it's far from 1.0, adjust weights.\n",
        "\n",
        "## Issue 2: CBOW Gradient Dominance\n",
        "\n",
        "CBOW averages 6 context embeddings into 1 prediction. Skip-gram makes 6 separate predictions. The gradient magnitudes might be different.\n",
        "\n",
        "**How to detect:** Look at gradient norm plot. If CBOW >> Skip-gram, CBOW dominates.\n",
        "\n",
        "## Issue 3: Collapsing Too Much\n",
        "\n",
        "We collapsed `water[level=0]` and `water[level=7]` separately (source vs flowing). But what if we missed something important?\n",
        "\n",
        "**How to detect:** Check if water blocks cluster correctly in evaluation.\n",
        "\n",
        "## Issue 4: Smaller Vocab = Faster Overfitting\n",
        "\n",
        "With 1,007 tokens instead of 3,717, there are fewer parameters to train. The model might memorize faster.\n",
        "\n",
        "**How to detect:** Watch category coherence. If it peaks early then drops, we're overfitting.\n",
        "\n",
        "## Issue 5: Semantic vs Structural Confusion\n",
        "\n",
        "Skip-gram pulls oak_planks toward oak_stairs (structural).\n",
        "CBOW pulls oak_planks toward spruce_planks (semantic).\n",
        "\n",
        "These are opposite directions! The embedding might end up as a confused compromise.\n",
        "\n",
        "**How to detect:** Check nearest neighbors. If oak_planks has both planks AND stairs as neighbors, the hybrid is working. If it has random blocks, the signals are conflicting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 9: Let's Start Training!\n",
        "\n",
        "Now that you understand the concepts, let's implement it. The code below is extensively commented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 1: Imports and Setup\n",
        "# ============================================================\n",
        "# These are the libraries we need:\n",
        "# - torch: PyTorch, the deep learning framework\n",
        "# - numpy: For numerical operations on arrays\n",
        "# - h5py: For reading HDF5 files (our training data)\n",
        "# - json: For reading the vocabulary file\n",
        "# - matplotlib: For visualization\n",
        "# - sklearn: For t-SNE and cosine similarity\n",
        "\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Iterator, Optional\n",
        "\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 2: Configuration\n",
        "# ============================================================\n",
        "# V2 configuration with changes from V1 highlighted\n",
        "\n",
        "# === Data Paths ===\n",
        "DATA_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/train\"\n",
        "VOCAB_PATH = \"/kaggle/input/minecraft-schematics/tok2block.json\"  # Original (3,717 tokens)\n",
        "COLLAPSED_VOCAB_PATH = \"/kaggle/input/minecraft-schematics/v2/tok2block_collapsed.json\"  # NEW (1,007 tokens)\n",
        "MAPPING_PATH = \"/kaggle/input/minecraft-schematics/v2/original_to_collapsed.json\"  # NEW\n",
        "OUTPUT_DIR = \"/kaggle/working\"\n",
        "\n",
        "# === Model Architecture ===\n",
        "EMBEDDING_DIM = 32  # Same as V1\n",
        "\n",
        "# === V2 Hybrid Loss Weights ===\n",
        "# alpha controls skip-gram (co-occurrence)\n",
        "# beta controls CBOW (distributional similarity)\n",
        "ALPHA = 1.0  # Skip-gram weight\n",
        "BETA = 1.0   # CBOW weight - NEW!\n",
        "\n",
        "# === Training Hyperparameters ===\n",
        "EPOCHS = 25          # V1 was 50, but peaked at 10. V2 uses 25 with early stopping\n",
        "BATCH_SIZE = 4096    # Slightly smaller than V1 (8192) due to CBOW overhead\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "PATIENCE = 5         # NEW: Early stopping patience\n",
        "\n",
        "# === Negative Sampling ===\n",
        "NUM_NEGATIVE_SAMPLES = 15  # V1 was 5. More negatives = better discrimination\n",
        "\n",
        "# === Subsampling ===\n",
        "SUBSAMPLE_THRESHOLD = 0.0001  # V1 was 0.001. More aggressive\n",
        "INCLUDE_AIR = False           # NEW: Completely exclude air blocks\n",
        "\n",
        "# === Checkpointing ===\n",
        "CHECKPOINT_EVERY = 5  # Save checkpoint every N epochs\n",
        "EVAL_EVERY = 5        # Evaluate category coherence every N epochs\n",
        "\n",
        "# === Other ===\n",
        "SEED = 42\n",
        "\n",
        "print(\"V2 Configuration:\")\n",
        "print(f\"  Epochs: {EPOCHS} (with early stopping, patience={PATIENCE})\")\n",
        "print(f\"  Loss weights: alpha={ALPHA} (skip-gram), beta={BETA} (CBOW)\")\n",
        "print(f\"  Negative samples: {NUM_NEGATIVE_SAMPLES} (V1 was 5)\")\n",
        "print(f\"  Include air: {INCLUDE_AIR} (V1 was True with subsampling)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3: Load Vocabularies\n",
        "# ============================================================\n",
        "# V2 uses TWO vocabularies:\n",
        "# 1. Original: 3,717 tokens (what's in the H5 files)\n",
        "# 2. Collapsed: 1,007 tokens (what we train on)\n",
        "# Plus a mapping between them\n",
        "\n",
        "# Original vocabulary (for reference)\n",
        "with open(VOCAB_PATH, 'r') as f:\n",
        "    tok2block_original = {int(k): v for k, v in json.load(f).items()}\n",
        "print(f\"Original vocabulary: {len(tok2block_original)} tokens\")\n",
        "\n",
        "# Collapsed vocabulary (what we train on)\n",
        "with open(COLLAPSED_VOCAB_PATH, 'r') as f:\n",
        "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
        "VOCAB_SIZE = len(tok2block)\n",
        "print(f\"Collapsed vocabulary: {VOCAB_SIZE} tokens\")\n",
        "print(f\"Reduction: {len(tok2block_original) - VOCAB_SIZE} tokens removed ({100*(1-VOCAB_SIZE/len(tok2block_original)):.1f}%)\")\n",
        "\n",
        "# Mapping: original token ID → collapsed token ID\n",
        "with open(MAPPING_PATH, 'r') as f:\n",
        "    original_to_collapsed = {int(k): int(v) for k, v in json.load(f).items()}\n",
        "\n",
        "# Find air token in collapsed vocabulary\n",
        "AIR_TOKEN = None\n",
        "for tok, name in tok2block.items():\n",
        "    if name == \"minecraft:air\":\n",
        "        AIR_TOKEN = tok\n",
        "        break\n",
        "print(f\"\\nAir token (collapsed): {AIR_TOKEN}\")\n",
        "\n",
        "# Show some examples of collapsing\n",
        "print(\"\\nExample collapses:\")\n",
        "shown = set()\n",
        "for orig_tok, collapsed_tok in list(original_to_collapsed.items())[:500]:\n",
        "    orig_name = tok2block_original.get(orig_tok, \"?\")\n",
        "    collapsed_name = tok2block.get(collapsed_tok, \"?\")\n",
        "    if \"[\" in orig_name and collapsed_name not in shown and len(shown) < 5:\n",
        "        print(f\"  {orig_name}\")\n",
        "        print(f\"    → {collapsed_name}\")\n",
        "        shown.add(collapsed_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4: Category Coherence Evaluation Function\n",
        "# ============================================================\n",
        "# We define this early so we can use it during training.\n",
        "# This measures: \"For each planks block, are its nearest neighbors also planks?\"\n",
        "\n",
        "# Categories we'll evaluate\n",
        "# These are blocks that SHOULD be similar to each other\n",
        "EVAL_CATEGORIES = {\n",
        "    \"planks\": [\"oak_planks\", \"spruce_planks\", \"birch_planks\", \"jungle_planks\", \n",
        "               \"acacia_planks\", \"dark_oak_planks\", \"crimson_planks\", \"warped_planks\"],\n",
        "    \"logs\": [\"oak_log\", \"spruce_log\", \"birch_log\", \"jungle_log\", \n",
        "             \"acacia_log\", \"dark_oak_log\", \"crimson_stem\", \"warped_stem\"],\n",
        "    \"stone\": [\"stone\", \"cobblestone\", \"stone_bricks\", \"andesite\", \"diorite\", \"granite\"],\n",
        "    \"ores\": [\"coal_ore\", \"iron_ore\", \"gold_ore\", \"diamond_ore\", \"emerald_ore\"],\n",
        "    \"wool\": [\"white_wool\", \"red_wool\", \"blue_wool\", \"green_wool\", \"yellow_wool\"],\n",
        "}\n",
        "\n",
        "def find_token_for_block(block_name, tok2block):\n",
        "    \"\"\"Find the token ID for a block name.\"\"\"\n",
        "    for tok, name in tok2block.items():\n",
        "        if f\"minecraft:{block_name}\" == name:\n",
        "            return tok\n",
        "    return None\n",
        "\n",
        "def evaluate_category_coherence(embeddings, tok2block, categories, k=5):\n",
        "    \"\"\"\n",
        "    For each block in each category, check if its k nearest neighbors\n",
        "    are also in the same category.\n",
        "    \n",
        "    Returns a dict of category → precision (0.0 to 1.0)\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Handle both torch tensors and numpy arrays\n",
        "    if torch.is_tensor(embeddings):\n",
        "        embeddings = embeddings.cpu().numpy()\n",
        "    \n",
        "    for cat_name, block_names in categories.items():\n",
        "        # Get tokens for blocks in this category\n",
        "        tokens = [find_token_for_block(b, tok2block) for b in block_names]\n",
        "        tokens = [t for t in tokens if t is not None]\n",
        "        \n",
        "        if len(tokens) < 2:\n",
        "            continue\n",
        "        \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for token in tokens:\n",
        "            # Get k nearest neighbors\n",
        "            query = embeddings[token].reshape(1, -1)\n",
        "            sims = cosine_similarity(query, embeddings)[0]\n",
        "            top_indices = np.argsort(sims)[::-1][1:k+1]  # Exclude self\n",
        "            \n",
        "            # Check if neighbors are in same category\n",
        "            for idx in top_indices:\n",
        "                total += 1\n",
        "                if idx in tokens:\n",
        "                    correct += 1\n",
        "        \n",
        "        if total > 0:\n",
        "            results[cat_name] = correct / total\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Evaluation function defined.\")\n",
        "print(f\"Will evaluate {len(EVAL_CATEGORIES)} categories: {list(EVAL_CATEGORIES.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 10: The V2 Model\n",
        "\n",
        "The key change from V1 is adding CBOW loss alongside skip-gram loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 5: Block2Vec V2 Model\n",
        "# ============================================================\n",
        "\n",
        "class Block2VecV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Hybrid Skip-gram + CBOW model for block embeddings.\n",
        "    \n",
        "    The model has TWO embedding matrices (same as V1):\n",
        "    1. center_embeddings: Used when a block is the CENTER\n",
        "    2. context_embeddings: Used when a block is in the CONTEXT\n",
        "    \n",
        "    NEW in V2: We compute BOTH losses and combine them:\n",
        "    - Skip-gram: center → predict each context\n",
        "    - CBOW: average of contexts → predict center\n",
        "    \n",
        "    Total loss = alpha * skipgram_loss + beta * cbow_loss\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim=32, alpha=1.0, beta=1.0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.alpha = alpha  # Skip-gram weight\n",
        "        self.beta = beta    # CBOW weight\n",
        "        \n",
        "        # Two embedding matrices\n",
        "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # Initialize with small random values\n",
        "        init_range = 0.5 / embedding_dim\n",
        "        self.center_embeddings.weight.data.uniform_(-init_range, init_range)\n",
        "        self.context_embeddings.weight.data.uniform_(-init_range, init_range)\n",
        "    \n",
        "    def forward(self, center_ids, context_id, negative_ids, all_context_ids, context_mask):\n",
        "        \"\"\"\n",
        "        Compute hybrid loss.\n",
        "        \n",
        "        Args:\n",
        "            center_ids: Center block tokens [batch_size]\n",
        "            context_id: ONE positive context [batch_size] (for skip-gram)\n",
        "            negative_ids: Negative samples [batch_size, num_negatives]\n",
        "            all_context_ids: ALL context tokens [batch_size, 6] (for CBOW)\n",
        "            context_mask: Which contexts are valid [batch_size, 6]\n",
        "        \"\"\"\n",
        "        \n",
        "        # =========================================\n",
        "        # SKIP-GRAM LOSS: center → predict context\n",
        "        # =========================================\n",
        "        \n",
        "        # Get embeddings\n",
        "        center_emb = self.center_embeddings(center_ids)      # [B, D]\n",
        "        context_emb = self.context_embeddings(context_id)    # [B, D]\n",
        "        \n",
        "        # Positive score: dot product of center and context\n",
        "        # We want this to be HIGH\n",
        "        pos_score = torch.sum(center_emb * context_emb, dim=1)  # [B]\n",
        "        pos_loss = F.logsigmoid(pos_score)  # log(sigmoid(x)) → 0 when x is large\n",
        "        \n",
        "        # Negative scores: center against random blocks\n",
        "        # We want these to be LOW\n",
        "        neg_emb = self.context_embeddings(negative_ids)  # [B, N, D]\n",
        "        neg_score = torch.sum(center_emb.unsqueeze(1) * neg_emb, dim=2)  # [B, N]\n",
        "        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)  # log(sigmoid(-x)) → 0 when x is small\n",
        "        \n",
        "        # Skip-gram total (negate because we maximize log-likelihood)\n",
        "        skipgram_loss = -(pos_loss + neg_loss).mean()\n",
        "        \n",
        "        # =========================================\n",
        "        # CBOW LOSS: context average → predict center\n",
        "        # =========================================\n",
        "        \n",
        "        # Get context embeddings for ALL neighbors\n",
        "        ctx_emb = self.center_embeddings(all_context_ids)  # [B, 6, D]\n",
        "        \n",
        "        # Apply mask (some contexts might be invalid - out of bounds or air)\n",
        "        mask_exp = context_mask.unsqueeze(-1).float()  # [B, 6, 1]\n",
        "        \n",
        "        # Compute masked average\n",
        "        ctx_sum = (ctx_emb * mask_exp).sum(dim=1)  # [B, D]\n",
        "        ctx_count = context_mask.sum(dim=1, keepdim=True).float().clamp(min=1)  # [B, 1]\n",
        "        ctx_avg = ctx_sum / ctx_count  # [B, D] - this is the \"context representation\"\n",
        "        \n",
        "        # CBOW positive: context average should predict center\n",
        "        # We use context_embeddings for the output (standard practice)\n",
        "        center_out = self.context_embeddings(center_ids)  # [B, D]\n",
        "        cbow_pos = torch.sum(ctx_avg * center_out, dim=1)  # [B]\n",
        "        cbow_pos_loss = F.logsigmoid(cbow_pos)\n",
        "        \n",
        "        # CBOW negatives: context average should NOT predict random blocks\n",
        "        cbow_neg_score = torch.sum(ctx_avg.unsqueeze(1) * neg_emb, dim=2)  # [B, N]\n",
        "        cbow_neg_loss = F.logsigmoid(-cbow_neg_score).sum(dim=1)  # [B]\n",
        "        \n",
        "        # CBOW total\n",
        "        cbow_loss = -(cbow_pos_loss + cbow_neg_loss).mean()\n",
        "        \n",
        "        # =========================================\n",
        "        # COMBINED LOSS\n",
        "        # =========================================\n",
        "        total_loss = self.alpha * skipgram_loss + self.beta * cbow_loss\n",
        "        \n",
        "        # Return all losses for tracking\n",
        "        return {\n",
        "            \"total_loss\": total_loss,\n",
        "            \"skipgram_loss\": skipgram_loss,\n",
        "            \"cbow_loss\": cbow_loss,\n",
        "        }\n",
        "    \n",
        "    def get_embeddings(self):\n",
        "        \"\"\"Return the learned embeddings (center embeddings).\"\"\"\n",
        "        return self.center_embeddings.weight.data.clone()\n",
        "\n",
        "\n",
        "# Test it\n",
        "model = Block2VecV2(VOCAB_SIZE, EMBEDDING_DIM, ALPHA, BETA)\n",
        "print(f\"Model created!\")\n",
        "print(f\"  Vocabulary: {VOCAB_SIZE} tokens\")\n",
        "print(f\"  Embedding dim: {EMBEDDING_DIM}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  = {VOCAB_SIZE} × {EMBEDDING_DIM} × 2 matrices\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 11: The V2 Dataset\n",
        "\n",
        "The dataset handles:\n",
        "1. Loading H5 files\n",
        "2. Collapsing block states using our mapping\n",
        "3. Extracting (center, all_contexts) for each block\n",
        "4. Sampling negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 6: V2 Dataset with State Collapsing\n",
        "# ============================================================\n",
        "\n",
        "class Block2VecDatasetV2(IterableDataset):\n",
        "    \"\"\"\n",
        "    V2 Dataset changes from V1:\n",
        "    1. Collapses block states during loading\n",
        "    2. Returns ALL 6 contexts (for CBOW), not just one\n",
        "    3. Completely excludes air blocks\n",
        "    \"\"\"\n",
        "    \n",
        "    # The 6 neighbors: up, down, left, right, front, back\n",
        "    NEIGHBORS_6 = [\n",
        "        (-1, 0, 0), (1, 0, 0),   # y-axis (up/down in Minecraft)\n",
        "        (0, -1, 0), (0, 1, 0),   # x-axis\n",
        "        (0, 0, -1), (0, 0, 1),   # z-axis\n",
        "    ]\n",
        "    \n",
        "    def __init__(self, data_dir, vocab_size, original_to_collapsed,\n",
        "                 num_negative_samples=15, subsample_threshold=0.0001,\n",
        "                 air_token=0, include_air=False, seed=42):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.original_to_collapsed = original_to_collapsed\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "        self.subsample_threshold = subsample_threshold\n",
        "        self.air_token = air_token\n",
        "        self.include_air = include_air\n",
        "        self.seed = seed\n",
        "        \n",
        "        # Find all H5 files\n",
        "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
        "        print(f\"Found {len(self.h5_files)} training files\")\n",
        "        \n",
        "        # Will be computed on first iteration\n",
        "        self._block_freqs = None\n",
        "        self._negative_table = None\n",
        "        self._subsample_probs = None\n",
        "    \n",
        "    def _collapse_build(self, build):\n",
        "        \"\"\"\n",
        "        Convert a build from original tokens to collapsed tokens.\n",
        "        \n",
        "        Example:\n",
        "          Input:  [29, 30, 31, 32]  (different oak_stairs orientations)\n",
        "          Output: [42, 42, 42, 42]  (all collapsed to oak_stairs)\n",
        "        \"\"\"\n",
        "        collapsed = np.zeros_like(build)\n",
        "        for orig_id, collapsed_id in self.original_to_collapsed.items():\n",
        "            collapsed[build == orig_id] = collapsed_id\n",
        "        return collapsed\n",
        "    \n",
        "    def _compute_frequencies(self):\n",
        "        \"\"\"Count block frequencies for subsampling and negative sampling.\"\"\"\n",
        "        print(\"Computing collapsed block frequencies...\")\n",
        "        freqs = np.zeros(self.vocab_size, dtype=np.float64)\n",
        "        \n",
        "        for h5_path in tqdm(self.h5_files, desc=\"Counting blocks\"):\n",
        "            try:\n",
        "                with h5py.File(h5_path, 'r') as f:\n",
        "                    build = f[list(f.keys())[0]][:]\n",
        "                collapsed = self._collapse_build(build)\n",
        "                unique, counts = np.unique(collapsed, return_counts=True)\n",
        "                for tok, count in zip(unique, counts):\n",
        "                    if tok < self.vocab_size:\n",
        "                        freqs[tok] += count\n",
        "            except Exception:\n",
        "                continue\n",
        "        \n",
        "        # Normalize to probabilities\n",
        "        freqs /= freqs.sum()\n",
        "        self._block_freqs = freqs\n",
        "        \n",
        "        # Build negative sampling table (frequency^0.75 weighting)\n",
        "        weighted = np.power(freqs + 1e-10, 0.75)\n",
        "        weighted /= weighted.sum()\n",
        "        self._negative_table = np.random.choice(\n",
        "            self.vocab_size, size=100_000_000, p=weighted\n",
        "        )\n",
        "        \n",
        "        # Compute subsampling probabilities\n",
        "        self._subsample_probs = np.ones(self.vocab_size, dtype=np.float32)\n",
        "        for i, freq in enumerate(freqs):\n",
        "            if freq > self.subsample_threshold:\n",
        "                self._subsample_probs[i] = np.sqrt(self.subsample_threshold / freq)\n",
        "        \n",
        "        print(f\"  Air frequency: {freqs[self.air_token]:.4f}\")\n",
        "        print(f\"  Non-zero blocks: {(freqs > 0).sum()}/{self.vocab_size}\")\n",
        "    \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield training examples.\"\"\"\n",
        "        if self._block_freqs is None:\n",
        "            self._compute_frequencies()\n",
        "        \n",
        "        # Handle multi-worker loading\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        files = self.h5_files\n",
        "        worker_id = 0\n",
        "        if worker_info:\n",
        "            per_worker = len(self.h5_files) // worker_info.num_workers\n",
        "            worker_id = worker_info.id\n",
        "            start = worker_id * per_worker\n",
        "            end = start + per_worker if worker_id < worker_info.num_workers - 1 else len(self.h5_files)\n",
        "            files = self.h5_files[start:end]\n",
        "        \n",
        "        rng = random.Random(self.seed + worker_id)\n",
        "        \n",
        "        for h5_path in files:\n",
        "            try:\n",
        "                with h5py.File(h5_path, 'r') as f:\n",
        "                    build = f[list(f.keys())[0]][:]\n",
        "            except Exception:\n",
        "                continue\n",
        "            \n",
        "            # Collapse block states\n",
        "            build = self._collapse_build(build)\n",
        "            h, w, d = build.shape\n",
        "            \n",
        "            # Iterate through every position\n",
        "            for y in range(h):\n",
        "                for x in range(w):\n",
        "                    for z in range(d):\n",
        "                        center = int(build[y, x, z])\n",
        "                        \n",
        "                        # Skip air (V2 excludes air entirely)\n",
        "                        if not self.include_air and center == self.air_token:\n",
        "                            continue\n",
        "                        \n",
        "                        # Subsampling check (skip frequent blocks sometimes)\n",
        "                        if rng.random() >= self._subsample_probs[center]:\n",
        "                            continue\n",
        "                        \n",
        "                        # Get ALL 6 neighbors (for CBOW)\n",
        "                        all_contexts = []\n",
        "                        context_mask = []\n",
        "                        \n",
        "                        for dy, dx, dz in self.NEIGHBORS_6:\n",
        "                            ny, nx, nz = y + dy, x + dx, z + dz\n",
        "                            \n",
        "                            if 0 <= ny < h and 0 <= nx < w and 0 <= nz < d:\n",
        "                                ctx = int(build[ny, nx, nz])\n",
        "                                if not self.include_air and ctx == self.air_token:\n",
        "                                    # Air context - mark as invalid\n",
        "                                    all_contexts.append(0)\n",
        "                                    context_mask.append(False)\n",
        "                                else:\n",
        "                                    all_contexts.append(ctx)\n",
        "                                    context_mask.append(True)\n",
        "                            else:\n",
        "                                # Out of bounds - mark as invalid\n",
        "                                all_contexts.append(0)\n",
        "                                context_mask.append(False)\n",
        "                        \n",
        "                        # Skip if no valid contexts\n",
        "                        if not any(context_mask):\n",
        "                            continue\n",
        "                        \n",
        "                        # Sample negatives\n",
        "                        neg_indices = rng.sample(range(len(self._negative_table)), self.num_negative_samples)\n",
        "                        negatives = self._negative_table[neg_indices]\n",
        "                        \n",
        "                        # Yield one example PER VALID CONTEXT (for skip-gram)\n",
        "                        # But include ALL contexts (for CBOW)\n",
        "                        for ctx, valid in zip(all_contexts, context_mask):\n",
        "                            if valid:\n",
        "                                yield (\n",
        "                                    center,\n",
        "                                    ctx,  # Single context for skip-gram\n",
        "                                    negatives,\n",
        "                                    np.array(all_contexts, dtype=np.int64),  # All contexts for CBOW\n",
        "                                    np.array(context_mask, dtype=bool),\n",
        "                                )\n",
        "\n",
        "\n",
        "def collate_fn_v2(batch):\n",
        "    \"\"\"Convert list of examples to tensors.\"\"\"\n",
        "    centers = torch.tensor([b[0] for b in batch], dtype=torch.long)\n",
        "    contexts = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
        "    negatives = torch.tensor(np.stack([b[2] for b in batch]), dtype=torch.long)\n",
        "    all_contexts = torch.tensor(np.stack([b[3] for b in batch]), dtype=torch.long)\n",
        "    masks = torch.tensor(np.stack([b[4] for b in batch]), dtype=torch.bool)\n",
        "    return centers, contexts, negatives, all_contexts, masks\n",
        "\n",
        "\n",
        "print(\"V2 Dataset class defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 7: Create DataLoader\n",
        "# ============================================================\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "train_dataset = Block2VecDatasetV2(\n",
        "    data_dir=DATA_DIR,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    original_to_collapsed=original_to_collapsed,\n",
        "    num_negative_samples=NUM_NEGATIVE_SAMPLES,\n",
        "    subsample_threshold=SUBSAMPLE_THRESHOLD,\n",
        "    air_token=AIR_TOKEN,\n",
        "    include_air=INCLUDE_AIR,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn_v2,\n",
        "    pin_memory=(device == \"cuda\"),\n",
        ")\n",
        "\n",
        "print(f\"DataLoader ready with batch size {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 8: Setup Model and Optimizer\n",
        "# ============================================================\n",
        "\n",
        "model = Block2VecV2(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    alpha=ALPHA,\n",
        "    beta=BETA\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "# History tracking - V2 tracks much more than V1!\n",
        "history = {\n",
        "    \"total_loss\": [],\n",
        "    \"skipgram_loss\": [],    # NEW: track SG separately\n",
        "    \"cbow_loss\": [],        # NEW: track CBOW separately\n",
        "    \"sg_grad_norm\": [],     # NEW: gradient magnitudes\n",
        "    \"cbow_grad_norm\": [],\n",
        "    \"category_coherence\": [],  # NEW: quality metric during training\n",
        "    \"best_epoch\": 0,\n",
        "}\n",
        "\n",
        "print(f\"Model on {device}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 12: Training Loop\n",
        "\n",
        "The training loop is similar to V1 but with:\n",
        "- Separate loss tracking\n",
        "- Gradient norm monitoring\n",
        "- Category coherence evaluation\n",
        "- Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 9: Training Loop with Enhanced Tracking\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Starting V2 Training (Hybrid Skip-gram + CBOW)\")\n",
        "print(f\"Epochs: {EPOCHS}, Early stopping patience: {PATIENCE}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_stats = {\"total\": 0, \"sg\": 0, \"cbow\": 0, \"sg_grad\": 0, \"cbow_grad\": 0}\n",
        "    num_batches = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    for batch in pbar:\n",
        "        # Unpack and move to device\n",
        "        center_ids, context_ids, neg_ids, all_ctx, ctx_mask = [b.to(device) for b in batch]\n",
        "        \n",
        "        # Forward pass\n",
        "        losses = model(center_ids, context_ids, neg_ids, all_ctx, ctx_mask)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses[\"total_loss\"].backward()\n",
        "        \n",
        "        # Compute gradient norms BEFORE clipping\n",
        "        # This tells us how much each component is contributing\n",
        "        sg_grad = 0\n",
        "        cbow_grad = 0\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad_norm = param.grad.norm().item()\n",
        "                # center_embeddings get gradient from both SG and CBOW\n",
        "                # context_embeddings get gradient from both SG and CBOW\n",
        "                if 'center' in name:\n",
        "                    sg_grad += grad_norm\n",
        "                else:\n",
        "                    cbow_grad += grad_norm\n",
        "        \n",
        "        # Gradient clipping (prevents exploding gradients)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track statistics\n",
        "        epoch_stats[\"total\"] += losses[\"total_loss\"].item()\n",
        "        epoch_stats[\"sg\"] += losses[\"skipgram_loss\"].item()\n",
        "        epoch_stats[\"cbow\"] += losses[\"cbow_loss\"].item()\n",
        "        epoch_stats[\"sg_grad\"] += sg_grad\n",
        "        epoch_stats[\"cbow_grad\"] += cbow_grad\n",
        "        num_batches += 1\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            \"loss\": f\"{losses['total_loss'].item():.3f}\",\n",
        "            \"sg\": f\"{losses['skipgram_loss'].item():.3f}\",\n",
        "            \"cbow\": f\"{losses['cbow_loss'].item():.3f}\",\n",
        "        })\n",
        "    \n",
        "    # Compute epoch averages\n",
        "    avg_total = epoch_stats[\"total\"] / num_batches\n",
        "    avg_sg = epoch_stats[\"sg\"] / num_batches\n",
        "    avg_cbow = epoch_stats[\"cbow\"] / num_batches\n",
        "    avg_sg_grad = epoch_stats[\"sg_grad\"] / num_batches\n",
        "    avg_cbow_grad = epoch_stats[\"cbow_grad\"] / num_batches\n",
        "    \n",
        "    # Store in history\n",
        "    history[\"total_loss\"].append(avg_total)\n",
        "    history[\"skipgram_loss\"].append(avg_sg)\n",
        "    history[\"cbow_loss\"].append(avg_cbow)\n",
        "    history[\"sg_grad_norm\"].append(avg_sg_grad)\n",
        "    history[\"cbow_grad_norm\"].append(avg_cbow_grad)\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1}: loss={avg_total:.4f}, sg={avg_sg:.4f}, cbow={avg_cbow:.4f}, \"\n",
        "          f\"sg_grad={avg_sg_grad:.2f}, cbow_grad={avg_cbow_grad:.2f}, time={elapsed:.0f}s\")\n",
        "    \n",
        "    # Evaluate category coherence periodically\n",
        "    if (epoch + 1) % EVAL_EVERY == 0 or epoch == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            emb = model.get_embeddings().cpu().numpy()\n",
        "            coherence = evaluate_category_coherence(emb, tok2block, EVAL_CATEGORIES)\n",
        "            avg_coh = np.mean(list(coherence.values()))\n",
        "            history[\"category_coherence\"].append({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"avg\": avg_coh,\n",
        "                \"per_cat\": coherence\n",
        "            })\n",
        "            print(f\"  Category coherence: {avg_coh*100:.1f}% (V1 was 20.4%)\")\n",
        "        model.train()\n",
        "    \n",
        "    # Save checkpoint\n",
        "    if (epoch + 1) % CHECKPOINT_EVERY == 0:\n",
        "        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/block2vec_v2_epoch{epoch+1}.pt\")\n",
        "        print(f\"  Checkpoint saved: epoch {epoch+1}\")\n",
        "    \n",
        "    # Early stopping check\n",
        "    if avg_total < best_loss:\n",
        "        best_loss = avg_total\n",
        "        history[\"best_epoch\"] = epoch + 1\n",
        "        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/block2vec_v2_best.pt\")\n",
        "        patience_counter = 0\n",
        "        print(f\"  -> New best model!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch+1} (no improvement for {PATIENCE} epochs)\")\n",
        "            break\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"Training complete in {time.time() - start_time:.0f}s\")\n",
        "print(f\"Best epoch: {history['best_epoch']}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 10: Save Results\n",
        "# ============================================================\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(f\"{OUTPUT_DIR}/block2vec_v2_best.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# Save embeddings\n",
        "embeddings = model.get_embeddings().cpu().numpy()\n",
        "np.save(f\"{OUTPUT_DIR}/block_embeddings_v2.npy\", embeddings)\n",
        "print(f\"Embeddings saved: {embeddings.shape}\")\n",
        "\n",
        "# Save training history\n",
        "with open(f\"{OUTPUT_DIR}/training_history_v2.json\", 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "print(\"Training history saved\")\n",
        "\n",
        "# Save vocabulary\n",
        "with open(f\"{OUTPUT_DIR}/tok2block_collapsed.json\", 'w') as f:\n",
        "    json.dump({str(k): v for k, v in tok2block.items()}, f, indent=2)\n",
        "print(\"Vocabulary saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 11: Plot Training Analysis\n",
        "# ============================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: All losses\n",
        "axes[0,0].plot(history[\"total_loss\"], 'b-', label='Total', linewidth=2)\n",
        "axes[0,0].plot(history[\"skipgram_loss\"], 'g--', label='Skip-gram', linewidth=1.5)\n",
        "axes[0,0].plot(history[\"cbow_loss\"], 'r--', label='CBOW', linewidth=1.5)\n",
        "axes[0,0].axvline(history[\"best_epoch\"]-1, color='k', linestyle=':', \n",
        "                  label=f'Best (epoch {history[\"best_epoch\"]})')\n",
        "axes[0,0].set_xlabel('Epoch')\n",
        "axes[0,0].set_ylabel('Loss')\n",
        "axes[0,0].set_title('Training Losses')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Gradient norms\n",
        "axes[0,1].plot(history[\"sg_grad_norm\"], 'g-', label='Center embeddings', linewidth=2)\n",
        "axes[0,1].plot(history[\"cbow_grad_norm\"], 'r-', label='Context embeddings', linewidth=2)\n",
        "axes[0,1].set_xlabel('Epoch')\n",
        "axes[0,1].set_ylabel('Gradient Norm')\n",
        "axes[0,1].set_title('Gradient Norms (detect imbalance)')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Category coherence\n",
        "if history[\"category_coherence\"]:\n",
        "    epochs = [c[\"epoch\"] for c in history[\"category_coherence\"]]\n",
        "    coherences = [c[\"avg\"] * 100 for c in history[\"category_coherence\"]]\n",
        "    axes[1,0].plot(epochs, coherences, 'b-o', linewidth=2, markersize=8)\n",
        "    axes[1,0].axhline(20.4, color='r', linestyle='--', label='V1 baseline (20.4%)')\n",
        "    axes[1,0].set_xlabel('Epoch')\n",
        "    axes[1,0].set_ylabel('Category Coherence (%)')\n",
        "    axes[1,0].set_title('Category Coherence Over Training')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Loss ratio\n",
        "loss_ratio = [sg/cbow if cbow > 0 else 1 for sg, cbow in \n",
        "              zip(history[\"skipgram_loss\"], history[\"cbow_loss\"])]\n",
        "axes[1,1].plot(loss_ratio, 'purple', linewidth=2)\n",
        "axes[1,1].axhline(1.0, color='k', linestyle='--', label='Balanced (ratio=1)')\n",
        "axes[1,1].set_xlabel('Epoch')\n",
        "axes[1,1].set_ylabel('Skip-gram / CBOW Loss')\n",
        "axes[1,1].set_title('Loss Ratio (detect if one dominates)')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/training_analysis_v2.png\", dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 12: t-SNE Visualization\n",
        "# ============================================================\n",
        "\n",
        "import re\n",
        "\n",
        "def get_block_category(block_name):\n",
        "    \"\"\"Categorize a block for coloring in t-SNE.\"\"\"\n",
        "    name = block_name.replace(\"minecraft:\", \"\")\n",
        "    name = re.sub(r\"\\[.*\\]\", \"\", name)\n",
        "    \n",
        "    if any(x in name for x in [\"planks\", \"log\", \"wood\", \"fence\", \"door\"]):\n",
        "        return \"wood\"\n",
        "    elif any(x in name for x in [\"stone\", \"cobble\", \"brick\", \"andesite\", \"diorite\", \"granite\"]):\n",
        "        return \"stone\"\n",
        "    elif \"ore\" in name or any(x in name for x in [\"diamond\", \"gold\", \"iron\", \"coal\", \"emerald\"]):\n",
        "        return \"ore/mineral\"\n",
        "    elif \"glass\" in name:\n",
        "        return \"glass\"\n",
        "    elif \"wool\" in name or \"carpet\" in name:\n",
        "        return \"wool\"\n",
        "    elif \"concrete\" in name:\n",
        "        return \"concrete\"\n",
        "    elif \"leaves\" in name:\n",
        "        return \"leaves\"\n",
        "    else:\n",
        "        return \"other\"\n",
        "\n",
        "# Sample for visualization\n",
        "sample_size = min(500, VOCAB_SIZE)\n",
        "indices = np.random.choice(VOCAB_SIZE, sample_size, replace=False)\n",
        "sampled_embeddings = embeddings[indices]\n",
        "\n",
        "categories = [get_block_category(tok2block.get(i, \"unknown\")) for i in indices]\n",
        "unique_cats = list(set(categories))\n",
        "\n",
        "print(f\"Running t-SNE on {len(indices)} blocks...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "coords = tsne.fit_transform(sampled_embeddings)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "cmap = plt.cm.get_cmap('tab10', len(unique_cats))\n",
        "\n",
        "for i, cat in enumerate(unique_cats):\n",
        "    mask = [c == cat for c in categories]\n",
        "    plt.scatter(coords[mask, 0], coords[mask, 1], c=[cmap(i)], label=cat, alpha=0.6, s=30)\n",
        "\n",
        "plt.title('Block2Vec V2 Embeddings (t-SNE)', fontsize=14)\n",
        "plt.xlabel('t-SNE dimension 1')\n",
        "plt.ylabel('t-SNE dimension 2')\n",
        "plt.legend(loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/tsne_embeddings_v2.png\", dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 13: Final Evaluation\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nFinal Category Coherence:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "coherence = evaluate_category_coherence(embeddings, tok2block, EVAL_CATEGORIES)\n",
        "for cat, prec in coherence.items():\n",
        "    status = \"GOOD\" if prec > 0.5 else \"POOR\" if prec > 0.3 else \"BAD\"\n",
        "    print(f\"  {cat:15} {prec*100:5.1f}% [{status}]\")\n",
        "\n",
        "avg_coherence = np.mean(list(coherence.values()))\n",
        "print(f\"\\n  V2 Average: {avg_coherence*100:.1f}%\")\n",
        "print(f\"  V1 Average: 20.4%\")\n",
        "print(f\"  Improvement: {avg_coherence*100 - 20.4:+.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 14: Nearest Neighbor Examples\n",
        "# ============================================================\n",
        "\n",
        "def find_similar_blocks(block_name, embeddings, tok2block, top_k=5):\n",
        "    \"\"\"Find and print the most similar blocks.\"\"\"\n",
        "    token = find_token_for_block(block_name, tok2block)\n",
        "    if token is None:\n",
        "        print(f\"Block '{block_name}' not found\")\n",
        "        return\n",
        "    \n",
        "    query = embeddings[token].reshape(1, -1)\n",
        "    sims = cosine_similarity(query, embeddings)[0]\n",
        "    top_indices = np.argsort(sims)[::-1]\n",
        "    \n",
        "    print(f\"\\nNearest neighbors for '{tok2block[token]}':\")\n",
        "    count = 0\n",
        "    for idx in top_indices:\n",
        "        if idx != token:\n",
        "            name = tok2block.get(idx, \"unknown\")\n",
        "            short = name.replace(\"minecraft:\", \"\")[:40]\n",
        "            print(f\"  {count+1}. {short:40} sim={sims[idx]:.3f}\")\n",
        "            count += 1\n",
        "            if count >= top_k:\n",
        "                break\n",
        "\n",
        "# Test key blocks\n",
        "test_blocks = [\"oak_planks\", \"stone\", \"diamond_ore\", \"white_wool\", \"glass\"]\n",
        "for block in test_blocks:\n",
        "    find_similar_blocks(block, embeddings, tok2block)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 15: Summary\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BLOCK2VEC V2 TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nModel:\")\n",
        "print(f\"  Vocabulary: {VOCAB_SIZE} collapsed tokens (was 3,717)\")\n",
        "print(f\"  Embedding dim: {EMBEDDING_DIM}\")\n",
        "print(f\"  Loss weights: alpha={ALPHA} (skip-gram), beta={BETA} (CBOW)\")\n",
        "\n",
        "print(f\"\\nTraining:\")\n",
        "print(f\"  Epochs completed: {len(history['total_loss'])}/{EPOCHS}\")\n",
        "print(f\"  Best epoch: {history['best_epoch']}\")\n",
        "print(f\"  Best loss: {best_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nEvaluation:\")\n",
        "print(f\"  V2 category coherence: {avg_coherence*100:.1f}%\")\n",
        "print(f\"  V1 category coherence: 20.4%\")\n",
        "improvement = \"IMPROVED\" if avg_coherence > 0.204 else \"NO IMPROVEMENT\"\n",
        "print(f\"  Result: {improvement}\")\n",
        "\n",
        "print(f\"\\nOutput files:\")\n",
        "print(f\"  - block_embeddings_v2.npy (use this for VQ-VAE)\")\n",
        "print(f\"  - block2vec_v2_best.pt\")\n",
        "print(f\"  - training_history_v2.json (for debugging)\")\n",
        "print(f\"  - training_analysis_v2.png\")\n",
        "print(f\"  - tsne_embeddings_v2.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
