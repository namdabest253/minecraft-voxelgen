{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block2Vec V3: Compositional Embeddings\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook trains **version 3** of our Block2Vec model - a neural network that learns numerical representations (\"embeddings\") for Minecraft blocks. V3 uses a completely different approach from V1 and V2 to solve a fundamental problem we discovered.\n",
    "\n",
    "## Why V3? What Went Wrong With V1 and V2?\n",
    "\n",
    "### The Problem We're Solving\n",
    "\n",
    "We want blocks that serve the **same function** to have similar embeddings. For example:\n",
    "- `oak_planks` and `spruce_planks` should be similar (both are wooden planks)\n",
    "- `oak_stairs` and `stone_stairs` should be similar (both are stairs)\n",
    "- `white_wool` and `red_wool` should be similar (both are wool)\n",
    "\n",
    "This is important because when our VQ-VAE (Phase 3) learns to generate structures, it should understand that swapping `oak_planks` for `spruce_planks` is a minor change, while swapping `oak_planks` for `lava` is a major change.\n",
    "\n",
    "### What V1 Did (Skip-gram Only)\n",
    "\n",
    "V1 used the same approach as Word2Vec: **\"You shall know a block by its neighbors.\"**\n",
    "\n",
    "The idea is that blocks appearing next to similar neighbors should have similar embeddings. This worked great for some blocks:\n",
    "\n",
    "```\n",
    "diamond_ore neighbors: [stone, stone, stone, deepslate, air, stone]\n",
    "emerald_ore neighbors: [stone, stone, deepslate, stone, air, stone]\n",
    "→ 95% overlap! So diamond_ore ≈ emerald_ore ✓\n",
    "```\n",
    "\n",
    "But it **completely failed** for wood planks:\n",
    "\n",
    "```\n",
    "oak_planks neighbors: [oak_planks, oak_stairs, oak_log, dark_oak_planks, air]\n",
    "spruce_planks neighbors: [spruce_planks, spruce_stairs, spruce_log, air]\n",
    "→ Almost NO overlap! oak_planks and spruce_planks never \"meet\"\n",
    "```\n",
    "\n",
    "**The result:** Ores clustered beautifully (~80% coherence), but planks were scattered (~20% coherence).\n",
    "\n",
    "### What V2 Did (Hybrid Skip-gram + CBOW)\n",
    "\n",
    "V2 tried to fix this by adding **CBOW** (Continuous Bag of Words), which learns in the opposite direction:\n",
    "- Skip-gram: Given center block, predict neighbors\n",
    "- CBOW: Given neighbors, predict center block\n",
    "\n",
    "The theory was that CBOW would learn: \"If neighbors are [planks, stairs, log, air], the center is probably some kind of plank.\" This should connect oak_planks and spruce_planks.\n",
    "\n",
    "**But it failed even worse!**\n",
    "\n",
    "| Metric | V1 | V2 |\n",
    "|--------|----|----|  \n",
    "| Overall Coherence | 20.4% | 16.1% |\n",
    "| Planks Coherence | ~20% | **2.5%** |\n",
    "| Ores Coherence | ~80% | 44% |\n",
    "\n",
    "### Why V2 Failed: The Chicken-and-Egg Problem\n",
    "\n",
    "Here's the subtle issue:\n",
    "\n",
    "For CBOW to learn that `oak_planks ≈ spruce_planks`, it needs:\n",
    "- `oak_stairs ≈ spruce_stairs` (so their contexts look similar)\n",
    "- `oak_log ≈ spruce_log` (so their contexts look similar)\n",
    "\n",
    "But `oak_stairs` and `spruce_stairs` face the same problem! Their neighbors don't overlap either.\n",
    "\n",
    "**It's a chicken-and-egg problem.** The model can't learn planks are similar without stairs being similar, and can't learn stairs are similar without planks being similar.\n",
    "\n",
    "### The Root Cause\n",
    "\n",
    "The fundamental issue is that **Minecraft builders don't mix wood types**. A house made of oak uses oak planks, oak stairs, oak logs, oak doors. A house made of spruce uses all spruce. There's no \"bridge\" connecting oak and spruce in the training data.\n",
    "\n",
    "This is a **data structure problem**, not an algorithm problem. No amount of hyperparameter tuning will fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The V3 Solution - Compositional Embeddings\n",
    "\n",
    "## The Key Insight\n",
    "\n",
    "Instead of learning one embedding per block and hoping similar blocks become similar, we **force** the structure we want by decomposing blocks into components:\n",
    "\n",
    "```\n",
    "oak_planks = material(oak) + shape(planks) + property(solid)\n",
    "spruce_planks = material(spruce) + shape(planks) + property(solid)\n",
    "```\n",
    "\n",
    "Since `oak_planks` and `spruce_planks` share the **same shape embedding**, they are **guaranteed** to be similar!\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Each block embedding is composed of three parts:\n",
    "\n",
    "### 1. Material Embedding (16 dimensions)\n",
    "What the block is made of: oak, spruce, stone, iron, diamond, white, red, etc.\n",
    "\n",
    "### 2. Shape Embedding (16 dimensions)\n",
    "The geometric form: planks, stairs, slab, fence, door, block, ore, wool, etc.\n",
    "\n",
    "### 3. Property Embedding (8 dimensions)\n",
    "Functional characteristics: solid, transparent, light_emitting, interactable, etc.\n",
    "\n",
    "## The Mathematical Guarantee\n",
    "\n",
    "```\n",
    "similarity(oak_planks, spruce_planks)\n",
    "= similarity(oak + planks + solid, spruce + planks + solid)\n",
    "= (oak · spruce) + (planks · planks) + (solid · solid)\n",
    "= (small)       + (1.0)             + (1.0)\n",
    "= HIGH SIMILARITY! ✓\n",
    "```\n",
    "\n",
    "The shape and property components contribute strongly to similarity, even if the materials are different.\n",
    "\n",
    "## Expected Improvements\n",
    "\n",
    "| Metric | V1 | V2 | V3 (Expected) |\n",
    "|--------|----|----|---------------|\n",
    "| Overall Coherence | 20.4% | 16.1% | **>50%** |\n",
    "| Planks Coherence | ~20% | 2.5% | **>80%** |\n",
    "| Stairs Coherence | ~60% | ~44% | **>80%** |\n",
    "| Wool Coherence | ? | ? | **>80%** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Setup and Configuration\n",
    "\n",
    "Let's start by importing libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# ============================================================\n",
    "# These are the libraries we need:\n",
    "# - torch: PyTorch deep learning framework\n",
    "# - numpy: Numerical operations on arrays\n",
    "# - h5py: Reading HDF5 files (our training data)\n",
    "# - json: Reading/writing configuration files\n",
    "# - matplotlib: Creating visualizations\n",
    "# - sklearn: For t-SNE dimensionality reduction and similarity metrics\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Tuple, Iterator\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check if GPU is available - this is why we're using Kaggle!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 2: Configuration\n# ============================================================\n# These are our HYPERPARAMETERS - values we choose before training.\n#\n# KEY DIFFERENCE FROM V1/V2:\n# Instead of one 32-dim embedding per block, we have:\n# - 16 dims for MATERIAL (what it's made of)\n# - 16 dims for SHAPE (geometric form)\n# - 8 dims for PROPERTIES (functional characteristics)\n# Total: 40 dimensions\n\n# === Data Paths ===\nDATA_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/train\"\nVOCAB_PATH = \"/kaggle/input/minecraft-schematics/tok2block.json\"\nOUTPUT_DIR = \"/kaggle/working\"\n\n# === Compositional Embedding Dimensions ===\nMATERIAL_DIM = 16   # Captures: oak, spruce, stone, iron, white, red, etc.\nSHAPE_DIM = 16      # Captures: planks, stairs, slab, fence, door, etc.\nPROPERTY_DIM = 8    # Captures: solid, transparent, light_emitting, etc.\nTOTAL_EMBEDDING_DIM = MATERIAL_DIM + SHAPE_DIM + PROPERTY_DIM  # 40\n\n# === Training Hyperparameters ===\n# NOTE: Using fewer epochs than V1/V2 because:\n# 1. V3 has far fewer parameters (~425 components vs 3717 blocks)\n# 2. Compositional structure constrains the solution space\n# 3. V1 peaked at epoch 10, V2 at epoch 25\n# 4. This is a validation run - we can train longer if it works\nEPOCHS = 15              # Reduced from 50 - enough to validate approach\nBATCH_SIZE = 4096        # Examples per gradient update\nLEARNING_RATE = 0.001    # Step size for optimization\nNUM_NEGATIVE = 10        # Negative samples per positive pair\nSUBSAMPLE_THRESHOLD = 0.001  # Subsample blocks more frequent than this\n\n# === Other ===\nSEED = 42  # Random seed for reproducibility\n\n# Set random seeds\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nprint(\"V3 Compositional Embedding Configuration:\")\nprint(f\"  Material dimensions: {MATERIAL_DIM}\")\nprint(f\"  Shape dimensions: {SHAPE_DIM}\")\nprint(f\"  Property dimensions: {PROPERTY_DIM}\")\nprint(f\"  Total embedding size: {TOTAL_EMBEDDING_DIM}\")\nprint(f\"\\nTraining:\")\nprint(f\"  Epochs: {EPOCHS} (reduced for validation)\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Block Decomposition\n",
    "\n",
    "## The Core Innovation of V3\n",
    "\n",
    "We need to decompose each Minecraft block name into its components:\n",
    "\n",
    "```\n",
    "\"minecraft:oak_planks\" → material=oak, shape=planks, properties=[solid]\n",
    "\"minecraft:white_wool\" → material=white, shape=wool, properties=[solid]\n",
    "\"minecraft:glass\"      → material=None, shape=glass, properties=[transparent]\n",
    "\"minecraft:torch\"      → material=None, shape=torch, properties=[light_emitting, solid]\n",
    "```\n",
    "\n",
    "This decomposition is done automatically using pattern matching on block names.\n",
    "\n",
    "## Material Categories\n",
    "\n",
    "We define several categories of materials:\n",
    "\n",
    "- **Wood types**: oak, spruce, birch, jungle, acacia, dark_oak, etc.\n",
    "- **Stone types**: stone, cobblestone, granite, diorite, andesite, etc.\n",
    "- **Colors**: white, orange, red, blue, green, etc. (for wool, concrete, terracotta)\n",
    "- **Metals**: iron, gold, copper, netherite\n",
    "- **Minerals**: diamond, emerald, lapis, redstone, coal, quartz\n",
    "\n",
    "## Shape Categories\n",
    "\n",
    "Shapes are identified by suffixes in block names:\n",
    "\n",
    "- `_planks` → shape is \"planks\"\n",
    "- `_stairs` → shape is \"stairs\"\n",
    "- `_slab` → shape is \"slab\"\n",
    "- `_fence` → shape is \"fence\"\n",
    "- `_door` → shape is \"door\"\n",
    "- etc.\n",
    "\n",
    "Some blocks ARE their own shape (like `air`, `water`, `dirt`, `torch`).\n",
    "\n",
    "## Properties\n",
    "\n",
    "Properties describe functional characteristics:\n",
    "\n",
    "- **solid**: Most blocks (stone, planks, wool)\n",
    "- **transparent**: Glass, ice, water, air, leaves\n",
    "- **light_emitting**: Torch, glowstone, lantern, fire\n",
    "- **interactable**: Doors, chests, buttons, furnaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 3: Block Decomposition Definitions\n# ============================================================\n# This cell defines how we parse Minecraft block names into\n# their material, shape, and property components.\n\n@dataclass\nclass BlockComponents:\n    \"\"\"Holds the decomposed components of a Minecraft block.\"\"\"\n    original_name: str      # e.g., \"minecraft:oak_planks[axis=y]\"\n    base_name: str          # e.g., \"oak_planks\" (without namespace and state)\n    material: Optional[str] = None   # e.g., \"oak\"\n    shape: str = \"block\"    # e.g., \"planks\"\n    properties: list = field(default_factory=list)  # e.g., [\"solid\"]\n\n\n# === MATERIAL DEFINITIONS ===\n# These are the \"what it's made of\" prefixes\n\nWOOD_MATERIALS = {\n    \"oak\", \"spruce\", \"birch\", \"jungle\", \"acacia\", \"dark_oak\",\n    \"mangrove\", \"cherry\", \"bamboo\", \"crimson\", \"warped\"\n}\n\nSTONE_MATERIALS = {\n    \"stone\", \"cobblestone\", \"mossy_cobblestone\", \"smooth_stone\",\n    \"granite\", \"polished_granite\", \"diorite\", \"polished_diorite\",\n    \"andesite\", \"polished_andesite\", \"deepslate\", \"cobbled_deepslate\",\n    \"polished_deepslate\", \"calcite\", \"tuff\", \"dripstone\", \"blackstone\",\n    \"polished_blackstone\", \"basalt\", \"smooth_basalt\", \"polished_basalt\"\n}\n\nBRICK_MATERIALS = {\n    \"brick\", \"stone_brick\", \"mossy_stone_brick\", \"nether_brick\",\n    \"red_nether_brick\", \"end_stone_brick\", \"prismarine_brick\",\n    \"deepslate_brick\", \"polished_blackstone_brick\", \"mud_brick\"\n}\n\nSANDSTONE_MATERIALS = {\n    \"sandstone\", \"red_sandstone\", \"smooth_sandstone\", \"smooth_red_sandstone\",\n    \"cut_sandstone\", \"cut_red_sandstone\", \"chiseled_sandstone\", \"chiseled_red_sandstone\"\n}\n\nMETAL_MATERIALS = {\n    \"iron\", \"gold\", \"copper\", \"exposed_copper\", \"weathered_copper\",\n    \"oxidized_copper\", \"waxed_copper\", \"netherite\"\n}\n\nMINERAL_MATERIALS = {\"diamond\", \"emerald\", \"lapis\", \"redstone\", \"coal\", \"quartz\", \"amethyst\"}\n\n# Colors for wool, concrete, terracotta, etc.\nCOLOR_MATERIALS = {\n    \"white\", \"orange\", \"magenta\", \"light_blue\", \"yellow\", \"lime\",\n    \"pink\", \"gray\", \"light_gray\", \"cyan\", \"purple\", \"blue\",\n    \"brown\", \"green\", \"red\", \"black\"\n}\n\nNETHER_MATERIALS = {\n    \"nether\", \"soul\", \"shroomlight\", \"glowstone\", \"netherrack\", \"magma\"\n}\n\nEND_MATERIALS = {\"end_stone\", \"purpur\", \"end\"}\n\n# Combine all materials\nALL_MATERIALS = (WOOD_MATERIALS | STONE_MATERIALS | BRICK_MATERIALS | \n                 SANDSTONE_MATERIALS | METAL_MATERIALS | MINERAL_MATERIALS | \n                 COLOR_MATERIALS | NETHER_MATERIALS | END_MATERIALS)\n\nprint(f\"Defined {len(ALL_MATERIALS)} material types\")\nprint(f\"  Wood: {len(WOOD_MATERIALS)} types\")\nprint(f\"  Stone: {len(STONE_MATERIALS)} types\")\nprint(f\"  Brick: {len(BRICK_MATERIALS)} types\")\nprint(f\"  Sandstone: {len(SANDSTONE_MATERIALS)} types\")\nprint(f\"  Metal: {len(METAL_MATERIALS)} types\")\nprint(f\"  Colors: {len(COLOR_MATERIALS)} types\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 4: Shape and Property Definitions\n# ============================================================\n# Shapes are identified by suffixes in block names.\n# Properties describe functional characteristics.\n\n# === SHAPE PATTERNS ===\n# Maps block name suffixes to shape categories\n# Longer patterns must come first to avoid partial matches\nSHAPE_PATTERNS = {\n    # Exact suffix matches (order matters - longer first)\n    \"_pressure_plate\": \"pressure_plate\",\n    \"_wall_hanging_sign\": \"wall_hanging_sign\",\n    \"_wall_sign\": \"wall_sign\",\n    \"_wall_banner\": \"wall_banner\",\n    \"_wall_head\": \"wall_head\",\n    \"_wall_skull\": \"wall_skull\",\n    \"_wall_torch\": \"wall_torch\",\n    \"_wall_fan\": \"wall_fan\",\n    \"_hanging_sign\": \"hanging_sign\",\n    \"_fence_gate\": \"fence_gate\",\n    \"_trap_door\": \"trapdoor\",\n    \"_trapdoor\": \"trapdoor\",\n    \"_coral_block\": \"coral_block\",\n    \"_coral_fan\": \"coral_fan\",\n    \"_coral\": \"coral\",\n    \"_mushroom_block\": \"mushroom_block\",\n    \"_mushroom\": \"mushroom\",\n    \"_concrete_powder\": \"concrete_powder\",\n    \"_glazed_terracotta\": \"glazed_terracotta\",\n    \"_stained_glass_pane\": \"stained_glass_pane\",\n    \"_stained_glass\": \"stained_glass\",\n    \"_candle_cake\": \"candle_cake\",\n    \"_shulker_box\": \"shulker_box\",\n    \"_amethyst_bud\": \"amethyst_bud\",\n    \"_froglight\": \"froglight\",\n    \"_button\": \"button\",\n    \"_stairs\": \"stairs\",\n    \"_planks\": \"planks\",\n    \"_slab\": \"slab\",\n    \"_wall\": \"wall\",\n    \"_fence\": \"fence\",\n    \"_door\": \"door\",\n    \"_sign\": \"sign\",\n    \"_log\": \"log\",\n    \"_wood\": \"wood\",\n    \"_stem\": \"stem\",\n    \"_hyphae\": \"hyphae\",\n    \"_roots\": \"roots\",\n    \"_leaves\": \"leaves\",\n    \"_sapling\": \"sapling\",\n    \"_carpet\": \"carpet\",\n    \"_bed\": \"bed\",\n    \"_banner\": \"banner\",\n    \"_candle\": \"candle\",\n    \"_head\": \"head\",\n    \"_skull\": \"skull\",\n    \"_pot\": \"pot\",\n    \"_ore\": \"ore\",\n    \"_block\": \"block\",\n    \"_glass\": \"glass\",\n    \"_pane\": \"pane\",\n    \"_bars\": \"bars\",\n    \"_chain\": \"chain\",\n    \"_lantern\": \"lantern\",\n    \"_torch\": \"torch\",\n    \"_rail\": \"rail\",\n    \"_chest\": \"chest\",\n    \"_terracotta\": \"terracotta\",\n    \"_concrete\": \"concrete\",\n    \"_wool\": \"wool\",\n    \"_tulip\": \"flower\",\n    \"_vines\": \"vines\",\n    \"_plant\": \"plant\",\n}\n\n# Blocks that ARE their own shape (no material prefix)\nSTANDALONE_SHAPES = {\n    # Air and fluids\n    \"air\", \"cave_air\", \"void_air\", \"water\", \"lava\", \"fire\", \"soul_fire\",\n    \"bubble_column\", \"powder_snow\",\n\n    # Terrain blocks\n    \"grass_block\", \"dirt\", \"coarse_dirt\", \"rooted_dirt\", \"podzol\", \"mycelium\",\n    \"sand\", \"red_sand\", \"gravel\", \"clay\", \"mud\", \"packed_mud\", \"farmland\", \"dirt_path\",\n    \"snow\", \"snow_block\", \"snow_layer\", \"ice\", \"packed_ice\", \"blue_ice\", \"frosted_ice\",\n    \"glass\", \"tinted_glass\", \"bedrock\", \"obsidian\", \"crying_obsidian\",\n\n    # Storage and crafting\n    \"chest\", \"trapped_chest\", \"ender_chest\", \"barrel\", \"shulker_box\",\n    \"crafting_table\", \"furnace\", \"blast_furnace\", \"smoker\",\n    \"anvil\", \"chipped_anvil\", \"damaged_anvil\",\n    \"grindstone\", \"stonecutter\", \"cartography_table\",\n    \"fletching_table\", \"smithing_table\", \"loom\", \"lectern\",\n    \"composter\", \"brewing_stand\", \"cauldron\", \"water_cauldron\", \"lava_cauldron\", \"powder_snow_cauldron\",\n\n    # Redstone\n    \"hopper\", \"dropper\", \"dispenser\", \"observer\", \"piston\", \"piston_head\",\n    \"sticky_piston\", \"moving_piston\", \"slime_block\", \"honey_block\",\n    \"tnt\", \"target\", \"repeater\", \"comparator\", \"daylight_detector\",\n    \"note_block\", \"jukebox\", \"lever\", \"tripwire\", \"tripwire_hook\",\n    \"lightning_rod\", \"redstone_wire\",\n\n    # Special blocks\n    \"respawn_anchor\", \"lodestone\", \"beacon\", \"conduit\",\n    \"enchanting_table\", \"end_portal_frame\", \"end_portal\", \"end_gateway\", \"dragon_egg\",\n    \"bell\", \"campfire\", \"soul_campfire\",\n    \"spawner\", \"structure_block\", \"structure_void\", \"jigsaw\", \"barrier\", \"light\",\n    \"command_block\", \"chain_command_block\", \"repeating_command_block\",\n\n    # Torches and lighting\n    \"torch\", \"soul_torch\", \"redstone_torch\", \"wall_torch\", \"soul_wall_torch\", \"redstone_wall_torch\",\n    \"lantern\", \"soul_lantern\", \"chain\", \"end_rod\", \"sea_lantern\",\n\n    # Flowers (single blocks)\n    \"dandelion\", \"poppy\", \"blue_orchid\", \"allium\", \"azure_bluet\",\n    \"oxeye_daisy\", \"cornflower\", \"lily_of_the_valley\", \"wither_rose\",\n    \"sunflower\", \"lilac\", \"rose_bush\", \"peony\", \"torchflower\",\n    \"pitcher_plant\", \"pitcher_crop\", \"spore_blossom\",\n\n    # Tall plants and grass\n    \"grass\", \"tall_grass\", \"fern\", \"large_fern\", \"dead_bush\",\n    \"seagrass\", \"tall_seagrass\", \"kelp\", \"kelp_plant\",\n    \"sugar_cane\", \"bamboo\", \"cactus\",\n    \"vine\", \"glow_lichen\", \"sculk_vein\",\n    \"hanging_roots\", \"azalea\", \"flowering_azalea\",\n    \"big_dripleaf\", \"big_dripleaf_stem\", \"small_dripleaf\",\n    \"lily_pad\", \"moss_carpet\",\n\n    # Crops\n    \"wheat\", \"carrots\", \"potatoes\", \"beetroots\", \"melon\", \"pumpkin\",\n    \"carved_pumpkin\", \"jack_o_lantern\", \"melon_stem\", \"pumpkin_stem\",\n    \"attached_melon_stem\", \"attached_pumpkin_stem\",\n    \"sweet_berry_bush\", \"cocoa\", \"nether_wart\", \"torchflower_crop\",\n\n    # Cave/underground\n    \"pointed_dripstone\", \"sculk\", \"sculk_sensor\", \"calibrated_sculk_sensor\",\n    \"sculk_catalyst\", \"sculk_shrieker\", \"moss_block\",\n    \"amethyst_cluster\", \"budding_amethyst\",\n    \"cave_vines\", \"cave_vines_plant\", \"glow_item_frame\", \"item_frame\",\n    \"twisting_vines\", \"twisting_vines_plant\", \"weeping_vines\", \"weeping_vines_plant\",\n\n    # Corals (standalone base types)\n    \"brain_coral\", \"bubble_coral\", \"fire_coral\", \"horn_coral\", \"tube_coral\",\n    \"dead_brain_coral\", \"dead_bubble_coral\", \"dead_fire_coral\", \"dead_horn_coral\", \"dead_tube_coral\",\n\n    # Nether\n    \"nether_sprouts\", \"crimson_fungus\", \"warped_fungus\", \"crimson_nylium\", \"warped_nylium\",\n    \"shroomlight\", \"nether_wart_block\", \"warped_wart_block\",\n\n    # Misc blocks\n    \"sponge\", \"wet_sponge\", \"cobweb\", \"bookshelf\", \"chiseled_bookshelf\",\n    \"hay_block\", \"bone_block\", \"honeycomb_block\", \"dried_kelp_block\",\n    \"mushroom_stem\", \"chorus_flower\", \"chorus_plant\",\n    \"decorated_pot\", \"flower_pot\", \"scaffolding\", \"ladder\",\n    \"rail\", \"powered_rail\", \"detector_rail\", \"activator_rail\",\n    \"frogspawn\", \"turtle_egg\", \"sniffer_egg\",\n\n    # Skulls and heads\n    \"skull\", \"creeper_head\", \"dragon_head\", \"piglin_head\", \"player_head\", \"zombie_head\",\n    \"skeleton_skull\", \"wither_skeleton_skull\",\n\n    # Infested blocks\n    \"infested_stone\", \"infested_cobblestone\", \"infested_stone_bricks\",\n    \"infested_mossy_stone_bricks\", \"infested_cracked_stone_bricks\",\n    \"infested_chiseled_stone_bricks\", \"infested_deepslate\",\n\n    # Prismarine\n    \"prismarine\", \"prismarine_bricks\", \"dark_prismarine\",\n\n    # Generic/base blocks (when no color prefix)\n    \"terracotta\", \"concrete_powder\", \"banner\", \"wall_banner\",\n    \"button\", \"carpet\", \"wool\", \"candle\", \"candle_cake\",\n    \"stained_glass\", \"stained_glass_pane\", \"stained_hardened_clay\",\n\n    # Remaining unique blocks\n    \"ancient_debris\", \"azalea_leaves_flowers\",\n    \"bee_hive\", \"bee_nest\", \"beehive\",\n    \"bricks\", \"cake\",\n    \"exposed_cut_copper\", \"oxidized_cut_copper\", \"weathered_cut_copper\",\n    \"gilded_blackstone\", \"reinforced_deepslate\",\n    \"mossy_stone_bricks\", \"mud_bricks\",\n    \"sea_pickle\", \"suspicious_gravel\", \"suspicious_sand\",\n}\n\n# === PROPERTY KEYWORDS ===\nTRANSPARENT_KEYWORDS = {\"glass\", \"pane\", \"ice\", \"leaves\", \"air\", \"water\", \"lava\", \"barrier\"}\nLIGHT_KEYWORDS = {\"torch\", \"lantern\", \"glowstone\", \"shroomlight\", \"fire\", \"lava\", \"beacon\", \n                  \"sea_lantern\", \"end_rod\", \"froglight\", \"campfire\", \"magma\"}\nINTERACTABLE_KEYWORDS = {\"door\", \"trapdoor\", \"fence_gate\", \"button\", \"chest\", \"furnace\",\n                         \"lever\", \"bed\", \"barrel\", \"anvil\", \"enchanting_table\"}\n\nprint(f\"Defined {len(SHAPE_PATTERNS)} shape patterns\")\nprint(f\"Defined {len(STANDALONE_SHAPES)} standalone shapes\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 5: Block Decomposition Functions\n# ============================================================\n# These functions parse a block name into its components.\n\ndef extract_base_name(block_name: str) -> str:\n    \"\"\"\n    Remove namespace and block state from a block name.\n    \n    Examples:\n        \"minecraft:oak_planks\" → \"oak_planks\"\n        \"minecraft:oak_stairs[facing=north]\" → \"oak_stairs\"\n    \"\"\"\n    name = block_name.replace(\"minecraft:\", \"\")\n    if \"[\" in name:\n        name = name[:name.index(\"[\")]\n    return name\n\n\ndef identify_material(base_name: str) -> Optional[str]:\n    \"\"\"\n    Identify the material component of a block name.\n    \n    Examples:\n        \"oak_planks\" → \"oak\"\n        \"white_wool\" → \"white\"\n        \"stone\" → \"stone\"\n        \"glass\" → None (no material prefix)\n    \"\"\"\n    for material in ALL_MATERIALS:\n        if base_name.startswith(material + \"_\") or base_name == material:\n            return material\n    return None\n\n\ndef identify_shape(base_name: str) -> str:\n    \"\"\"\n    Identify the shape component of a block name.\n    \n    Examples:\n        \"oak_planks\" → \"planks\"\n        \"stone_stairs\" → \"stairs\"\n        \"glass\" → \"glass\" (standalone shape)\n        \"potted_cactus\" → \"potted_plant\"\n        \"stripped_oak_log\" → \"stripped_log\"\n    \"\"\"\n    # Check standalone shapes first\n    if base_name in STANDALONE_SHAPES:\n        return base_name\n    \n    # Special case: potted plants\n    if base_name.startswith(\"potted_\"):\n        return \"potted_plant\"\n    \n    # Special case: stripped wood/logs\n    if base_name.startswith(\"stripped_\"):\n        rest = base_name[9:]  # Remove \"stripped_\"\n        if rest.endswith(\"_log\"):\n            return \"stripped_log\"\n        elif rest.endswith(\"_wood\"):\n            return \"stripped_wood\"\n        elif rest.endswith(\"_stem\"):\n            return \"stripped_stem\"\n        elif rest.endswith(\"_hyphae\"):\n            return \"stripped_hyphae\"\n        elif rest.endswith(\"_block\"):\n            return \"stripped_block\"\n    \n    # Special case: chiseled variants\n    if base_name.startswith(\"chiseled_\"):\n        return \"chiseled_block\"\n    \n    # Special case: cracked variants\n    if base_name.startswith(\"cracked_\"):\n        return \"cracked_block\"\n    \n    # Special case: cut variants (cut_copper, cut_sandstone)\n    if base_name.startswith(\"cut_\"):\n        return \"cut_block\"\n    \n    # Special case: smooth variants\n    if base_name.startswith(\"smooth_\"):\n        rest = base_name[7:]\n        if any(rest.endswith(s) for s in [\"_slab\", \"_stairs\"]):\n            pass  # Let it fall through to pattern matching\n        else:\n            return \"smooth_block\"\n    \n    # Special case: waxed copper variants\n    if base_name.startswith(\"waxed_\"):\n        rest = base_name[6:]\n        if rest.endswith(\"_slab\"):\n            return \"slab\"\n        elif rest.endswith(\"_stairs\"):\n            return \"stairs\"\n        else:\n            return \"waxed_block\"\n    \n    # Special case: raw ore blocks\n    if base_name.startswith(\"raw_\") and base_name.endswith(\"_block\"):\n        return \"raw_block\"\n    \n    # Check shape patterns (longer patterns first)\n    for pattern, shape in sorted(SHAPE_PATTERNS.items(), key=lambda x: -len(x[0])):\n        if base_name.endswith(pattern):\n            return shape\n    \n    # Default to \"block\" for full blocks\n    return \"block\"\n\n\ndef identify_properties(base_name: str, shape: str) -> list:\n    \"\"\"\n    Identify functional properties of a block.\n    \n    Properties:\n        - solid: Block is solid (most blocks)\n        - transparent: Light passes through (glass, water, air)\n        - light_emitting: Block produces light (torch, glowstone)\n        - interactable: Block can be interacted with (doors, chests)\n    \"\"\"\n    properties = []\n    \n    # Check transparency\n    for kw in TRANSPARENT_KEYWORDS:\n        if kw in base_name or kw == shape:\n            properties.append(\"transparent\")\n            break\n    \n    # Check light emission\n    for kw in LIGHT_KEYWORDS:\n        if kw in base_name or kw == shape:\n            properties.append(\"light_emitting\")\n            break\n    \n    # Check interactability\n    for kw in INTERACTABLE_KEYWORDS:\n        if kw in base_name or kw == shape:\n            properties.append(\"interactable\")\n            break\n    \n    # Default to solid if not transparent and not air/water/lava\n    if \"transparent\" not in properties and base_name not in {\"air\", \"water\", \"lava\", \"fire\"}:\n        properties.append(\"solid\")\n    \n    return properties\n\n\ndef decompose_block(block_name: str) -> BlockComponents:\n    \"\"\"\n    Decompose a full block name into its semantic components.\n    \n    Example:\n        \"minecraft:oak_stairs[facing=north]\" →\n        BlockComponents(material=\"oak\", shape=\"stairs\", properties=[\"solid\"])\n    \"\"\"\n    base_name = extract_base_name(block_name)\n    material = identify_material(base_name)\n    shape = identify_shape(base_name)\n    properties = identify_properties(base_name, shape)\n    \n    return BlockComponents(\n        original_name=block_name,\n        base_name=base_name,\n        material=material,\n        shape=shape,\n        properties=properties\n    )\n\n\n# Test the decomposition\nprint(\"Testing block decomposition:\")\nprint(\"=\"*70)\ntest_blocks = [\n    \"minecraft:oak_planks\",\n    \"minecraft:spruce_planks\",\n    \"minecraft:oak_stairs[facing=north]\",\n    \"minecraft:stone\",\n    \"minecraft:glass\",\n    \"minecraft:white_wool\",\n    \"minecraft:torch\",\n    \"minecraft:iron_door[facing=east]\",\n    \"minecraft:diamond_ore\",\n    \"minecraft:potted_cactus\",\n    \"minecraft:stripped_oak_log\",\n    \"minecraft:chiseled_stone_bricks\",\n]\n\nfor block in test_blocks:\n    comp = decompose_block(block)\n    print(f\"{comp.base_name:<25} material={str(comp.material):<10} shape={comp.shape:<15} props={comp.properties}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Loading and Processing the Vocabulary\n",
    "\n",
    "Now we load the full vocabulary and decompose all 3,717 blocks into their components. We'll also create index mappings for each component type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Load Vocabulary and Decompose All Blocks\n",
    "# ============================================================\n",
    "# We load the tok2block.json mapping and decompose every block.\n",
    "\n",
    "# Load vocabulary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE} unique block states\")\n",
    "\n",
    "# Decompose all blocks and collect unique components\n",
    "block_components = {}  # token_id → BlockComponents\n",
    "materials_set = set()\n",
    "shapes_set = set()\n",
    "properties_set = set()\n",
    "\n",
    "print(\"\\nDecomposing all blocks...\")\n",
    "for token_id, block_name in tqdm(tok2block.items(), desc=\"Decomposing\"):\n",
    "    comp = decompose_block(block_name)\n",
    "    block_components[token_id] = comp\n",
    "    \n",
    "    if comp.material:\n",
    "        materials_set.add(comp.material)\n",
    "    shapes_set.add(comp.shape)\n",
    "    properties_set.update(comp.properties)\n",
    "\n",
    "# Create sorted lists for consistent indexing\n",
    "# \"_none_\" represents blocks without a material (like \"glass\" or \"air\")\n",
    "materials_list = [\"_none_\"] + sorted(materials_set)\n",
    "shapes_list = sorted(shapes_set)\n",
    "properties_list = sorted(properties_set)\n",
    "\n",
    "# Create index mappings\n",
    "material2idx = {m: i for i, m in enumerate(materials_list)}\n",
    "shape2idx = {s: i for i, s in enumerate(shapes_list)}\n",
    "property2idx = {p: i for i, p in enumerate(properties_list)}\n",
    "\n",
    "NUM_MATERIALS = len(materials_list)\n",
    "NUM_SHAPES = len(shapes_list)\n",
    "NUM_PROPERTIES = len(properties_list)\n",
    "\n",
    "print(f\"\\nComponent counts:\")\n",
    "print(f\"  Materials: {NUM_MATERIALS} (including _none_)\")\n",
    "print(f\"  Shapes: {NUM_SHAPES}\")\n",
    "print(f\"  Properties: {NUM_PROPERTIES}\")\n",
    "\n",
    "print(f\"\\nShapes found: {shapes_list}\")\n",
    "print(f\"\\nProperties found: {properties_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Create Block-to-Component Mapping Tensors\n",
    "# ============================================================\n",
    "# We create tensors that map each block token to its component indices.\n",
    "# These will be used by the model to look up component embeddings.\n",
    "\n",
    "# Create mapping tensors\n",
    "# block_to_material[token_id] = material index\n",
    "# block_to_shape[token_id] = shape index\n",
    "# block_to_properties[token_id, property_idx] = 1 if block has property, else 0\n",
    "\n",
    "block_to_material = torch.zeros(VOCAB_SIZE, dtype=torch.long)\n",
    "block_to_shape = torch.zeros(VOCAB_SIZE, dtype=torch.long)\n",
    "block_to_properties = torch.zeros(VOCAB_SIZE, NUM_PROPERTIES)\n",
    "\n",
    "for token_id, comp in block_components.items():\n",
    "    # Material index (0 = _none_ for blocks without material)\n",
    "    if comp.material:\n",
    "        block_to_material[token_id] = material2idx[comp.material]\n",
    "    else:\n",
    "        block_to_material[token_id] = 0  # _none_\n",
    "    \n",
    "    # Shape index\n",
    "    block_to_shape[token_id] = shape2idx[comp.shape]\n",
    "    \n",
    "    # Properties (multi-hot encoding)\n",
    "    for prop in comp.properties:\n",
    "        block_to_properties[token_id, property2idx[prop]] = 1.0\n",
    "\n",
    "print(\"Created block-to-component mapping tensors\")\n",
    "print(f\"  block_to_material: {block_to_material.shape}\")\n",
    "print(f\"  block_to_shape: {block_to_shape.shape}\")\n",
    "print(f\"  block_to_properties: {block_to_properties.shape}\")\n",
    "\n",
    "# Show distribution of shapes\n",
    "print(\"\\nBlocks per shape (top 15):\")\n",
    "shape_counts = Counter(comp.shape for comp in block_components.values())\n",
    "for shape, count in shape_counts.most_common(15):\n",
    "    print(f\"  {shape}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: The Compositional Block2Vec Model\n",
    "\n",
    "## How the Model Works\n",
    "\n",
    "The key difference from V1/V2 is how we compute block embeddings:\n",
    "\n",
    "### V1/V2 (Direct Lookup)\n",
    "```\n",
    "embedding = embedding_matrix[block_token]\n",
    "```\n",
    "Each block has its own independent embedding. No guaranteed relationship between similar blocks.\n",
    "\n",
    "### V3 (Compositional)\n",
    "```\n",
    "material_emb = material_matrix[block_to_material[block_token]]\n",
    "shape_emb = shape_matrix[block_to_shape[block_token]]\n",
    "property_emb = property_matrix @ block_to_properties[block_token]\n",
    "\n",
    "embedding = concat(material_emb, shape_emb, property_emb)\n",
    "```\n",
    "\n",
    "Blocks sharing components (same material OR same shape) automatically share embedding dimensions.\n",
    "\n",
    "## Training Objective\n",
    "\n",
    "We still use **Skip-gram with negative sampling** - the same as V1. The difference is only in how embeddings are computed, not in how they're trained.\n",
    "\n",
    "- **Positive pairs**: (center block, neighbor block) from actual builds\n",
    "- **Negative pairs**: (center block, random block)\n",
    "- **Goal**: Make positive pairs have high similarity, negative pairs have low similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Compositional Block2Vec Model\n",
    "# ============================================================\n",
    "\n",
    "class CompositionalBlock2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Block2Vec V3 with compositional embeddings.\n",
    "    \n",
    "    Instead of one embedding per block, we compose embeddings from:\n",
    "    - Material embedding (16 dims)\n",
    "    - Shape embedding (16 dims)\n",
    "    - Property embedding (8 dims)\n",
    "    \n",
    "    This guarantees that blocks with the same shape (e.g., oak_planks\n",
    "    and spruce_planks) will have similar embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks: int,\n",
    "        num_materials: int,\n",
    "        num_shapes: int,\n",
    "        num_properties: int,\n",
    "        material_dim: int,\n",
    "        shape_dim: int,\n",
    "        property_dim: int,\n",
    "        block_to_material: torch.Tensor,\n",
    "        block_to_shape: torch.Tensor,\n",
    "        block_to_properties: torch.Tensor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_blocks = num_blocks\n",
    "        self.embedding_dim = material_dim + shape_dim + property_dim\n",
    "        \n",
    "        # Component embedding matrices\n",
    "        # Each component type has its own learned embedding table\n",
    "        self.material_emb = nn.Embedding(num_materials, material_dim)\n",
    "        self.shape_emb = nn.Embedding(num_shapes, shape_dim)\n",
    "        self.property_emb = nn.Embedding(num_properties, property_dim)\n",
    "        \n",
    "        # Context embedding for Skip-gram (like V1)\n",
    "        # This is a separate embedding used for context/neighbor blocks\n",
    "        self.context_emb = nn.Embedding(num_blocks, self.embedding_dim)\n",
    "        \n",
    "        # Register mappings as buffers (saved with model but not trained)\n",
    "        self.register_buffer('block_to_material', block_to_material)\n",
    "        self.register_buffer('block_to_shape', block_to_shape)\n",
    "        self.register_buffer('block_to_properties', block_to_properties.float())\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize embeddings with small uniform random values.\"\"\"\n",
    "        nn.init.uniform_(self.material_emb.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.shape_emb.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.property_emb.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.context_emb.weight, -0.1, 0.1)\n",
    "    \n",
    "    def get_block_embedding(self, block_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute compositional embeddings for blocks.\n",
    "        \n",
    "        This is the KEY FUNCTION that makes V3 different from V1/V2.\n",
    "        \n",
    "        Args:\n",
    "            block_ids: Tensor of block tokens [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of embeddings [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        flat_ids = block_ids.view(-1)\n",
    "        \n",
    "        # Step 1: Look up component indices for each block\n",
    "        material_ids = self.block_to_material[flat_ids]  # [N]\n",
    "        shape_ids = self.block_to_shape[flat_ids]        # [N]\n",
    "        property_mask = self.block_to_properties[flat_ids]  # [N, num_properties]\n",
    "        \n",
    "        # Step 2: Get component embeddings\n",
    "        mat_emb = self.material_emb(material_ids)  # [N, material_dim]\n",
    "        shp_emb = self.shape_emb(shape_ids)        # [N, shape_dim]\n",
    "        \n",
    "        # Step 3: Property embedding (average of all properties the block has)\n",
    "        # property_emb.weight: [num_properties, property_dim]\n",
    "        # property_mask: [N, num_properties]\n",
    "        # Result: weighted sum of property embeddings\n",
    "        prop_emb = torch.matmul(property_mask, self.property_emb.weight)  # [N, property_dim]\n",
    "        num_props = property_mask.sum(dim=1, keepdim=True).clamp(min=1)   # Avoid div by 0\n",
    "        prop_emb = prop_emb / num_props\n",
    "        \n",
    "        # Step 4: Concatenate all components\n",
    "        combined = torch.cat([mat_emb, shp_emb, prop_emb], dim=-1)  # [N, total_dim]\n",
    "        \n",
    "        return combined.view(*block_ids.shape, self.embedding_dim)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        center_ids: torch.Tensor,\n",
    "        context_ids: torch.Tensor,\n",
    "        negative_ids: torch.Tensor,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Skip-gram forward pass with negative sampling.\n",
    "        \n",
    "        Same as V1, but center embeddings are compositional.\n",
    "        \n",
    "        Args:\n",
    "            center_ids: Center block tokens [batch_size]\n",
    "            context_ids: Positive context tokens [batch_size]\n",
    "            negative_ids: Negative sample tokens [batch_size, num_neg]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with loss and sub-losses\n",
    "        \"\"\"\n",
    "        # Get center embeddings (COMPOSITIONAL - this is the V3 innovation)\n",
    "        center_emb = self.get_block_embedding(center_ids)  # [B, D]\n",
    "        \n",
    "        # Get context embeddings (direct lookup, like V1)\n",
    "        pos_ctx = self.context_emb(context_ids)       # [B, D]\n",
    "        neg_ctx = self.context_emb(negative_ids)      # [B, num_neg, D]\n",
    "        \n",
    "        # Positive scores (dot product)\n",
    "        pos_scores = (center_emb * pos_ctx).sum(dim=1)  # [B]\n",
    "        \n",
    "        # Negative scores\n",
    "        neg_scores = torch.bmm(neg_ctx, center_emb.unsqueeze(2)).squeeze(2)  # [B, num_neg]\n",
    "        \n",
    "        # Loss: maximize positive, minimize negative\n",
    "        pos_loss = F.logsigmoid(pos_scores).mean()\n",
    "        neg_loss = F.logsigmoid(-neg_scores).mean()\n",
    "        loss = -(pos_loss + neg_loss)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'pos_loss': -pos_loss.item(),\n",
    "            'neg_loss': -neg_loss.item(),\n",
    "        }\n",
    "    \n",
    "    def get_all_embeddings(self) -> np.ndarray:\n",
    "        \"\"\"Get embeddings for all blocks as numpy array.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            all_ids = torch.arange(self.num_blocks, device=self.material_emb.weight.device)\n",
    "            return self.get_block_embedding(all_ids).cpu().numpy()\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = CompositionalBlock2Vec(\n",
    "    num_blocks=VOCAB_SIZE,\n",
    "    num_materials=NUM_MATERIALS,\n",
    "    num_shapes=NUM_SHAPES,\n",
    "    num_properties=NUM_PROPERTIES,\n",
    "    material_dim=MATERIAL_DIM,\n",
    "    shape_dim=SHAPE_DIM,\n",
    "    property_dim=PROPERTY_DIM,\n",
    "    block_to_material=block_to_material,\n",
    "    block_to_shape=block_to_shape,\n",
    "    block_to_properties=block_to_properties,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model created!\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Embedding dimension: {model.embedding_dim}\")\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "print(f\"  Material embeddings: {NUM_MATERIALS} × {MATERIAL_DIM} = {NUM_MATERIALS * MATERIAL_DIM:,}\")\n",
    "print(f\"  Shape embeddings: {NUM_SHAPES} × {SHAPE_DIM} = {NUM_SHAPES * SHAPE_DIM:,}\")\n",
    "print(f\"  Property embeddings: {NUM_PROPERTIES} × {PROPERTY_DIM} = {NUM_PROPERTIES * PROPERTY_DIM:,}\")\n",
    "print(f\"  Context embeddings: {VOCAB_SIZE} × {model.embedding_dim} = {VOCAB_SIZE * model.embedding_dim:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: The Dataset\n",
    "\n",
    "We use the same training data approach as V1: iterate through Minecraft builds, extract (center, neighbor) pairs, and sample negative examples.\n",
    "\n",
    "The dataset is identical to V1 - only the model has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Dataset Class\n",
    "# ============================================================\n",
    "# Same dataset as V1 - we only changed the model, not the data.\n",
    "\n",
    "class Block2VecDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Dataset that yields (center, context, negatives) tuples.\n",
    "    \n",
    "    Streams through H5 files, extracting training pairs on-the-fly.\n",
    "    Uses subsampling to reduce dominance of frequent blocks (like air).\n",
    "    \"\"\"\n",
    "    \n",
    "    NEIGHBORS_6 = [\n",
    "        (-1, 0, 0), (1, 0, 0),  # left, right (x-axis)\n",
    "        (0, -1, 0), (0, 1, 0),  # down, up (y-axis)\n",
    "        (0, 0, -1), (0, 0, 1),  # back, front (z-axis)\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        vocab_size: int,\n",
    "        num_negative: int = 10,\n",
    "        subsample_threshold: float = 0.001,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_negative = num_negative\n",
    "        self.subsample_threshold = subsample_threshold\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        print(f\"Found {len(self.h5_files)} training files\")\n",
    "        \n",
    "        self._negative_table = None\n",
    "        self._subsample_probs = None\n",
    "        self._build_tables()\n",
    "    \n",
    "    def _build_tables(self):\n",
    "        \"\"\"Build frequency table for negative sampling and subsampling.\"\"\"\n",
    "        print(\"Building frequency tables...\")\n",
    "        freqs = np.zeros(self.vocab_size, dtype=np.float64)\n",
    "        \n",
    "        for h5_path in tqdm(self.h5_files[:100], desc=\"Counting blocks\"):\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                build = f[list(f.keys())[0]][:]\n",
    "                unique, counts = np.unique(build, return_counts=True)\n",
    "                for tok, count in zip(unique, counts):\n",
    "                    if tok < self.vocab_size:\n",
    "                        freqs[tok] += count\n",
    "        \n",
    "        freqs /= freqs.sum()\n",
    "        \n",
    "        # Negative sampling table (frequency^0.75)\n",
    "        weighted = np.power(freqs + 1e-10, 0.75)\n",
    "        weighted /= weighted.sum()\n",
    "        self._negative_table = np.random.choice(\n",
    "            self.vocab_size, size=10_000_000, p=weighted\n",
    "        )\n",
    "        \n",
    "        # Subsampling probabilities\n",
    "        self._subsample_probs = np.ones(self.vocab_size, dtype=np.float32)\n",
    "        for i, freq in enumerate(freqs):\n",
    "            if freq > self.subsample_threshold:\n",
    "                self._subsample_probs[i] = np.sqrt(self.subsample_threshold / freq)\n",
    "        \n",
    "        print(f\"Tables built!\")\n",
    "    \n",
    "    def __iter__(self) -> Iterator:\n",
    "        rng = random.Random(self.seed)\n",
    "        neg_idx = 0\n",
    "        files = list(self.h5_files)\n",
    "        rng.shuffle(files)\n",
    "        \n",
    "        for h5_path in files:\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                build = f[list(f.keys())[0]][:]\n",
    "            \n",
    "            h, w, d = build.shape\n",
    "            \n",
    "            for y in range(1, h-1):\n",
    "                for x in range(1, w-1):\n",
    "                    for z in range(1, d-1):\n",
    "                        center = int(build[y, x, z])\n",
    "                        \n",
    "                        # Subsampling check\n",
    "                        if rng.random() >= self._subsample_probs[center]:\n",
    "                            continue\n",
    "                        \n",
    "                        # Check each neighbor\n",
    "                        for dy, dx, dz in self.NEIGHBORS_6:\n",
    "                            context = int(build[y+dy, x+dx, z+dz])\n",
    "                            \n",
    "                            # Get negatives\n",
    "                            negatives = self._negative_table[neg_idx:neg_idx + self.num_negative]\n",
    "                            neg_idx = (neg_idx + self.num_negative) % len(self._negative_table)\n",
    "                            \n",
    "                            yield center, context, negatives\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Convert batch to tensors.\"\"\"\n",
    "    centers = torch.tensor([b[0] for b in batch], dtype=torch.long)\n",
    "    contexts = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    negatives = torch.tensor(np.array([b[2] for b in batch]), dtype=torch.long)\n",
    "    return centers, contexts, negatives\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = Block2VecDataset(\n",
    "    data_dir=DATA_DIR,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_negative=NUM_NEGATIVE,\n",
    "    subsample_threshold=SUBSAMPLE_THRESHOLD,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader ready with batch size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Training\n",
    "\n",
    "Now we train the model. This is the same training loop as V1 - only the model is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Create Optimizer\n",
    "# ============================================================\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "print(f\"Scheduler: CosineAnnealingLR over {EPOCHS} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Training Loop\n",
    "# ============================================================\n",
    "\n",
    "# Estimate batches per epoch (rough, since dataset is iterable)\n",
    "BATCHES_PER_EPOCH = len(dataset.h5_files) * 500  # ~500 useful samples per file after subsampling\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING BLOCK2VEC V3 TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Estimated batches per epoch: ~{BATCHES_PER_EPOCH}\")\n",
    "print()\n",
    "\n",
    "history = {'loss': [], 'pos_loss': [], 'neg_loss': []}\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_pos = 0\n",
    "    epoch_neg = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    \n",
    "    for centers, contexts, negatives in pbar:\n",
    "        # Move to GPU\n",
    "        centers = centers.to(device)\n",
    "        contexts = contexts.to(device)\n",
    "        negatives = negatives.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        out = model(centers, contexts, negatives)\n",
    "        \n",
    "        # Backward pass\n",
    "        out['loss'].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += out['loss'].item()\n",
    "        epoch_pos += out['pos_loss']\n",
    "        epoch_neg += out['neg_loss']\n",
    "        n_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({'loss': f\"{out['loss'].item():.4f}\"})\n",
    "        \n",
    "        # Limit batches per epoch for reasonable training time\n",
    "        if n_batches >= BATCHES_PER_EPOCH:\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record epoch metrics\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    avg_pos = epoch_pos / n_batches\n",
    "    avg_neg = epoch_neg / n_batches\n",
    "    \n",
    "    history['loss'].append(avg_loss)\n",
    "    history['pos_loss'].append(avg_pos)\n",
    "    history['neg_loss'].append(avg_neg)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{EPOCHS} | Loss: {avg_loss:.4f} | \"\n",
    "          f\"Pos: {avg_pos:.4f} | Neg: {avg_neg:.4f} | \"\n",
    "          f\"LR: {lr:.6f} | Time: {elapsed/60:.1f}m\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining complete in {total_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Save Results\n",
    "\n",
    "Let's save the trained embeddings and model for use in Phase 3 (VQ-VAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Extract and Save Embeddings\n",
    "# ============================================================\n",
    "\n",
    "# Get final embeddings\n",
    "embeddings = model.get_all_embeddings()\n",
    "print(f\"Final embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save(f\"{OUTPUT_DIR}/block_embeddings_v3.npy\", embeddings)\n",
    "print(f\"Saved embeddings to {OUTPUT_DIR}/block_embeddings_v3.npy\")\n",
    "\n",
    "# Save component embeddings separately (for analysis)\n",
    "component_embs = {\n",
    "    'material': model.material_emb.weight.detach().cpu().numpy(),\n",
    "    'shape': model.shape_emb.weight.detach().cpu().numpy(),\n",
    "    'property': model.property_emb.weight.detach().cpu().numpy(),\n",
    "}\n",
    "np.savez(f\"{OUTPUT_DIR}/component_embeddings_v3.npz\", **component_embs)\n",
    "print(f\"Saved component embeddings\")\n",
    "\n",
    "# Save training history\n",
    "with open(f\"{OUTPUT_DIR}/training_history_v3.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"Saved training history\")\n",
    "\n",
    "# Save vocabulary info\n",
    "vocab_info = {\n",
    "    'materials': materials_list,\n",
    "    'shapes': shapes_list,\n",
    "    'properties': properties_list,\n",
    "    'material2idx': material2idx,\n",
    "    'shape2idx': shape2idx,\n",
    "    'property2idx': property2idx,\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/vocab_info_v3.json\", 'w') as f:\n",
    "    json.dump(vocab_info, f, indent=2)\n",
    "print(f\"Saved vocabulary info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: Evaluation - Did V3 Fix the Problem?\n",
    "\n",
    "The key question: **Do blocks with the same shape now have high similarity?**\n",
    "\n",
    "We compute \"coherence\" for each shape category - the average cosine similarity between all blocks of that shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Compute Category Coherence\n",
    "# ============================================================\n",
    "\n",
    "def compute_category_coherence(embeddings, block_components):\n",
    "    \"\"\"\n",
    "    Compute coherence (average similarity) for each shape category.\n",
    "    \n",
    "    High coherence = blocks of this shape are similar to each other.\n",
    "    \"\"\"\n",
    "    # Group blocks by shape\n",
    "    shape_groups = defaultdict(list)\n",
    "    for token_id, comp in block_components.items():\n",
    "        shape_groups[comp.shape].append(token_id)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for shape, token_ids in shape_groups.items():\n",
    "        if len(token_ids) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Get embeddings for this shape\n",
    "        shape_embs = embeddings[token_ids]\n",
    "        \n",
    "        # Compute pairwise cosine similarities\n",
    "        sims = cosine_similarity(shape_embs)\n",
    "        \n",
    "        # Average similarity (excluding self-similarity on diagonal)\n",
    "        n = len(token_ids)\n",
    "        avg_sim = (sims.sum() - n) / (n * (n - 1)) if n > 1 else 0\n",
    "        \n",
    "        results[shape] = {\n",
    "            'count': n,\n",
    "            'avg_similarity': avg_sim,\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "coherence = compute_category_coherence(embeddings, block_components)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CATEGORY COHERENCE BY SHAPE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis is the KEY METRIC - higher is better!\")\n",
    "print(\"V1 had ~20% overall, V2 had ~16% overall.\")\n",
    "print(\"V3 should be >50% for most shapes.\\n\")\n",
    "\n",
    "# Sort by count\n",
    "sorted_shapes = sorted(coherence.items(), key=lambda x: -x[1]['count'])\n",
    "\n",
    "print(f\"{'Shape':<25} {'Count':>8} {'Coherence':>12}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "total_coherence = 0\n",
    "total_count = 0\n",
    "\n",
    "for shape, data in sorted_shapes[:20]:\n",
    "    coh = data['avg_similarity']\n",
    "    count = data['count']\n",
    "    indicator = \"***\" if coh > 0.8 else \"**\" if coh > 0.5 else \"*\" if coh > 0.3 else \"\"\n",
    "    print(f\"{shape:<25} {count:>8} {coh:>11.1%} {indicator}\")\n",
    "    total_coherence += coh * count\n",
    "    total_count += count\n",
    "\n",
    "overall = total_coherence / total_count if total_count > 0 else 0\n",
    "print(f\"\\n{'OVERALL (weighted)':<25} {total_count:>8} {overall:>11.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: Compare Specific Block Pairs\n",
    "# ============================================================\n",
    "# Let's check the exact similarity between blocks we care about.\n",
    "\n",
    "def get_similarity(block1_name, block2_name, embeddings, tok2block):\n",
    "    \"\"\"Get cosine similarity between two blocks.\"\"\"\n",
    "    block2tok = {v.replace('minecraft:', '').split('[')[0]: k for k, v in tok2block.items()}\n",
    "    \n",
    "    tok1 = block2tok.get(block1_name)\n",
    "    tok2 = block2tok.get(block2_name)\n",
    "    \n",
    "    if tok1 is None or tok2 is None:\n",
    "        return None\n",
    "    \n",
    "    emb1 = embeddings[tok1]\n",
    "    emb2 = embeddings[tok2]\n",
    "    return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPECIFIC BLOCK PAIR SIMILARITIES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThese pairs SHOULD be highly similar in V3:\")\n",
    "\n",
    "# Block pairs that should be similar\n",
    "should_be_similar = [\n",
    "    (\"oak_planks\", \"spruce_planks\"),\n",
    "    (\"oak_planks\", \"dark_oak_planks\"),\n",
    "    (\"oak_stairs\", \"spruce_stairs\"),\n",
    "    (\"oak_stairs\", \"stone_stairs\"),\n",
    "    (\"white_wool\", \"red_wool\"),\n",
    "    (\"white_concrete\", \"black_concrete\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Block 1':<20} {'Block 2':<20} {'Similarity':>12}  {'Status'}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "for b1, b2 in should_be_similar:\n",
    "    sim = get_similarity(b1, b2, embeddings, tok2block)\n",
    "    if sim is not None:\n",
    "        status = \"GREAT\" if sim > 0.8 else \"Good\" if sim > 0.5 else \"Poor\"\n",
    "        print(f\"{b1:<20} {b2:<20} {sim:>11.3f}   {status}\")\n",
    "\n",
    "print(\"\\nThese pairs should have LOWER similarity (different shapes):\")\n",
    "\n",
    "should_be_different = [\n",
    "    (\"oak_planks\", \"oak_stairs\"),\n",
    "    (\"oak_planks\", \"stone\"),\n",
    "    (\"glass\", \"dirt\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Block 1':<20} {'Block 2':<20} {'Similarity':>12}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "for b1, b2 in should_be_different:\n",
    "    sim = get_similarity(b1, b2, embeddings, tok2block)\n",
    "    if sim is not None:\n",
    "        print(f\"{b1:<20} {b2:<20} {sim:>11.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 10: Visualizations\n",
    "\n",
    "Let's create visualizations to understand what the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 15: Create Visualizations\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Training Loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['loss'], label='Total Loss', linewidth=2)\n",
    "ax.plot(history['pos_loss'], label='Positive Loss', alpha=0.7)\n",
    "ax.plot(history['neg_loss'], label='Negative Loss', alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. t-SNE colored by shape\n",
    "ax = axes[0, 1]\n",
    "print(\"Running t-SNE...\")\n",
    "\n",
    "# Sample blocks\n",
    "np.random.seed(42)\n",
    "sample_ids = np.random.choice(VOCAB_SIZE, min(500, VOCAB_SIZE), replace=False)\n",
    "sample_embs = embeddings[sample_ids]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "coords = tsne.fit_transform(sample_embs)\n",
    "\n",
    "# Color by shape category\n",
    "shape_colors = {}\n",
    "color_idx = 0\n",
    "cmap = plt.cm.get_cmap('tab20')\n",
    "\n",
    "for sid in sample_ids:\n",
    "    shape = block_components[sid].shape\n",
    "    if shape not in shape_colors:\n",
    "        shape_colors[shape] = cmap(color_idx % 20)\n",
    "        color_idx += 1\n",
    "\n",
    "colors = [shape_colors[block_components[sid].shape] for sid in sample_ids]\n",
    "ax.scatter(coords[:, 0], coords[:, 1], c=colors, s=20, alpha=0.6)\n",
    "ax.set_title('t-SNE (colored by shape)')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# 3. Shape Coherence Bar Chart\n",
    "ax = axes[1, 0]\n",
    "top_shapes = sorted_shapes[:15]\n",
    "shape_names = [s[0] for s in top_shapes]\n",
    "coherences = [s[1]['avg_similarity'] for s in top_shapes]\n",
    "colors_bar = ['green' if c > 0.5 else 'orange' if c > 0.3 else 'red' for c in coherences]\n",
    "ax.barh(shape_names, coherences, color=colors_bar)\n",
    "ax.set_xlabel('Average Cosine Similarity')\n",
    "ax.set_title('Shape Coherence (Top 15 by count)')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.axvline(x=0.5, color='green', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "\n",
    "# 4. Material Embedding Heatmap\n",
    "ax = axes[1, 1]\n",
    "mat_embs = model.material_emb.weight.detach().cpu().numpy()\n",
    "mat_sims = cosine_similarity(mat_embs[:20], mat_embs[:20])\n",
    "im = ax.imshow(mat_sims, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(20))\n",
    "ax.set_yticks(range(20))\n",
    "ax.set_xticklabels(materials_list[:20], rotation=45, ha='right', fontsize=8)\n",
    "ax.set_yticklabels(materials_list[:20], fontsize=8)\n",
    "ax.set_title('Material Embedding Similarities')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/block2vec_v3_results.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved visualization to {OUTPUT_DIR}/block2vec_v3_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 11: Final Summary\n",
    "\n",
    "Let's summarize what V3 achieved compared to V1 and V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 16: Final Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BLOCK2VEC V3 TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n## Model Architecture\")\n",
    "print(f\"  - Material embedding: {NUM_MATERIALS} materials × {MATERIAL_DIM} dims\")\n",
    "print(f\"  - Shape embedding: {NUM_SHAPES} shapes × {SHAPE_DIM} dims\")\n",
    "print(f\"  - Property embedding: {NUM_PROPERTIES} properties × {PROPERTY_DIM} dims\")\n",
    "print(f\"  - Total embedding dimension: {model.embedding_dim}\")\n",
    "\n",
    "print(f\"\\n## Training\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Final loss: {history['loss'][-1]:.4f}\")\n",
    "print(f\"  - Training time: {total_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\n## Key Coherence Scores (V3 vs V1 vs V2)\")\n",
    "print(f\"  {'Shape':<20} {'V3':>10} {'V1':>10} {'V2':>10}\")\n",
    "print(f\"  {'-'*50}\")\n",
    "key_shapes = ['planks', 'stairs', 'slab', 'wool', 'concrete', 'door']\n",
    "v1_approx = {'planks': 0.20, 'stairs': 0.60, 'slab': 0.40, 'wool': 0.30, 'concrete': 0.25, 'door': 0.50}\n",
    "v2_approx = {'planks': 0.025, 'stairs': 0.44, 'slab': 0.30, 'wool': 0.20, 'concrete': 0.15, 'door': 0.40}\n",
    "\n",
    "for shape in key_shapes:\n",
    "    if shape in coherence:\n",
    "        v3_val = coherence[shape]['avg_similarity']\n",
    "        v1_val = v1_approx.get(shape, 0)\n",
    "        v2_val = v2_approx.get(shape, 0)\n",
    "        improvement = \"↑↑↑\" if v3_val > v1_val * 2 else \"↑↑\" if v3_val > v1_val * 1.5 else \"↑\" if v3_val > v1_val else \"\"\n",
    "        print(f\"  {shape:<20} {v3_val:>9.1%} {v1_val:>9.1%} {v2_val:>9.1%}  {improvement}\")\n",
    "\n",
    "print(f\"\\n## Output Files\")\n",
    "print(f\"  - {OUTPUT_DIR}/block_embeddings_v3.npy\")\n",
    "print(f\"  - {OUTPUT_DIR}/component_embeddings_v3.npz\")\n",
    "print(f\"  - {OUTPUT_DIR}/training_history_v3.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/vocab_info_v3.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/block2vec_v3_results.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"V3 uses COMPOSITIONAL embeddings to GUARANTEE shape similarity.\")\n",
    "print(\"This solves the fundamental problem that V1 and V2 couldn't fix.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Part 12: Diagnostics and Failure Detection\n\nThese diagnostics help us understand if V3 is working correctly and identify potential issues. If V3 fails, these metrics will help us understand why.\n\n## Key Diagnostics\n\n1. **Similarity Distribution**: Within-shape similarity should be HIGH, across-shape should be LOWER\n2. **Embedding Collapse Detection**: Check if embeddings are degenerating\n3. **Component Usage Statistics**: Identify underrepresented components\n4. **Nearest Neighbor Quality**: Is each block's nearest neighbor the same shape?\n5. **Embedding Norm Distribution**: Are norms uniform across blocks?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 17: Similarity Distribution Analysis\n# ============================================================\n# This is a KEY DIAGNOSTIC: Within-shape similarity should be HIGH,\n# across-shape similarity should be LOWER. The gap between them\n# tells us if V3's compositional structure is working.\n\nfrom scipy.spatial.distance import pdist\n\nprint(\"=\"*60)\nprint(\"SIMILARITY DISTRIBUTION ANALYSIS\")\nprint(\"=\"*60)\n\n# Compute within-shape similarities\nwithin_shape_sims = []\nacross_shape_sims = []\n\n# Group blocks by shape\nshape_groups = defaultdict(list)\nfor token_id, comp in block_components.items():\n    shape_groups[comp.shape].append(token_id)\n\n# Sample within-shape pairs\nprint(\"\\nComputing within-shape similarities...\")\nfor shape, token_ids in shape_groups.items():\n    if len(token_ids) >= 2:\n        shape_embs = embeddings[token_ids]\n        sims = cosine_similarity(shape_embs)\n        # Get upper triangle (excluding diagonal)\n        n = len(token_ids)\n        for i in range(n):\n            for j in range(i+1, n):\n                within_shape_sims.append(sims[i, j])\n\n# Sample across-shape pairs (sample to avoid O(n^2) computation)\nprint(\"Computing across-shape similarities (sampling)...\")\nnp.random.seed(42)\nshapes_with_blocks = [s for s, ids in shape_groups.items() if len(ids) >= 2]\nn_samples = min(10000, len(within_shape_sims))\n\nfor _ in range(n_samples):\n    # Pick two different shapes\n    s1, s2 = np.random.choice(shapes_with_blocks, 2, replace=False)\n    # Pick one block from each\n    t1 = np.random.choice(shape_groups[s1])\n    t2 = np.random.choice(shape_groups[s2])\n    # Compute similarity\n    sim = np.dot(embeddings[t1], embeddings[t2]) / (\n        np.linalg.norm(embeddings[t1]) * np.linalg.norm(embeddings[t2])\n    )\n    across_shape_sims.append(sim)\n\nwithin_shape_sims = np.array(within_shape_sims)\nacross_shape_sims = np.array(across_shape_sims)\n\n# Statistics\nprint(f\"\\nWithin-Shape Similarity:\")\nprint(f\"  Mean: {within_shape_sims.mean():.3f}\")\nprint(f\"  Std:  {within_shape_sims.std():.3f}\")\nprint(f\"  Min:  {within_shape_sims.min():.3f}\")\nprint(f\"  Max:  {within_shape_sims.max():.3f}\")\n\nprint(f\"\\nAcross-Shape Similarity:\")\nprint(f\"  Mean: {across_shape_sims.mean():.3f}\")\nprint(f\"  Std:  {across_shape_sims.std():.3f}\")\nprint(f\"  Min:  {across_shape_sims.min():.3f}\")\nprint(f\"  Max:  {across_shape_sims.max():.3f}\")\n\nseparation = within_shape_sims.mean() - across_shape_sims.mean()\nprint(f\"\\n*** SEPARATION (within - across): {separation:.3f} ***\")\nprint(\"  > 0.3 = GOOD (clear distinction)\")\nprint(\"  > 0.5 = GREAT (strong separation)\")\nprint(\"  < 0.1 = POOR (shapes not distinct)\")\n\n# Save for visualization\ndiagnostics = {\n    'within_shape_mean': float(within_shape_sims.mean()),\n    'within_shape_std': float(within_shape_sims.std()),\n    'across_shape_mean': float(across_shape_sims.mean()),\n    'across_shape_std': float(across_shape_sims.std()),\n    'separation': float(separation),\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 18: Embedding Collapse Detection\n# ============================================================\n# Check if embeddings are degenerating (becoming too similar or uniform)\n\nprint(\"=\"*60)\nprint(\"EMBEDDING COLLAPSE DETECTION\")\nprint(\"=\"*60)\n\n# Check embedding standard deviation per dimension\nemb_std_per_dim = embeddings.std(axis=0)\nprint(f\"\\nPer-dimension std dev:\")\nprint(f\"  Mean: {emb_std_per_dim.mean():.4f}\")\nprint(f\"  Min:  {emb_std_per_dim.min():.4f}\")\nprint(f\"  Max:  {emb_std_per_dim.max():.4f}\")\n\n# Count collapsed dimensions (std < 0.01)\ncollapsed_dims = (emb_std_per_dim < 0.01).sum()\nprint(f\"\\n  Collapsed dimensions (<0.01 std): {collapsed_dims}/{len(emb_std_per_dim)}\")\nif collapsed_dims > len(emb_std_per_dim) // 4:\n    print(\"  ⚠️ WARNING: Many dimensions have collapsed!\")\n\n# Check pairwise distances\nprint(\"\\nPairwise distance statistics (sampling 1000 pairs)...\")\nnp.random.seed(42)\nsample_indices = np.random.choice(len(embeddings), min(1000, len(embeddings)), replace=False)\nsample_embs = embeddings[sample_indices]\npairwise_dists = pdist(sample_embs, metric='cosine')\n\nprint(f\"  Min distance: {pairwise_dists.min():.4f}\")\nprint(f\"  Mean distance: {pairwise_dists.mean():.4f}\")\nprint(f\"  Max distance: {pairwise_dists.max():.4f}\")\n\nif pairwise_dists.min() < 0.001:\n    print(\"  ⚠️ WARNING: Some embeddings are nearly identical!\")\n\n# Check embedding norms\nnorms = np.linalg.norm(embeddings, axis=1)\nprint(f\"\\nEmbedding norms:\")\nprint(f\"  Mean: {norms.mean():.4f}\")\nprint(f\"  Std:  {norms.std():.4f}\")\nprint(f\"  Min:  {norms.min():.4f}\")\nprint(f\"  Max:  {norms.max():.4f}\")\n\nif norms.std() / norms.mean() > 0.5:\n    print(\"  ⚠️ WARNING: Norm variance is high (some blocks have very different scales)\")\n\ndiagnostics['collapsed_dims'] = int(collapsed_dims)\ndiagnostics['norm_mean'] = float(norms.mean())\ndiagnostics['norm_std'] = float(norms.std())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 19: Nearest Neighbor Quality\n# ============================================================\n# For each block, is its nearest neighbor the same shape?\n# This is a strong test of embedding quality.\n\nprint(\"=\"*60)\nprint(\"NEAREST NEIGHBOR QUALITY\")\nprint(\"=\"*60)\n\n# Compute nearest neighbors\nprint(\"\\nFinding nearest neighbors for all blocks...\")\nall_sims = cosine_similarity(embeddings)\n\n# For each block, find nearest neighbor (excluding self)\nnn_same_shape = []\nnn_same_material = []\n\nfor i in range(len(embeddings)):\n    # Set self-similarity to -inf\n    sims = all_sims[i].copy()\n    sims[i] = -float('inf')\n    \n    # Find nearest neighbor\n    nearest = sims.argmax()\n    \n    # Check if same shape\n    my_shape = block_components[i].shape\n    nn_shape = block_components[nearest].shape\n    nn_same_shape.append(my_shape == nn_shape)\n    \n    # Check if same material\n    my_mat = block_components[i].material\n    nn_mat = block_components[nearest].material\n    nn_same_material.append(my_mat == nn_mat)\n\nnn_same_shape_pct = np.mean(nn_same_shape)\nnn_same_material_pct = np.mean(nn_same_material)\n\nprint(f\"\\nNearest Neighbor Results:\")\nprint(f\"  Same Shape:    {nn_same_shape_pct:>6.1%}\")\nprint(f\"  Same Material: {nn_same_material_pct:>6.1%}\")\n\nprint(\"\\nInterpretation:\")\nprint(\"  V3 expects HIGHER same-shape NN than V1/V2\")\nprint(\"  > 50% same shape = GOOD\")\nprint(\"  > 70% same shape = GREAT\")\nprint(\"  Random baseline ~5% (if 20 shapes)\")\n\ndiagnostics['nn_same_shape'] = float(nn_same_shape_pct)\ndiagnostics['nn_same_material'] = float(nn_same_material_pct)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 20: Diagnostic Visualizations\n# ============================================================\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# 1. Similarity Distribution Histogram\nax = axes[0, 0]\nax.hist(within_shape_sims, bins=50, alpha=0.7, label=f'Same Shape (μ={within_shape_sims.mean():.2f})', color='green', density=True)\nax.hist(across_shape_sims, bins=50, alpha=0.7, label=f'Different Shape (μ={across_shape_sims.mean():.2f})', color='red', density=True)\nax.axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Threshold')\nax.set_xlabel('Cosine Similarity')\nax.set_ylabel('Density')\nax.set_title(f'Similarity Distribution (Separation: {separation:.3f})')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# 2. Embedding Norm Distribution\nax = axes[0, 1]\nax.hist(norms, bins=50, color='blue', alpha=0.7)\nax.axvline(norms.mean(), color='red', linestyle='-', linewidth=2, label=f'Mean: {norms.mean():.2f}')\nax.axvline(norms.mean() - norms.std(), color='red', linestyle='--', alpha=0.5)\nax.axvline(norms.mean() + norms.std(), color='red', linestyle='--', alpha=0.5)\nax.set_xlabel('Embedding L2 Norm')\nax.set_ylabel('Count')\nax.set_title('Embedding Norm Distribution')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# 3. Component Embedding t-SNE: Materials\nax = axes[1, 0]\nmat_embs = model.material_emb.weight.detach().cpu().numpy()\nif len(mat_embs) > 3:\n    tsne_mat = TSNE(n_components=2, random_state=42, perplexity=min(5, len(mat_embs)-1))\n    mat_coords = tsne_mat.fit_transform(mat_embs)\n    ax.scatter(mat_coords[:, 0], mat_coords[:, 1], c='blue', s=50, alpha=0.7)\n    # Label some materials\n    for i, mat in enumerate(materials_list[:15]):\n        ax.annotate(mat, (mat_coords[i, 0], mat_coords[i, 1]), fontsize=8, alpha=0.8)\nax.set_title('Material Embedding Space (t-SNE)')\nax.set_xticks([])\nax.set_yticks([])\n\n# 4. Shape Embedding t-SNE\nax = axes[1, 1]\nshp_embs = model.shape_emb.weight.detach().cpu().numpy()\nif len(shp_embs) > 3:\n    # Sample if too many shapes\n    n_shapes = min(50, len(shp_embs))\n    shape_indices = np.random.choice(len(shp_embs), n_shapes, replace=False)\n    shp_sample = shp_embs[shape_indices]\n    shape_names_sample = [shapes_list[i] for i in shape_indices]\n    \n    tsne_shp = TSNE(n_components=2, random_state=42, perplexity=min(10, n_shapes-1))\n    shp_coords = tsne_shp.fit_transform(shp_sample)\n    ax.scatter(shp_coords[:, 0], shp_coords[:, 1], c='green', s=50, alpha=0.7)\n    # Label shapes\n    for i, shp in enumerate(shape_names_sample):\n        ax.annotate(shp, (shp_coords[i, 0], shp_coords[i, 1]), fontsize=7, alpha=0.8)\nax.set_title('Shape Embedding Space (t-SNE)')\nax.set_xticks([])\nax.set_yticks([])\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_DIR}/block2vec_v3_diagnostics.png\", dpi=150)\nplt.show()\n\nprint(f\"\\nSaved diagnostic visualization to {OUTPUT_DIR}/block2vec_v3_diagnostics.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CELL 21: Save All Diagnostics\n# ============================================================\n\n# Add coherence scores to diagnostics\ndiagnostics['overall_coherence'] = float(overall)\ndiagnostics['key_shape_coherence'] = {\n    shape: float(coherence[shape]['avg_similarity']) \n    for shape in key_shapes if shape in coherence\n}\n\n# Save diagnostics\nwith open(f\"{OUTPUT_DIR}/diagnostics_v3.json\", 'w') as f:\n    json.dump(diagnostics, f, indent=2)\n\nprint(\"=\"*60)\nprint(\"DIAGNOSTICS SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\n1. Similarity Separation: {diagnostics['separation']:.3f}\")\nprint(f\"   (Higher = better shape distinction)\")\nprint(f\"\\n2. Nearest Neighbor Same Shape: {diagnostics['nn_same_shape']:.1%}\")\nprint(f\"   (Higher = better shape clustering)\")\nprint(f\"\\n3. Collapsed Dimensions: {diagnostics['collapsed_dims']}/{TOTAL_EMBEDDING_DIM}\")\nprint(f\"   (Lower = healthier embeddings)\")\nprint(f\"\\n4. Embedding Norm Std: {diagnostics['norm_std']:.4f}\")\nprint(f\"   (Moderate = consistent embedding scales)\")\nprint(f\"\\n5. Overall Coherence: {diagnostics['overall_coherence']:.1%}\")\nprint(f\"   (V1 was ~20%, V2 was ~16%)\")\n\nprint(f\"\\nDiagnostics saved to {OUTPUT_DIR}/diagnostics_v3.json\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"If V3 fails, check these metrics to understand why:\")\nprint(\"  - Low separation → Components not distinct enough\")\nprint(\"  - Low NN same shape → Shape embedding not dominating\")\nprint(\"  - Many collapsed dims → Optimization issue\")\nprint(\"  - High norm variance → Imbalanced training\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}