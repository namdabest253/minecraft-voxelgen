{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE Embedding Validation V2\n",
    "\n",
    "## Improvements over V1 Validation\n",
    "\n",
    "| Issue | V1 Validation | V2 Validation (this notebook) |\n",
    "|-------|---------------|-------------------------------|\n",
    "| Codebook Learning | Gradient-based (unstable) | **EMA updates (stable)** |\n",
    "| Dead Code Reset | None | **Reset underutilized codes** |\n",
    "| Structure Weighting | 1x (equal) | **10x weight on structure** |\n",
    "| Commitment Cost | 0.25 | **0.5** |\n",
    "| Epochs | 10 | **20** |\n",
    "| Architecture | [32, 64, 128] | **[64, 128, 256]** |\n",
    "| Codebook Monitoring | None | **Track usage per epoch** |\n",
    "\n",
    "## Embeddings Tested\n",
    "\n",
    "| Embedding | Dimensions | Description |\n",
    "|-----------|------------|-------------|\n",
    "| V1 | 32 | Skip-gram (co-occurrence) |\n",
    "| V2 | 32 | Hybrid + state collapsing |\n",
    "| V3 | 40 | Compositional (material + shape + properties) |\n",
    "| Random | 32 | Random baseline |\n",
    "\n",
    "## Key Metric: Structure Accuracy\n",
    "\n",
    "~80% of voxels are air. Overall accuracy is meaningless. We track **structure accuracy** (non-air blocks only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Configuration\n",
    "# ============================================================\n",
    "\n",
    "# === Data Paths ===\n",
    "DATA_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/train\"\n",
    "VAL_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/val\"\n",
    "VOCAB_PATH = \"/kaggle/input/minecraft-schematics/tok2block.json\"\n",
    "\n",
    "# Embeddings paths\n",
    "V1_EMBEDDINGS_PATH = \"/kaggle/input/block2vec-embeddings/block_embeddings.npy\"\n",
    "V2_EMBEDDINGS_PATH = \"/kaggle/input/block2vec-v2/block_embeddings_v2.npy\"\n",
    "V2_MAPPING_PATH = \"/kaggle/input/block2vec-v2/original_to_collapsed.json\"\n",
    "V3_EMBEDDINGS_PATH = \"/kaggle/input/block2vec-v3/block_embeddings_v3.npy\"\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# === Model Architecture (IMPROVED) ===\n",
    "HIDDEN_DIMS = [64, 128, 256]  # Larger than v1 validation\n",
    "LATENT_DIM = 256\n",
    "NUM_CODEBOOK_ENTRIES = 512\n",
    "\n",
    "# === VQ-VAE Fixes (from vqvae_training_analysis.md) ===\n",
    "COMMITMENT_COST = 0.5       # Was 0.25 - too low\n",
    "EMA_DECAY = 0.99            # NEW: EMA codebook updates\n",
    "DEAD_CODE_THRESHOLD = 2     # NEW: Reset codes used less than this\n",
    "STRUCTURE_WEIGHT = 10.0     # NEW: Weight structure blocks 10x\n",
    "\n",
    "# === Training (IMPROVED) ===\n",
    "EPOCHS = 20                 # Was 10 - not enough\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 3e-4\n",
    "USE_AMP = True\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "# === Other ===\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(\"VQ-VAE Validation V2 Configuration:\")\n",
    "print(f\"  Hidden dims: {HIDDEN_DIMS}\")\n",
    "print(f\"  Latent dim: {LATENT_DIM}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS})\")\n",
    "print(f\"  Commitment cost: {COMMITMENT_COST}\")\n",
    "print(f\"  EMA decay: {EMA_DECAY}\")\n",
    "print(f\"  Structure weight: {STRUCTURE_WEIGHT}x\")\n",
    "print(f\"  Dead code threshold: {DEAD_CODE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load Vocabulary and Find Air Tokens\n",
    "# ============================================================\n",
    "\n",
    "# Load vocabulary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE} block types\")\n",
    "\n",
    "# Find ALL air tokens (critical bug fix from v1)\n",
    "# Token 0 is UNKNOWN_BLOCK, not air!\n",
    "AIR_TOKENS = set()\n",
    "for tok, block in tok2block.items():\n",
    "    block_lower = block.lower()\n",
    "    if 'air' in block_lower and 'stair' not in block_lower:\n",
    "        AIR_TOKENS.add(tok)\n",
    "        print(f\"  Found air token: {tok} = '{block}'\")\n",
    "\n",
    "AIR_TOKENS_LIST = sorted(AIR_TOKENS)\n",
    "AIR_TOKENS_TENSOR = torch.tensor(AIR_TOKENS_LIST, dtype=torch.long)\n",
    "print(f\"\\nAir tokens: {AIR_TOKENS_LIST}\")\n",
    "\n",
    "# Save for reproducibility\n",
    "air_info = {\n",
    "    \"air_tokens\": AIR_TOKENS_LIST,\n",
    "    \"note\": \"These tokens are excluded from structure accuracy calculation\"\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/air_tokens_used.json\", 'w') as f:\n",
    "    json.dump(air_info, f, indent=2)\n",
    "print(f\"Saved air tokens to {OUTPUT_DIR}/air_tokens_used.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Load and Prepare All Embeddings\n",
    "# ============================================================\n",
    "\n",
    "# V1: Skip-gram embeddings (32-dim)\n",
    "v1_embeddings = np.load(V1_EMBEDDINGS_PATH).astype(np.float32)\n",
    "print(f\"V1 embeddings: {v1_embeddings.shape}\")\n",
    "\n",
    "# V2: Hybrid embeddings with mapping (32-dim, collapsed vocab)\n",
    "v2_collapsed = np.load(V2_EMBEDDINGS_PATH).astype(np.float32)\n",
    "with open(V2_MAPPING_PATH, 'r') as f:\n",
    "    original_to_collapsed = {int(k): int(v) for k, v in json.load(f).items()}\n",
    "print(f\"V2 collapsed embeddings: {v2_collapsed.shape}\")\n",
    "\n",
    "# Expand V2 to full vocabulary\n",
    "v2_embeddings = np.zeros((VOCAB_SIZE, 32), dtype=np.float32)\n",
    "for orig_tok in range(VOCAB_SIZE):\n",
    "    if orig_tok in original_to_collapsed:\n",
    "        collapsed_tok = original_to_collapsed[orig_tok]\n",
    "        if collapsed_tok < len(v2_collapsed):\n",
    "            v2_embeddings[orig_tok] = v2_collapsed[collapsed_tok]\n",
    "print(f\"V2 expanded embeddings: {v2_embeddings.shape}\")\n",
    "\n",
    "# V3: Compositional embeddings (40-dim)\n",
    "v3_embeddings = np.load(V3_EMBEDDINGS_PATH).astype(np.float32)\n",
    "print(f\"V3 embeddings: {v3_embeddings.shape}\")\n",
    "\n",
    "# Random: Same scale as V1\n",
    "np.random.seed(SEED)\n",
    "v1_std = v1_embeddings.std()\n",
    "random_embeddings = np.random.randn(VOCAB_SIZE, 32).astype(np.float32) * v1_std\n",
    "print(f\"Random embeddings: {random_embeddings.shape} (std={v1_std:.4f})\")\n",
    "\n",
    "# Store all variants with their dimensions\n",
    "EMBEDDINGS = {\n",
    "    'V1': {'embeddings': v1_embeddings, 'dim': 32},\n",
    "    'V2': {'embeddings': v2_embeddings, 'dim': 32},\n",
    "    'V3': {'embeddings': v3_embeddings, 'dim': 40},\n",
    "    'Random': {'embeddings': random_embeddings, 'dim': 32},\n",
    "}\n",
    "\n",
    "print(\"\\nAll embeddings loaded:\")\n",
    "for name, data in EMBEDDINGS.items():\n",
    "    print(f\"  {name}: {data['embeddings'].shape} ({data['dim']}-dim)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Dataset\n",
    "# ============================================================\n",
    "\n",
    "class VQVAEDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, seed: int = 42):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        if not self.h5_files:\n",
    "            raise ValueError(f\"No H5 files found in {data_dir}\")\n",
    "        print(f\"Found {len(self.h5_files)} structures in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.h5_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_files[idx], 'r') as f:\n",
    "            key = list(f.keys())[0]\n",
    "            structure = f[key][:].astype(np.int64)\n",
    "        return torch.from_numpy(structure).long()\n",
    "\n",
    "\n",
    "train_dataset = VQVAEDataset(DATA_DIR, seed=SEED)\n",
    "val_dataset = VQVAEDataset(VAL_DIR, seed=SEED)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Improved VQ-VAE Model with EMA and Dead Code Reset\n",
    "# ============================================================\n",
    "\n",
    "class ResidualBlock3D(nn.Module):\n",
    "    \"\"\"3D Residual block with batch normalization.\"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(channels)\n",
    "        self.bn2 = nn.BatchNorm3d(channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class VectorQuantizerEMA(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantizer with EMA updates and dead code reset.\n",
    "    \n",
    "    Key improvements over gradient-based VQ:\n",
    "    1. EMA updates are more stable than gradient descent\n",
    "    2. Dead code reset prevents codebook collapse\n",
    "    3. Tracks codebook usage for monitoring\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_codes: int,\n",
    "        latent_dim: int,\n",
    "        commitment_cost: float = 0.5,\n",
    "        ema_decay: float = 0.99,\n",
    "        dead_code_threshold: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_codes = num_codes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.ema_decay = ema_decay\n",
    "        self.dead_code_threshold = dead_code_threshold\n",
    "        \n",
    "        # Codebook\n",
    "        self.register_buffer('codebook', torch.randn(num_codes, latent_dim))\n",
    "        self.codebook.data.uniform_(-1/num_codes, 1/num_codes)\n",
    "        \n",
    "        # EMA tracking\n",
    "        self.register_buffer('ema_cluster_size', torch.zeros(num_codes))\n",
    "        self.register_buffer('ema_embed_sum', torch.zeros(num_codes, latent_dim))\n",
    "        \n",
    "        # Usage tracking (for monitoring)\n",
    "        self.register_buffer('code_usage', torch.zeros(num_codes))\n",
    "    \n",
    "    def reset_usage_stats(self):\n",
    "        \"\"\"Reset usage stats at start of each epoch.\"\"\"\n",
    "        self.code_usage.zero_()\n",
    "    \n",
    "    def get_codebook_usage(self) -> float:\n",
    "        \"\"\"Return fraction of codes used this epoch.\"\"\"\n",
    "        return (self.code_usage > 0).float().mean().item()\n",
    "    \n",
    "    def forward(self, z_e: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z_e: Encoder output [B, C, D, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            z_q: Quantized output [B, C, D, H, W]\n",
    "            vq_loss: Commitment loss (codebook updated via EMA)\n",
    "            indices: Code indices [B, D, H, W]\n",
    "        \"\"\"\n",
    "        # Reshape: [B, C, D, H, W] -> [B*D*H*W, C]\n",
    "        z_e_perm = z_e.permute(0, 2, 3, 4, 1).contiguous()\n",
    "        flat = z_e_perm.view(-1, self.latent_dim)\n",
    "        \n",
    "        # Compute distances to codebook entries\n",
    "        d = (\n",
    "            flat.pow(2).sum(1, keepdim=True)\n",
    "            + self.codebook.pow(2).sum(1)\n",
    "            - 2 * flat @ self.codebook.t()\n",
    "        )\n",
    "        \n",
    "        # Find nearest codes\n",
    "        indices = d.argmin(dim=1)\n",
    "        \n",
    "        # Update usage tracking\n",
    "        with torch.no_grad():\n",
    "            unique_indices = indices.unique()\n",
    "            self.code_usage[unique_indices] += 1\n",
    "        \n",
    "        # Get quantized vectors\n",
    "        z_q_flat = self.codebook[indices]\n",
    "        z_q_perm = z_q_flat.view(z_e_perm.shape)\n",
    "        \n",
    "        # EMA codebook update (only during training)\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                # One-hot encoding of assignments\n",
    "                encodings = F.one_hot(indices, self.num_codes).float()\n",
    "                \n",
    "                # Update cluster sizes\n",
    "                batch_cluster_size = encodings.sum(0)\n",
    "                self.ema_cluster_size = (\n",
    "                    self.ema_decay * self.ema_cluster_size\n",
    "                    + (1 - self.ema_decay) * batch_cluster_size\n",
    "                )\n",
    "                \n",
    "                # Update embedding sums\n",
    "                batch_embed_sum = encodings.t() @ flat\n",
    "                self.ema_embed_sum = (\n",
    "                    self.ema_decay * self.ema_embed_sum\n",
    "                    + (1 - self.ema_decay) * batch_embed_sum\n",
    "                )\n",
    "                \n",
    "                # Laplace smoothing to avoid division by zero\n",
    "                n = self.ema_cluster_size.sum()\n",
    "                smoothed_cluster_size = (\n",
    "                    (self.ema_cluster_size + 1e-5)\n",
    "                    / (n + self.num_codes * 1e-5)\n",
    "                    * n\n",
    "                )\n",
    "                \n",
    "                # Update codebook\n",
    "                self.codebook = self.ema_embed_sum / smoothed_cluster_size.unsqueeze(1)\n",
    "                \n",
    "                # Dead code reset: reinitialize underutilized codes\n",
    "                dead_codes = batch_cluster_size < self.dead_code_threshold\n",
    "                if dead_codes.any() and flat.size(0) > 0:\n",
    "                    # Sample random encoder outputs to reinitialize dead codes\n",
    "                    n_dead = dead_codes.sum().item()\n",
    "                    random_indices = torch.randint(0, flat.size(0), (n_dead,), device=flat.device)\n",
    "                    self.codebook[dead_codes] = flat[random_indices]\n",
    "                    # Reset EMA stats for these codes\n",
    "                    self.ema_cluster_size[dead_codes] = 1\n",
    "                    self.ema_embed_sum[dead_codes] = flat[random_indices]\n",
    "        \n",
    "        # Commitment loss (encoder should commit to codes)\n",
    "        commitment_loss = F.mse_loss(z_e_perm, z_q_perm.detach())\n",
    "        vq_loss = self.commitment_cost * commitment_loss\n",
    "        \n",
    "        # Straight-through estimator\n",
    "        z_q_st = z_e_perm + (z_q_perm - z_e_perm).detach()\n",
    "        z_q = z_q_st.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        \n",
    "        return z_q, vq_loss, indices.view(z_e_perm.shape[:-1])\n",
    "\n",
    "\n",
    "class ImprovedVQVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    VQ-VAE with all improvements from vqvae_training_analysis.md:\n",
    "    - EMA codebook updates\n",
    "    - Dead code reset\n",
    "    - Structure-weighted loss\n",
    "    - Flexible embedding dimensions\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_dims: List[int],\n",
    "        latent_dim: int,\n",
    "        num_codes: int,\n",
    "        commitment_cost: float,\n",
    "        ema_decay: float,\n",
    "        dead_code_threshold: int,\n",
    "        pretrained_embeddings: np.ndarray,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Block embeddings (frozen)\n",
    "        self.block_emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.block_emb.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.block_emb.weight.requires_grad = False\n",
    "        \n",
    "        # Encoder - note: first layer adapts to embedding_dim\n",
    "        enc_layers = []\n",
    "        in_ch = embedding_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            enc_layers.extend([\n",
    "                nn.Conv3d(in_ch, h_dim, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                ResidualBlock3D(h_dim),\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        enc_layers.append(nn.Conv3d(in_ch, latent_dim, 3, padding=1))\n",
    "        self.encoder = nn.Sequential(*enc_layers)\n",
    "        \n",
    "        # Vector Quantizer with EMA\n",
    "        self.quantizer = VectorQuantizerEMA(\n",
    "            num_codes=num_codes,\n",
    "            latent_dim=latent_dim,\n",
    "            commitment_cost=commitment_cost,\n",
    "            ema_decay=ema_decay,\n",
    "            dead_code_threshold=dead_code_threshold,\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        dec_layers = []\n",
    "        in_ch = latent_dim\n",
    "        for h_dim in reversed(hidden_dims):\n",
    "            dec_layers.extend([\n",
    "                ResidualBlock3D(in_ch),\n",
    "                nn.ConvTranspose3d(in_ch, h_dim, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm3d(h_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ])\n",
    "            in_ch = h_dim\n",
    "        dec_layers.append(nn.Conv3d(in_ch, vocab_size, 3, padding=1))\n",
    "        self.decoder = nn.Sequential(*dec_layers)\n",
    "    \n",
    "    def forward(self, block_ids: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # Embed blocks\n",
    "        x = self.block_emb(block_ids)  # [B, 32, 32, 32, emb_dim]\n",
    "        x = x.permute(0, 4, 1, 2, 3).contiguous()  # [B, emb_dim, 32, 32, 32]\n",
    "        \n",
    "        # Encode\n",
    "        z_e = self.encoder(x)\n",
    "        \n",
    "        # Quantize\n",
    "        z_q, vq_loss, indices = self.quantizer(z_e)\n",
    "        \n",
    "        # Decode\n",
    "        logits = self.decoder(z_q)\n",
    "        \n",
    "        return {'logits': logits, 'vq_loss': vq_loss, 'indices': indices}\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        block_ids: torch.Tensor,\n",
    "        air_tokens: torch.Tensor,\n",
    "        structure_weight: float = 10.0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute loss with structure weighting.\n",
    "        \n",
    "        Structure blocks are weighted 10x to counter the 80% air imbalance.\n",
    "        \"\"\"\n",
    "        out = self(block_ids)\n",
    "        \n",
    "        # Reshape logits for loss computation\n",
    "        logits = out['logits'].permute(0, 2, 3, 4, 1).contiguous()  # [B, 32, 32, 32, vocab]\n",
    "        logits_flat = logits.view(-1, self.vocab_size)\n",
    "        targets_flat = block_ids.view(-1)\n",
    "        \n",
    "        # Compute per-element cross entropy\n",
    "        ce_loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
    "        \n",
    "        # Apply structure weighting\n",
    "        air_tokens_device = air_tokens.to(targets_flat.device)\n",
    "        is_air = torch.isin(targets_flat, air_tokens_device)\n",
    "        is_structure = ~is_air\n",
    "        \n",
    "        # Weight: air=1, structure=structure_weight\n",
    "        weights = torch.ones_like(ce_loss)\n",
    "        weights[is_structure] = structure_weight\n",
    "        \n",
    "        # Weighted mean\n",
    "        recon_loss = (weights * ce_loss).sum() / weights.sum()\n",
    "        \n",
    "        total_loss = recon_loss + out['vq_loss']\n",
    "        \n",
    "        # Compute accuracy metrics\n",
    "        with torch.no_grad():\n",
    "            preds_flat = logits_flat.argmax(dim=-1)\n",
    "            correct = (preds_flat == targets_flat).float()\n",
    "            \n",
    "            # Overall accuracy\n",
    "            acc = correct.mean()\n",
    "            \n",
    "            # Air accuracy\n",
    "            air_acc = correct[is_air].mean() if is_air.any() else torch.tensor(0.0)\n",
    "            \n",
    "            # Structure accuracy (KEY METRIC)\n",
    "            struct_acc = correct[is_structure].mean() if is_structure.any() else torch.tensor(0.0)\n",
    "            \n",
    "            # Air percentage\n",
    "            air_pct = is_air.float().mean()\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'vq_loss': out['vq_loss'],\n",
    "            'accuracy': acc,\n",
    "            'air_accuracy': air_acc,\n",
    "            'struct_accuracy': struct_acc,\n",
    "            'air_percentage': air_pct,\n",
    "            'indices': out['indices'],\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"ImprovedVQVAE with EMA codebook and structure weighting defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Training Functions\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(\n",
    "    model: ImprovedVQVAE,\n",
    "    loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scaler: torch.cuda.amp.GradScaler,\n",
    "    device: str,\n",
    "    air_tokens: torch.Tensor,\n",
    "    structure_weight: float,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    model.quantizer.reset_usage_stats()\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': 0, 'recon': 0, 'vq': 0,\n",
    "        'acc': 0, 'air_acc': 0, 'struct_acc': 0, 'air_pct': 0\n",
    "    }\n",
    "    n = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss(batch, air_tokens, structure_weight)\n",
    "            loss = out['loss'] / GRAD_ACCUM_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['recon'] += out['recon_loss'].item()\n",
    "        metrics['vq'] += out['vq_loss'].item()\n",
    "        metrics['acc'] += out['accuracy'].item()\n",
    "        metrics['air_acc'] += out['air_accuracy'].item()\n",
    "        metrics['struct_acc'] += out['struct_accuracy'].item()\n",
    "        metrics['air_pct'] += out['air_percentage'].item()\n",
    "        n += 1\n",
    "    \n",
    "    # Handle remaining gradients\n",
    "    if len(loader) % GRAD_ACCUM_STEPS != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Add codebook usage\n",
    "    metrics['codebook_usage'] = model.quantizer.get_codebook_usage()\n",
    "    \n",
    "    return {k: v/n if k != 'codebook_usage' else v for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    model: ImprovedVQVAE,\n",
    "    loader: DataLoader,\n",
    "    device: str,\n",
    "    air_tokens: torch.Tensor,\n",
    "    structure_weight: float,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    model.quantizer.reset_usage_stats()\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': 0, 'recon': 0,\n",
    "        'acc': 0, 'air_acc': 0, 'struct_acc': 0, 'air_pct': 0\n",
    "    }\n",
    "    n = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "            out = model.compute_loss(batch, air_tokens, structure_weight)\n",
    "        \n",
    "        metrics['loss'] += out['loss'].item()\n",
    "        metrics['recon'] += out['recon_loss'].item()\n",
    "        metrics['acc'] += out['accuracy'].item()\n",
    "        metrics['air_acc'] += out['air_accuracy'].item()\n",
    "        metrics['struct_acc'] += out['struct_accuracy'].item()\n",
    "        metrics['air_pct'] += out['air_percentage'].item()\n",
    "        n += 1\n",
    "    \n",
    "    metrics['codebook_usage'] = model.quantizer.get_codebook_usage()\n",
    "    \n",
    "    return {k: v/n if k != 'codebook_usage' else v for k, v in metrics.items()}\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Run Experiment for One Embedding Type\n",
    "# ============================================================\n",
    "\n",
    "def run_experiment(\n",
    "    name: str,\n",
    "    embeddings: np.ndarray,\n",
    "    embedding_dim: int,\n",
    "    air_tokens: torch.Tensor,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Train and evaluate VQ-VAE with given embeddings.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training with {name} embeddings ({embedding_dim}-dim)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Set seeds\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "    # Create model\n",
    "    model = ImprovedVQVAE(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dims=HIDDEN_DIMS,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        num_codes=NUM_CODEBOOK_ENTRIES,\n",
    "        commitment_cost=COMMITMENT_COST,\n",
    "        ema_decay=EMA_DECAY,\n",
    "        dead_code_threshold=DEAD_CODE_THRESHOLD,\n",
    "        pretrained_embeddings=embeddings,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count params\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB allocated\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_air_acc': [],\n",
    "        'train_struct_acc': [], 'train_air_pct': [], 'train_codebook_usage': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_air_acc': [],\n",
    "        'val_struct_acc': [], 'val_air_pct': [], 'val_codebook_usage': [],\n",
    "    }\n",
    "    \n",
    "    best_val_struct_acc = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_metrics = train_epoch(\n",
    "            model, train_loader, optimizer, scaler, device,\n",
    "            air_tokens, STRUCTURE_WEIGHT\n",
    "        )\n",
    "        val_metrics = validate(\n",
    "            model, val_loader, device, air_tokens, STRUCTURE_WEIGHT\n",
    "        )\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_metrics['loss'])\n",
    "        history['train_acc'].append(train_metrics['acc'])\n",
    "        history['train_air_acc'].append(train_metrics['air_acc'])\n",
    "        history['train_struct_acc'].append(train_metrics['struct_acc'])\n",
    "        history['train_air_pct'].append(train_metrics['air_pct'])\n",
    "        history['train_codebook_usage'].append(train_metrics['codebook_usage'])\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['acc'])\n",
    "        history['val_air_acc'].append(val_metrics['air_acc'])\n",
    "        history['val_struct_acc'].append(val_metrics['struct_acc'])\n",
    "        history['val_air_pct'].append(val_metrics['air_pct'])\n",
    "        history['val_codebook_usage'].append(val_metrics['codebook_usage'])\n",
    "        \n",
    "        # Track best\n",
    "        if val_metrics['struct_acc'] > best_val_struct_acc:\n",
    "            best_val_struct_acc = val_metrics['struct_acc']\n",
    "        \n",
    "        # Print progress\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
    "            f\"Loss: {train_metrics['loss']:.3f} | \"\n",
    "            f\"Struct: {train_metrics['struct_acc']:.1%} | \"\n",
    "            f\"Val Struct: {val_metrics['struct_acc']:.1%} | \"\n",
    "            f\"CB Usage: {train_metrics['codebook_usage']:.1%}\"\n",
    "        )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training time: {train_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Sanity checks\n",
    "    final_struct = history['val_struct_acc'][-1]\n",
    "    final_overall = history['val_acc'][-1]\n",
    "    if abs(final_struct - final_overall) < 0.01:\n",
    "        print(\"WARNING: Structure and overall accuracy too similar - air detection may be broken!\")\n",
    "    else:\n",
    "        print(f\"OK: Structure ({final_struct:.1%}) differs from overall ({final_overall:.1%})\")\n",
    "    \n",
    "    final_codebook_usage = history['train_codebook_usage'][-1]\n",
    "    if final_codebook_usage < 0.2:\n",
    "        print(f\"WARNING: Low codebook usage ({final_codebook_usage:.1%}) - possible collapse!\")\n",
    "    else:\n",
    "        print(f\"OK: Codebook usage ({final_codebook_usage:.1%}) is healthy\")\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'embedding_dim': embedding_dim,\n",
    "        'final_val_loss': history['val_loss'][-1],\n",
    "        'final_val_acc': history['val_acc'][-1],\n",
    "        'final_val_struct_acc': history['val_struct_acc'][-1],\n",
    "        'final_val_air_acc': history['val_air_acc'][-1],\n",
    "        'best_val_struct_acc': best_val_struct_acc,\n",
    "        'best_val_loss': min(history['val_loss']),\n",
    "        'best_val_acc': max(history['val_acc']),\n",
    "        'final_codebook_usage': final_codebook_usage,\n",
    "        'avg_air_pct': np.mean(history['val_air_pct']),\n",
    "        'training_time': train_time,\n",
    "        'history': history,\n",
    "    }\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, optimizer, scaler\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Experiment function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Run All Experiments\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VQ-VAE EMBEDDING VALIDATION V2\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nImprovements over V1 validation:\")\n",
    "print(f\"  - EMA codebook (decay={EMA_DECAY})\")\n",
    "print(f\"  - Dead code reset (threshold={DEAD_CODE_THRESHOLD})\")\n",
    "print(f\"  - Structure weighting ({STRUCTURE_WEIGHT}x)\")\n",
    "print(f\"  - Commitment cost: {COMMITMENT_COST}\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Architecture: {HIDDEN_DIMS}\")\n",
    "print(f\"\\nAir tokens: {AIR_TOKENS_LIST}\")\n",
    "print(\"\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for name, data in EMBEDDINGS.items():\n",
    "    results = run_experiment(\n",
    "        name=name,\n",
    "        embeddings=data['embeddings'],\n",
    "        embedding_dim=data['dim'],\n",
    "        air_tokens=AIR_TOKENS_TENSOR,\n",
    "    )\n",
    "    all_results[name] = results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Compare Results\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get average air percentage\n",
    "avg_air_pct = np.mean([r['avg_air_pct'] for r in all_results.values()])\n",
    "print(f\"\\nAverage air percentage: {avg_air_pct:.1%}\")\n",
    "print(\"This is why STRUCTURE accuracy is the key metric!\\n\")\n",
    "\n",
    "# Results table\n",
    "print(\"{:<10} {:>8} {:>10} {:>12} {:>12} {:>10} {:>10}\".format(\n",
    "    \"Embedding\", \"Dim\", \"Val Loss\", \"Overall Acc\", \"STRUCT Acc\", \"CB Usage\", \"Time\"\n",
    "))\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Sort by structure accuracy\n",
    "sorted_names = sorted(\n",
    "    all_results.keys(),\n",
    "    key=lambda x: all_results[x]['best_val_struct_acc'],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for name in sorted_names:\n",
    "    r = all_results[name]\n",
    "    print(\"{:<10} {:>8} {:>10.4f} {:>12.1%} {:>12.1%} {:>10.1%} {:>8.1f}m\".format(\n",
    "        name,\n",
    "        r['embedding_dim'],\n",
    "        r['best_val_loss'],\n",
    "        r['best_val_acc'],\n",
    "        r['best_val_struct_acc'],\n",
    "        r['final_codebook_usage'],\n",
    "        r['training_time']/60,\n",
    "    ))\n",
    "\n",
    "# Improvement over random\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPROVEMENT OVER RANDOM BASELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "random_struct = all_results['Random']['best_val_struct_acc']\n",
    "\n",
    "for name in ['V1', 'V2', 'V3']:\n",
    "    struct = all_results[name]['best_val_struct_acc']\n",
    "    improvement = (struct - random_struct) / random_struct * 100\n",
    "    print(f\"\\n{name} vs Random:\")\n",
    "    print(f\"  Structure Acc: {struct:.1%} vs {random_struct:.1%}\")\n",
    "    print(f\"  Improvement: {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: Plot Results\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "colors = {'V1': 'blue', 'V2': 'green', 'V3': 'purple', 'Random': 'red'}\n",
    "names = ['V1', 'V2', 'V3', 'Random']\n",
    "\n",
    "# Row 1: Training metrics\n",
    "\n",
    "# Validation Loss\n",
    "ax = axes[0, 0]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name]['history']['val_loss'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Validation Loss', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overall Accuracy\n",
    "ax = axes[0, 1]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name]['history']['val_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Overall Accuracy (includes ~80% air)', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# STRUCTURE Accuracy (KEY)\n",
    "ax = axes[0, 2]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name]['history']['val_struct_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('STRUCTURE Accuracy (KEY METRIC)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Structure Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Codebook Usage\n",
    "ax = axes[0, 3]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name]['history']['train_codebook_usage'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Codebook Usage (should be >30%)', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Usage %')\n",
    "ax.axhline(y=0.3, color='gray', linestyle='--', alpha=0.5, label='Min healthy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: More metrics\n",
    "\n",
    "# Air Accuracy\n",
    "ax = axes[1, 0]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name]['history']['val_air_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Air Block Accuracy', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Air Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Training Loss\n",
    "ax = axes[1, 1]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name]['history']['train_loss'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_title('Training Loss', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Train vs Val Structure Accuracy (check overfitting)\n",
    "ax = axes[1, 2]\n",
    "for name in names:\n",
    "    ax.plot(all_results[name]['history']['train_struct_acc'], \n",
    "            label=f'{name} train', color=colors[name], linewidth=2, linestyle='--')\n",
    "    ax.plot(all_results[name]['history']['val_struct_acc'],\n",
    "            label=f'{name} val', color=colors[name], linewidth=2)\n",
    "ax.set_title('Train vs Val Structure Accuracy', fontsize=12)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Structure Accuracy')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart comparison\n",
    "ax = axes[1, 3]\n",
    "struct_accs = [all_results[name]['best_val_struct_acc'] for name in names]\n",
    "bar_colors = [colors[name] for name in names]\n",
    "bars = ax.bar(names, struct_accs, color=bar_colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_title('Best Structure Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim(0, max(struct_accs) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, struct_accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{acc:.1%}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/embedding_comparison_v2.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved plot to {OUTPUT_DIR}/embedding_comparison_v2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Save Results\n",
    "# ============================================================\n",
    "\n",
    "# Save summary\n",
    "summary = {}\n",
    "for name, r in all_results.items():\n",
    "    summary[name] = {\n",
    "        'embedding_dim': r['embedding_dim'],\n",
    "        'final_val_loss': float(r['final_val_loss']),\n",
    "        'final_val_acc': float(r['final_val_acc']),\n",
    "        'final_val_struct_acc': float(r['final_val_struct_acc']),\n",
    "        'final_val_air_acc': float(r['final_val_air_acc']),\n",
    "        'best_val_loss': float(r['best_val_loss']),\n",
    "        'best_val_acc': float(r['best_val_acc']),\n",
    "        'best_val_struct_acc': float(r['best_val_struct_acc']),\n",
    "        'final_codebook_usage': float(r['final_codebook_usage']),\n",
    "        'avg_air_percentage': float(r['avg_air_pct']),\n",
    "        'training_time_minutes': float(r['training_time'] / 60),\n",
    "    }\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/embedding_validation_v2_results.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Summary saved to {OUTPUT_DIR}/embedding_validation_v2_results.json\")\n",
    "\n",
    "# Save full history\n",
    "full_results = {}\n",
    "for name, r in all_results.items():\n",
    "    full_results[name] = {\n",
    "        'embedding_dim': r['embedding_dim'],\n",
    "        'history': {k: [float(x) for x in v] for k, v in r['history'].items()},\n",
    "        'training_time': float(r['training_time']),\n",
    "        'best_val_struct_acc': float(r['best_val_struct_acc']),\n",
    "        'best_val_loss': float(r['best_val_loss']),\n",
    "    }\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/embedding_validation_v2_full.json\", 'w') as f:\n",
    "    json.dump(full_results, f, indent=2)\n",
    "print(f\"Full history saved to {OUTPUT_DIR}/embedding_validation_v2_full.json\")\n",
    "\n",
    "# Save config for reproducibility\n",
    "config = {\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'hidden_dims': HIDDEN_DIMS,\n",
    "    'latent_dim': LATENT_DIM,\n",
    "    'num_codebook_entries': NUM_CODEBOOK_ENTRIES,\n",
    "    'commitment_cost': COMMITMENT_COST,\n",
    "    'ema_decay': EMA_DECAY,\n",
    "    'dead_code_threshold': DEAD_CODE_THRESHOLD,\n",
    "    'structure_weight': STRUCTURE_WEIGHT,\n",
    "    'seed': SEED,\n",
    "    'air_tokens': AIR_TOKENS_LIST,\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/validation_config_v2.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"Config saved to {OUTPUT_DIR}/validation_config_v2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Final Summary and Conclusion\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VQ-VAE EMBEDDING VALIDATION V2 - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Architecture: {HIDDEN_DIMS}\")\n",
    "print(f\"  EMA decay: {EMA_DECAY}\")\n",
    "print(f\"  Structure weight: {STRUCTURE_WEIGHT}x\")\n",
    "print(f\"  Commitment cost: {COMMITMENT_COST}\")\n",
    "\n",
    "print(f\"\\nAir tokens excluded: {AIR_TOKENS_LIST}\")\n",
    "print(f\"Average air percentage: {avg_air_pct:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS (sorted by structure accuracy)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n{:<12} {:>12} {:>15} {:>15} {:>12}\".format(\n",
    "    \"\", \"Val Loss\", \"Overall Acc\", \"STRUCT Acc\", \"CB Usage\"\n",
    "))\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name in sorted_names:\n",
    "    r = all_results[name]\n",
    "    print(\"{:<12} {:>12.4f} {:>15.1%} {:>15.1%} {:>12.1%}\".format(\n",
    "        name,\n",
    "        r['best_val_loss'],\n",
    "        r['best_val_acc'],\n",
    "        r['best_val_struct_acc'],\n",
    "        r['final_codebook_usage'],\n",
    "    ))\n",
    "\n",
    "# Winner\n",
    "winner = sorted_names[0]\n",
    "winner_struct = all_results[winner]['best_val_struct_acc']\n",
    "random_struct = all_results['Random']['best_val_struct_acc']\n",
    "improvement = (winner_struct - random_struct) / random_struct * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nWINNER: {winner} with {winner_struct:.1%} structure accuracy\")\n",
    "print(f\"Improvement over random: {improvement:+.1f}%\")\n",
    "\n",
    "# Compare V1 vs V3\n",
    "v1_struct = all_results['V1']['best_val_struct_acc']\n",
    "v3_struct = all_results['V3']['best_val_struct_acc']\n",
    "v1_vs_v3 = (v1_struct - v3_struct) / v3_struct * 100\n",
    "\n",
    "print(f\"\\nV1 vs V3: {v1_vs_v3:+.1f}%\")\n",
    "\n",
    "if abs(v1_vs_v3) < 5:\n",
    "    print(\"  -> V1 and V3 are comparable. Either could work.\")\n",
    "elif v1_vs_v3 > 0:\n",
    "    print(\"  -> V1 skip-gram is better. Use V1 for VQ-VAE.\")\n",
    "else:\n",
    "    print(\"  -> V3 compositional is better! Use V3 for VQ-VAE.\")\n",
    "\n",
    "# Codebook health check\n",
    "print(\"\\nCodebook Health:\")\n",
    "for name in names:\n",
    "    usage = all_results[name]['final_codebook_usage']\n",
    "    status = \"OK\" if usage > 0.3 else \"WARNING - possible collapse\"\n",
    "    print(f\"  {name}: {usage:.1%} ({status})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Files saved:\")\n",
    "print(f\"  - {OUTPUT_DIR}/embedding_comparison_v2.png\")\n",
    "print(f\"  - {OUTPUT_DIR}/embedding_validation_v2_results.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/embedding_validation_v2_full.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/validation_config_v2.json\")\n",
    "print(f\"  - {OUTPUT_DIR}/air_tokens_used.json\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
