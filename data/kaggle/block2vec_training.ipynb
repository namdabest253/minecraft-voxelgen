{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block2Vec: Learning Minecraft Block Embeddings\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook trains a neural network to learn **meaningful numerical representations** (called \"embeddings\") for Minecraft blocks. After training, similar blocks will have similar embeddings - for example, `oak_planks` and `spruce_planks` will be close together in the embedding space, while `oak_planks` and `lava` will be far apart.\n",
    "\n",
    "## Why Do We Need This?\n",
    "\n",
    "This is **Phase 2** of a larger project to generate Minecraft structures from text prompts. The pipeline is:\n",
    "\n",
    "1. **Phase 1 (Done)**: Prepare training data - 4,462 Minecraft builds as 3D arrays\n",
    "2. **Phase 2 (This Notebook)**: Train Block2Vec to learn block embeddings\n",
    "3. **Phase 3**: Train a VQ-VAE to compress/decompress structures\n",
    "4. **Phase 4**: Connect text descriptions to the VQ-VAE\n",
    "5. **Phase 5**: Generate new structures from text!\n",
    "\n",
    "The embeddings we learn here will be used as input to the VQ-VAE in Phase 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Embeddings\n",
    "\n",
    "## What is an Embedding?\n",
    "\n",
    "An **embedding** is a way to represent something (a word, a block, an image) as a list of numbers (a vector). The key insight is that we want **similar things to have similar vectors**.\n",
    "\n",
    "### Example: Representing Colors\n",
    "\n",
    "Imagine you want a computer to understand colors. You could:\n",
    "\n",
    "**Bad approach - One-hot encoding:**\n",
    "```\n",
    "red   = [1, 0, 0, 0, 0]\n",
    "blue  = [0, 1, 0, 0, 0]\n",
    "green = [0, 0, 1, 0, 0]\n",
    "pink  = [0, 0, 0, 1, 0]\n",
    "navy  = [0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "Problem: Every color is equally different from every other color. But pink is more similar to red than to green!\n",
    "\n",
    "**Good approach - Learned embeddings:**\n",
    "```\n",
    "red   = [0.9, 0.1, 0.2]  # High red, low blue, low green\n",
    "pink  = [0.8, 0.1, 0.5]  # Similar to red!\n",
    "blue  = [0.1, 0.9, 0.2]\n",
    "navy  = [0.1, 0.7, 0.1]  # Similar to blue!\n",
    "green = [0.2, 0.1, 0.9]\n",
    "```\n",
    "\n",
    "Now similar colors have similar numbers!\n",
    "\n",
    "### Why 32 Dimensions?\n",
    "\n",
    "We use 32-dimensional embeddings (each block becomes a list of 32 numbers). Why 32?\n",
    "\n",
    "- **Too few dimensions (e.g., 2)**: Can't capture enough nuance. Imagine trying to describe a person with only 2 numbers!\n",
    "- **Too many dimensions (e.g., 1000)**: Wastes memory, slower training, and risks \"overfitting\" (memorizing instead of learning)\n",
    "- **32 dimensions**: A sweet spot that captures block relationships without being excessive\n",
    "\n",
    "This is a **hyperparameter** - a choice we make before training. Finding good hyperparameters is part art, part science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: The Skip-Gram Model (Word2Vec for Blocks)\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "Block2Vec is inspired by **Word2Vec**, a famous algorithm for learning word embeddings. The key insight:\n",
    "\n",
    "> **\"You shall know a word by the company it keeps\"** - J.R. Firth, 1957\n",
    "\n",
    "In text, words that appear in similar contexts have similar meanings:\n",
    "- \"The **cat** sat on the mat\"\n",
    "- \"The **dog** sat on the mat\"\n",
    "\n",
    "Since \"cat\" and \"dog\" appear in similar contexts, they should have similar embeddings.\n",
    "\n",
    "## Applying This to Minecraft\n",
    "\n",
    "In Minecraft, we use **spatial context** instead of textual context:\n",
    "\n",
    "- Blocks that appear **next to similar blocks** should have similar embeddings\n",
    "- Oak stairs often appear next to oak planks\n",
    "- Stone bricks often appear next to other stone bricks\n",
    "- Torches often appear on walls (next to stone, wood, etc.)\n",
    "\n",
    "## Skip-Gram Architecture\n",
    "\n",
    "The Skip-Gram model works by:\n",
    "\n",
    "1. **Input**: A center block (e.g., `oak_planks`)\n",
    "2. **Goal**: Predict the neighboring blocks\n",
    "3. **Learning**: Adjust embeddings so prediction gets better\n",
    "\n",
    "```\n",
    "     [air]     [oak_stairs]\n",
    "        \\         /\n",
    "         \\       /\n",
    "    [oak_log]--[OAK_PLANKS]--[oak_log]\n",
    "         /       \\\n",
    "        /         \\\n",
    "   [stone]      [glass]\n",
    "   \n",
    "Center block: oak_planks\n",
    "Context blocks: air, oak_stairs, oak_log, oak_log, stone, glass\n",
    "```\n",
    "\n",
    "The model learns that `oak_planks` should be close to `oak_log` and `oak_stairs` in embedding space because they frequently appear together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Negative Sampling\n",
    "\n",
    "## The Problem with Naive Training\n",
    "\n",
    "A naive approach would be:\n",
    "\n",
    "1. For center block `oak_planks`, predict probability of ALL 3,717 blocks being neighbors\n",
    "2. The correct neighbors get high probability, others get low\n",
    "3. This requires computing 3,717 probabilities per training example!\n",
    "\n",
    "This is **extremely slow** because we have millions of training examples.\n",
    "\n",
    "## The Solution: Negative Sampling\n",
    "\n",
    "Instead of predicting all blocks, we:\n",
    "\n",
    "1. Take one **positive pair**: (oak_planks, oak_log) - they ARE neighbors\n",
    "2. Sample a few **negative pairs**: (oak_planks, diamond_ore), (oak_planks, lava) - random blocks that probably AREN'T neighbors\n",
    "3. Train the model to distinguish positive from negative\n",
    "\n",
    "```\n",
    "Positive: oak_planks + oak_log → Should output HIGH score (they're neighbors)\n",
    "Negative: oak_planks + diamond_ore → Should output LOW score (random, unlikely neighbors)\n",
    "Negative: oak_planks + lava → Should output LOW score\n",
    "```\n",
    "\n",
    "This is **much faster** - we only compute 6 scores instead of 3,717!\n",
    "\n",
    "## How Negative Sampling Works Mathematically\n",
    "\n",
    "For each (center, context) positive pair:\n",
    "\n",
    "1. **Positive score** = dot product of center embedding and context embedding\n",
    "2. We want this to be **high** (close to 1 after sigmoid)\n",
    "\n",
    "For each (center, random_negative) negative pair:\n",
    "\n",
    "1. **Negative score** = dot product of center embedding and negative embedding  \n",
    "2. We want this to be **low** (close to 0 after sigmoid)\n",
    "\n",
    "The **loss function** penalizes the model when:\n",
    "- Positive scores are low (should be high!)\n",
    "- Negative scores are high (should be low!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Subsampling Frequent Blocks\n",
    "\n",
    "## The Air Problem\n",
    "\n",
    "In Minecraft builds, **air** blocks are everywhere - often 70-80% of all blocks! If we train on every air block:\n",
    "\n",
    "1. Training is dominated by air-related pairs\n",
    "2. The model mostly learns about air, not interesting blocks\n",
    "3. Other blocks don't get enough training signal\n",
    "\n",
    "## The Solution: Subsampling\n",
    "\n",
    "We randomly **skip** frequent blocks during training. The probability of keeping a block is:\n",
    "\n",
    "```\n",
    "P(keep) = sqrt(threshold / frequency)\n",
    "```\n",
    "\n",
    "For example, if `threshold = 0.001` and air has `frequency = 0.75`:\n",
    "\n",
    "```\n",
    "P(keep air) = sqrt(0.001 / 0.75) = 0.036 = 3.6%\n",
    "```\n",
    "\n",
    "So we only keep 3.6% of air blocks! This balances the dataset.\n",
    "\n",
    "For rare blocks like `diamond_block` with `frequency = 0.0001`:\n",
    "\n",
    "```\n",
    "P(keep diamond_block) = sqrt(0.001 / 0.0001) = 3.16 → capped at 100%\n",
    "```\n",
    "\n",
    "Rare blocks are always kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: The Training Loop\n",
    "\n",
    "## What Happens During Training\n",
    "\n",
    "Each training step:\n",
    "\n",
    "1. **Forward Pass**: \n",
    "   - Get embeddings for center blocks\n",
    "   - Get embeddings for context blocks (positive)\n",
    "   - Get embeddings for negative samples\n",
    "   - Compute dot products (similarity scores)\n",
    "   - Compute loss (how wrong were we?)\n",
    "\n",
    "2. **Backward Pass** (Backpropagation):\n",
    "   - Calculate gradients (which direction should each embedding move?)\n",
    "   - This is done automatically by PyTorch!\n",
    "\n",
    "3. **Optimizer Step**:\n",
    "   - Update embeddings in the direction that reduces loss\n",
    "   - Learning rate controls how big each step is\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Imagine you're blindfolded on a hill, trying to find the lowest point:\n",
    "\n",
    "1. Feel the ground around you (compute gradient)\n",
    "2. Take a step downhill (update weights)\n",
    "3. Repeat until you reach the bottom (loss is minimized)\n",
    "\n",
    "The **gradient** tells us which direction is \"downhill\" for each embedding value.\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "How big are your steps?\n",
    "\n",
    "- **Too large**: You overshoot and bounce around, never converging\n",
    "- **Too small**: Training takes forever\n",
    "- **Just right**: Steady progress toward the minimum\n",
    "\n",
    "We use `learning_rate = 0.001` which is a common starting point.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "We don't update after every single example - that would be noisy and slow. Instead:\n",
    "\n",
    "1. Collect a **batch** of examples (e.g., 4096)\n",
    "2. Compute average loss across the batch\n",
    "3. Update once based on the average\n",
    "\n",
    "Larger batches = more stable gradients, but need more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Let's Start Coding!\n",
    "\n",
    "Now that you understand the concepts, let's implement it. First, let's set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Imports and Setup\n",
    "# ============================================================\n",
    "# These are the libraries we need:\n",
    "# - torch: PyTorch, the deep learning framework\n",
    "# - numpy: For numerical operations on arrays\n",
    "# - h5py: For reading HDF5 files (our training data)\n",
    "# - json: For reading the vocabulary file\n",
    "# - matplotlib: For visualization\n",
    "# - sklearn: For t-SNE dimensionality reduction\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Optional\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check if GPU is available - this is why we're using Kaggle!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 2: Configuration\n# ============================================================\n# These are our HYPERPARAMETERS - values we choose before training.\n# Finding good hyperparameters is crucial for ML success.\n\n# === Data Paths (adjust if needed for your Kaggle dataset) ===\nDATA_DIR = \"/kaggle/input/minecraft-schematics/minecraft_splits/splits/train\"  # Training data\nVOCAB_PATH = \"/kaggle/input/minecraft-schematics/tok2block.json\"  # Block vocabulary\nOUTPUT_DIR = \"/kaggle/working\"  # Where to save results\n\n# === Model Architecture ===\nEMBEDDING_DIM = 32  # Size of each block's embedding vector\n                     # 32 is a good balance between expressiveness and efficiency\n\n# === Training Hyperparameters ===\nEPOCHS = 50          # How many times to iterate through all data\n                     # More epochs = better embeddings, but longer training\n                     \nBATCH_SIZE = 8192    # How many (center, context) pairs per update\n                     # Larger = more stable gradients, but needs more memory\n                     \nLEARNING_RATE = 0.001  # How big of a step to take each update\n                        # 0.001 is a common starting point for Adam optimizer\n                        \nWEIGHT_DECAY = 0.0001  # L2 regularization - prevents overfitting\n                        # Adds a small penalty for large weights\n\n# === Skip-Gram Settings ===\nNUM_NEGATIVE_SAMPLES = 5  # How many negative samples per positive pair\n                           # More = better distinction, but slower\n                           \nCONTEXT_TYPE = \"neighbors_6\"  # Use 6 adjacent neighbors (up/down/left/right/front/back)\n                               # Alternative: \"neighbors_26\" for full 3x3x3 cube\n\n# === Subsampling ===\nSUBSAMPLE_THRESHOLD = 0.001  # Blocks more frequent than this get subsampled\n                              # Air is ~75% of blocks, so it will be heavily subsampled\n\n# === Other ===\nSEED = 42  # Random seed for reproducibility\n            # Using the same seed = same results every time\n\nprint(\"Configuration loaded!\")\nprint(f\"  Embedding dimension: {EMBEDDING_DIM}\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load Vocabulary\n",
    "# ============================================================\n",
    "# The vocabulary maps integer tokens (0, 1, 2, ...) to block names.\n",
    "# Our training data uses tokens, but we need names for interpretation.\n",
    "\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    tok2block = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "VOCAB_SIZE = len(tok2block)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE} unique block states\")\n",
    "\n",
    "# Let's look at some examples\n",
    "print(\"\\nSample blocks:\")\n",
    "for tok in [0, 102, 307, 500, 1000]:\n",
    "    if tok in tok2block:\n",
    "        print(f\"  Token {tok}: {tok2block[tok]}\")\n",
    "\n",
    "# Find the air token (we'll need this later)\n",
    "AIR_TOKEN = None\n",
    "for tok, name in tok2block.items():\n",
    "    if name == \"minecraft:air\":\n",
    "        AIR_TOKEN = tok\n",
    "        break\n",
    "print(f\"\\nAir token: {AIR_TOKEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: The Block2Vec Model\n",
    "\n",
    "Now let's define our neural network. In PyTorch, we create a class that inherits from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Block2Vec Model Definition\n",
    "# ============================================================\n",
    "\n",
    "class Block2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-gram model for learning Minecraft block embeddings.\n",
    "    \n",
    "    The model has TWO embedding matrices:\n",
    "    1. center_embeddings: Used when a block is the CENTER of a context window\n",
    "    2. context_embeddings: Used when a block is in the CONTEXT (neighbor)\n",
    "    \n",
    "    Why two matrices? This is a trick from Word2Vec that works better in practice.\n",
    "    At the end, we use center_embeddings as our final embeddings.\n",
    "    \n",
    "    Architecture:\n",
    "        Input: (center_token, context_token, [negative_tokens])\n",
    "        ↓\n",
    "        Look up embeddings from the matrices\n",
    "        ↓\n",
    "        Compute dot products (similarity scores)\n",
    "        ↓\n",
    "        Output: Loss value (how wrong were our predictions?)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 32):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Number of unique blocks (3717 in our case)\n",
    "            embedding_dim: Size of each embedding vector (32)\n",
    "        \"\"\"\n",
    "        # This calls the parent class constructor - required for PyTorch\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # nn.Embedding is a lookup table:\n",
    "        # - Input: integer token (e.g., 307 for stone)\n",
    "        # - Output: embedding vector of size embedding_dim\n",
    "        # The table starts with random values and learns during training\n",
    "        \n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        # This is important - bad initialization can prevent learning\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize embeddings with small uniform random values.\n",
    "        \n",
    "        We use range [-0.5/dim, 0.5/dim] so that initial dot products\n",
    "        are close to zero (neither strongly positive nor negative).\n",
    "        \"\"\"\n",
    "        init_range = 0.5 / self.embedding_dim\n",
    "        self.center_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        center_ids: torch.Tensor,\n",
    "        context_ids: torch.Tensor,\n",
    "        negative_ids: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Skip-gram loss with negative sampling.\n",
    "        \n",
    "        This is called automatically when you do: loss = model(centers, contexts, negatives)\n",
    "        \n",
    "        Args:\n",
    "            center_ids: Tensor of center block tokens [batch_size]\n",
    "            context_ids: Tensor of positive context tokens [batch_size]\n",
    "            negative_ids: Tensor of negative sample tokens [batch_size, num_negatives]\n",
    "            \n",
    "        Returns:\n",
    "            Loss value (scalar tensor)\n",
    "        \"\"\"\n",
    "        # Step 1: Look up embeddings\n",
    "        # center_ids might be [307, 102, 500, ...] (batch of center tokens)\n",
    "        # center_emb will be [batch_size, embedding_dim] - the embedding for each\n",
    "        center_emb = self.center_embeddings(center_ids)    # [B, D]\n",
    "        context_emb = self.context_embeddings(context_ids)  # [B, D]\n",
    "        neg_emb = self.context_embeddings(negative_ids)     # [B, N, D]\n",
    "        \n",
    "        # Step 2: Compute positive score (dot product)\n",
    "        # For each pair, compute: center · context (element-wise multiply, then sum)\n",
    "        # High score = model thinks they're neighbors\n",
    "        pos_score = torch.sum(center_emb * context_emb, dim=1)  # [B]\n",
    "        \n",
    "        # Step 3: Positive loss\n",
    "        # We want pos_score to be HIGH, so we use log-sigmoid\n",
    "        # log(sigmoid(x)) is close to 0 when x is large (good!)\n",
    "        # log(sigmoid(x)) is very negative when x is small (bad!)\n",
    "        pos_loss = F.logsigmoid(pos_score)  # [B]\n",
    "        \n",
    "        # Step 4: Compute negative scores\n",
    "        # We need to broadcast: each center against all its negatives\n",
    "        center_emb_expanded = center_emb.unsqueeze(1)  # [B, 1, D]\n",
    "        neg_score = torch.sum(center_emb_expanded * neg_emb, dim=2)  # [B, N]\n",
    "        \n",
    "        # Step 5: Negative loss\n",
    "        # We want neg_score to be LOW, so we use log-sigmoid of NEGATIVE score\n",
    "        # log(sigmoid(-x)) is close to 0 when x is small (good!)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)  # [B]\n",
    "        \n",
    "        # Step 6: Total loss\n",
    "        # We negate because we MAXIMIZE log-likelihood, but PyTorch MINIMIZES loss\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_embeddings(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the learned embeddings as a numpy array.\n",
    "        \n",
    "        Returns:\n",
    "            Array of shape [vocab_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.center_embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "# Let's test it!\n",
    "model = Block2Vec(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM)\n",
    "print(f\"Model created!\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  = {VOCAB_SIZE} blocks × {EMBEDDING_DIM} dims × 2 matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: The Dataset\n",
    "\n",
    "Now we need to create training data. We'll iterate through each build, extract (center, context) pairs, and sample negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Dataset Class\n",
    "# ============================================================\n",
    "\n",
    "class Block2VecDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Dataset that yields (center, context, negatives) tuples for training.\n",
    "    \n",
    "    This is an \"iterable\" dataset - it generates data on-the-fly rather than\n",
    "    loading everything into memory. This is necessary because we have millions\n",
    "    of training pairs!\n",
    "    \n",
    "    How it works:\n",
    "    1. Load each H5 file (one Minecraft build)\n",
    "    2. For each block position in the 3D array:\n",
    "       - Get the center block\n",
    "       - Get its neighbors (context)\n",
    "       - Sample random blocks (negatives)\n",
    "       - Yield the training example\n",
    "    \"\"\"\n",
    "    \n",
    "    # The 6 neighbors: +/- in each axis (x, y, z)\n",
    "    NEIGHBORS_6 = [\n",
    "        (-1, 0, 0), (1, 0, 0),  # left, right\n",
    "        (0, -1, 0), (0, 1, 0),  # down, up\n",
    "        (0, 0, -1), (0, 0, 1),  # back, front\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        vocab_size: int,\n",
    "        num_negative_samples: int = 5,\n",
    "        subsample_threshold: float = 0.001,\n",
    "        air_token: int = 102,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        self.subsample_threshold = subsample_threshold\n",
    "        self.air_token = air_token\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Find all H5 files\n",
    "        self.h5_files = sorted(self.data_dir.glob(\"*.h5\"))\n",
    "        print(f\"Found {len(self.h5_files)} training files\")\n",
    "        \n",
    "        # These will be computed on first iteration\n",
    "        self._block_freqs = None\n",
    "        self._negative_table = None\n",
    "        self._subsample_probs = None\n",
    "    \n",
    "    def _compute_frequencies(self):\n",
    "        \"\"\"\n",
    "        Count how often each block appears across all data.\n",
    "        \n",
    "        This is used for:\n",
    "        1. Subsampling - skip frequent blocks sometimes\n",
    "        2. Negative sampling - sample proportional to frequency^0.75\n",
    "        \"\"\"\n",
    "        print(\"Computing block frequencies (this only happens once)...\")\n",
    "        freqs = np.zeros(self.vocab_size, dtype=np.float64)\n",
    "        \n",
    "        for h5_path in tqdm(self.h5_files, desc=\"Counting blocks\"):\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                build = f[list(f.keys())[0]][:]\n",
    "                unique, counts = np.unique(build, return_counts=True)\n",
    "                for tok, count in zip(unique, counts):\n",
    "                    if tok < self.vocab_size:\n",
    "                        freqs[tok] += count\n",
    "        \n",
    "        # Normalize to probabilities\n",
    "        freqs /= freqs.sum()\n",
    "        self._block_freqs = freqs\n",
    "        \n",
    "        # Build negative sampling table\n",
    "        # We raise to power 0.75 to reduce dominance of frequent blocks\n",
    "        weighted = np.power(freqs, 0.75)\n",
    "        weighted /= weighted.sum()\n",
    "        \n",
    "        # Pre-sample 10 million negatives for efficiency\n",
    "        self._negative_table = np.random.choice(\n",
    "            self.vocab_size, \n",
    "            size=10_000_000, \n",
    "            p=weighted\n",
    "        )\n",
    "        \n",
    "        # Compute subsampling probabilities\n",
    "        self._subsample_probs = np.ones(self.vocab_size, dtype=np.float32)\n",
    "        for i, freq in enumerate(freqs):\n",
    "            if freq > self.subsample_threshold:\n",
    "                self._subsample_probs[i] = np.sqrt(self.subsample_threshold / freq)\n",
    "        \n",
    "        print(f\"  Air frequency: {freqs[self.air_token]:.2%}\")\n",
    "        print(f\"  Air keep probability: {self._subsample_probs[self.air_token]:.2%}\")\n",
    "    \n",
    "    def __iter__(self) -> Iterator:\n",
    "        \"\"\"\n",
    "        Iterate through all data, yielding training examples.\n",
    "        \n",
    "        Yields:\n",
    "            Tuple of (center_token, context_token, negative_tokens)\n",
    "        \"\"\"\n",
    "        # Compute frequencies on first iteration\n",
    "        if self._block_freqs is None:\n",
    "            self._compute_frequencies()\n",
    "        \n",
    "        rng = random.Random(self.seed)\n",
    "        neg_idx = 0\n",
    "        \n",
    "        for h5_path in self.h5_files:\n",
    "            # Load the build\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                build = f[list(f.keys())[0]][:]\n",
    "            \n",
    "            h, w, d = build.shape\n",
    "            \n",
    "            # Iterate through every position\n",
    "            for y in range(h):\n",
    "                for x in range(w):\n",
    "                    for z in range(d):\n",
    "                        center = int(build[y, x, z])\n",
    "                        \n",
    "                        # Subsampling check - skip frequent blocks sometimes\n",
    "                        if rng.random() >= self._subsample_probs[center]:\n",
    "                            continue\n",
    "                        \n",
    "                        # Check each neighbor\n",
    "                        for dy, dx, dz in self.NEIGHBORS_6:\n",
    "                            ny, nx, nz = y + dy, x + dx, z + dz\n",
    "                            \n",
    "                            # Skip if out of bounds\n",
    "                            if not (0 <= ny < h and 0 <= nx < w and 0 <= nz < d):\n",
    "                                continue\n",
    "                            \n",
    "                            context = int(build[ny, nx, nz])\n",
    "                            \n",
    "                            # Sample negatives from pre-computed table\n",
    "                            negatives = self._negative_table[\n",
    "                                neg_idx : neg_idx + self.num_negative_samples\n",
    "                            ]\n",
    "                            neg_idx = (neg_idx + self.num_negative_samples) % len(self._negative_table)\n",
    "                            \n",
    "                            yield center, context, negatives\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Convert a list of examples into tensors for the model.\n",
    "    \n",
    "    This is called automatically by the DataLoader.\n",
    "    \"\"\"\n",
    "    centers = torch.tensor([b[0] for b in batch], dtype=torch.long)\n",
    "    contexts = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    negatives = torch.tensor(np.array([b[2] for b in batch]), dtype=torch.long)\n",
    "    return centers, contexts, negatives\n",
    "\n",
    "\n",
    "print(\"Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: Training Loop\n",
    "\n",
    "Now let's put it all together and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Create Dataset and DataLoader\n",
    "# ============================================================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Create dataset\n",
    "dataset = Block2VecDataset(\n",
    "    data_dir=DATA_DIR,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_negative_samples=NUM_NEGATIVE_SAMPLES,\n",
    "    subsample_threshold=SUBSAMPLE_THRESHOLD,\n",
    "    air_token=AIR_TOKEN,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# DataLoader handles batching and (optionally) parallel loading\n",
    "# pin_memory=True speeds up CPU->GPU transfer\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=(device == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(f\"DataLoader ready with batch size {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Create Model and Optimizer\n",
    "# ============================================================\n",
    "\n",
    "# Create the model and move to GPU\n",
    "model = Block2Vec(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model on {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Create optimizer\n",
    "# AdamW is an improved version of Adam with better weight decay handling\n",
    "# Adam = Adaptive Moment Estimation - adjusts learning rate for each parameter\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Training Loop\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Track losses for plotting\n",
    "epoch_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Progress bar for this epoch\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for center_ids, context_ids, negative_ids in pbar:\n",
    "        # Move data to GPU\n",
    "        center_ids = center_ids.to(device)\n",
    "        context_ids = context_ids.to(device)\n",
    "        negative_ids = negative_ids.to(device)\n",
    "        \n",
    "        # Forward pass: compute loss\n",
    "        loss = model(center_ids, context_ids, negative_ids)\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()        # Compute new gradients\n",
    "        \n",
    "        # Optimizer step: update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track progress\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Epoch complete\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    epoch_losses.append(avg_loss)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}, time={elapsed:.0f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training complete in {time.time() - start_time:.0f}s\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 9: Save Results\n",
    "# ============================================================\n",
    "\n",
    "# Save the model\n",
    "model_path = f\"{OUTPUT_DIR}/block2vec_model.pt\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save embeddings as numpy file (easy to load later)\n",
    "embeddings = model.get_embeddings()\n",
    "embeddings_path = f\"{OUTPUT_DIR}/block_embeddings.npy\"\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f\"Embeddings saved to {embeddings_path}\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "\n",
    "# Save training stats\n",
    "stats = {\n",
    "    \"epoch_losses\": epoch_losses,\n",
    "    \"config\": {\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "    }\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/training_stats.json\", 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "print(\"Training stats saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 10: Visualizing the Results\n",
    "\n",
    "Now let's see what the model learned! We'll create:\n",
    "\n",
    "1. **Training loss plot** - Did the model learn?\n",
    "2. **t-SNE visualization** - 2D projection of embeddings\n",
    "3. **Similarity analysis** - Which blocks are most similar?\n",
    "4. **Nearest neighbors** - For key blocks, what's closest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 10: Plot Training Loss\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, 'b-', linewidth=2, marker='o')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Block2Vec Training Loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_loss.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {epoch_losses[0]:.4f}\")\n",
    "print(f\"Final loss: {epoch_losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {(epoch_losses[0] - epoch_losses[-1]) / epoch_losses[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 11: t-SNE Visualization\n",
    "# ============================================================\n",
    "# t-SNE (t-Distributed Stochastic Neighbor Embedding) reduces our\n",
    "# 32-dimensional embeddings to 2D for visualization.\n",
    "# Similar blocks should appear close together.\n",
    "\n",
    "import re\n",
    "\n",
    "def get_block_category(block_name: str) -> str:\n",
    "    \"\"\"Extract a category from block name for coloring.\"\"\"\n",
    "    name = block_name.replace(\"minecraft:\", \"\")\n",
    "    name = re.sub(r\"\\[.*\\]\", \"\", name)  # Remove block state\n",
    "    \n",
    "    if any(x in name for x in [\"planks\", \"log\", \"wood\", \"fence\", \"door\"]):\n",
    "        return \"wood\"\n",
    "    elif any(x in name for x in [\"stone\", \"cobble\", \"brick\", \"andesite\", \"diorite\", \"granite\"]):\n",
    "        return \"stone\"\n",
    "    elif \"ore\" in name or any(x in name for x in [\"diamond\", \"gold\", \"iron\", \"coal\", \"emerald\"]):\n",
    "        return \"ore/mineral\"\n",
    "    elif \"glass\" in name:\n",
    "        return \"glass\"\n",
    "    elif \"wool\" in name or \"carpet\" in name:\n",
    "        return \"wool\"\n",
    "    elif \"concrete\" in name:\n",
    "        return \"concrete\"\n",
    "    elif \"leaves\" in name:\n",
    "        return \"leaves\"\n",
    "    elif \"water\" in name:\n",
    "        return \"water\"\n",
    "    elif \"lava\" in name:\n",
    "        return \"lava\"\n",
    "    elif \"air\" in name:\n",
    "        return \"air\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# Sample blocks for visualization (too many would be cluttered)\n",
    "sample_size = 1000\n",
    "indices = np.random.choice(VOCAB_SIZE, min(sample_size, VOCAB_SIZE), replace=False)\n",
    "sampled_embeddings = embeddings[indices]\n",
    "\n",
    "# Get categories for coloring\n",
    "categories = [get_block_category(tok2block.get(i, \"unknown\")) for i in indices]\n",
    "unique_cats = list(set(categories))\n",
    "\n",
    "print(f\"Running t-SNE on {len(indices)} blocks...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "coords = tsne.fit_transform(sampled_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "cmap = plt.cm.get_cmap('tab10', len(unique_cats))\n",
    "\n",
    "for i, cat in enumerate(unique_cats):\n",
    "    mask = [c == cat for c in categories]\n",
    "    plt.scatter(\n",
    "        coords[mask, 0], coords[mask, 1],\n",
    "        c=[cmap(i)], label=cat, alpha=0.6, s=20\n",
    "    )\n",
    "\n",
    "plt.title('Block2Vec Embeddings (t-SNE projection)', fontsize=14)\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/tsne_embeddings.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nt-SNE complete! Similar blocks should be clustered together.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 12: Find Nearest Neighbors\n",
    "# ============================================================\n",
    "# For some key blocks, let's find their most similar blocks.\n",
    "# This tells us what relationships the model learned.\n",
    "\n",
    "def find_similar_blocks(block_name: str, embeddings: np.ndarray, tok2block: dict, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Find the most similar blocks to a given block.\n",
    "    \n",
    "    Uses cosine similarity: similarity = dot(a, b) / (|a| * |b|)\n",
    "    Ranges from -1 (opposite) to 1 (identical).\n",
    "    \"\"\"\n",
    "    # Find the token for this block\n",
    "    block2tok = {v: k for k, v in tok2block.items()}\n",
    "    \n",
    "    # Find matching blocks (handle block states)\n",
    "    matching = [k for k, v in tok2block.items() if block_name in v]\n",
    "    if not matching:\n",
    "        print(f\"Block '{block_name}' not found\")\n",
    "        return\n",
    "    \n",
    "    token = matching[0]  # Use first match\n",
    "    query = embeddings[token].reshape(1, -1)\n",
    "    \n",
    "    # Compute cosine similarity to all blocks\n",
    "    similarities = cosine_similarity(query, embeddings)[0]\n",
    "    \n",
    "    # Get top-k (excluding self)\n",
    "    top_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    print(f\"\\nMost similar to '{tok2block[token]}':\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    count = 0\n",
    "    for idx in top_indices:\n",
    "        if idx != token:\n",
    "            # Get base block name (without state)\n",
    "            name = tok2block[idx]\n",
    "            short_name = re.sub(r\"\\[.*\\]\", \"\", name.replace(\"minecraft:\", \"\"))\n",
    "            print(f\"  {count+1}. {short_name:30} (similarity: {similarities[idx]:.4f})\")\n",
    "            count += 1\n",
    "            if count >= top_k:\n",
    "                break\n",
    "\n",
    "# Test with some interesting blocks\n",
    "test_blocks = [\n",
    "    \"oak_planks\",\n",
    "    \"stone\",\n",
    "    \"glass\",\n",
    "    \"torch\",\n",
    "    \"water\",\n",
    "    \"diamond_block\",\n",
    "]\n",
    "\n",
    "for block in test_blocks:\n",
    "    find_similar_blocks(block, embeddings, tok2block, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 13: Similarity Heatmap\n",
    "# ============================================================\n",
    "\n",
    "# Select some common blocks for the heatmap\n",
    "blocks_to_compare = [\n",
    "    \"stone\", \"cobblestone\", \"stone_bricks\",\n",
    "    \"oak_planks\", \"spruce_planks\", \"oak_log\",\n",
    "    \"glass\", \"white_wool\", \"red_wool\",\n",
    "    \"iron_block\", \"gold_block\", \"diamond_block\",\n",
    "    \"dirt\", \"grass_block\", \"sand\",\n",
    "    \"water\", \"air\",\n",
    "]\n",
    "\n",
    "# Find tokens for these blocks\n",
    "selected_tokens = []\n",
    "selected_names = []\n",
    "\n",
    "for block in blocks_to_compare:\n",
    "    for tok, name in tok2block.items():\n",
    "        if block in name and \"[\" not in name:  # Exact match without states\n",
    "            selected_tokens.append(tok)\n",
    "            selected_names.append(block)\n",
    "            break\n",
    "\n",
    "if len(selected_tokens) > 1:\n",
    "    # Get embeddings and compute similarity matrix\n",
    "    selected_embeddings = embeddings[selected_tokens]\n",
    "    sim_matrix = cosine_similarity(selected_embeddings)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    im = plt.imshow(sim_matrix, cmap='RdYlBu_r', vmin=-1, vmax=1)\n",
    "    \n",
    "    plt.xticks(range(len(selected_names)), selected_names, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(selected_names)), selected_names)\n",
    "    plt.colorbar(im, label='Cosine Similarity')\n",
    "    plt.title('Block Embedding Similarity', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/similarity_heatmap.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nHeatmap interpretation:\")\n",
    "    print(\"  - Red/orange = similar blocks (often appear together)\")\n",
    "    print(\"  - Blue = dissimilar blocks (rarely appear together)\")\n",
    "else:\n",
    "    print(\"Not enough blocks found for heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 11: Summary and Next Steps\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "1. **Embeddings** represent blocks as vectors where similar blocks are close together\n",
    "2. **Skip-gram** learns embeddings by predicting neighbors from center blocks\n",
    "3. **Negative sampling** makes training efficient by sampling random \"wrong\" answers\n",
    "4. **Subsampling** prevents frequent blocks (like air) from dominating training\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "These embeddings will be used as input to a **VQ-VAE** (Vector Quantized Variational Autoencoder) in Phase 3:\n",
    "\n",
    "1. The VQ-VAE encoder compresses 32×32×32 builds into a small latent representation\n",
    "2. The VQ-VAE decoder reconstructs builds from the latent representation\n",
    "3. The block embeddings help the model understand that similar blocks are interchangeable\n",
    "\n",
    "## Download Your Results\n",
    "\n",
    "Make sure to download:\n",
    "- `block_embeddings.npy` - The learned embeddings\n",
    "- `block2vec_model.pt` - The full model\n",
    "- The visualization images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 14: Final Summary\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: Block2Vec\")\n",
    "print(f\"  Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"  Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Initial loss: {epoch_losses[0]:.4f}\")\n",
    "print(f\"  Final loss: {epoch_losses[-1]:.4f}\")\n",
    "print(f\"\\nOutput files in {OUTPUT_DIR}:\")\n",
    "print(f\"  - block_embeddings.npy\")\n",
    "print(f\"  - block2vec_model.pt\")\n",
    "print(f\"  - training_stats.json\")\n",
    "print(f\"  - training_loss.png\")\n",
    "print(f\"  - tsne_embeddings.png\")\n",
    "print(f\"  - similarity_heatmap.png\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}