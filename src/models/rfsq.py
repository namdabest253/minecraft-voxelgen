"""
Robust Residual FSQ (RFSQ) Module - With Fixed Statistics for Generation.

RFSQ improves upon standard FSQ by applying multi-stage residual quantization
with LayerNorm conditioning. This prevents residual magnitude decay across
stages and captures finer details that single-stage FSQ misses.

Key innovations:
- Multi-stage residual quantization: Stage 1 captures coarse structure, Stage 2+ refines details
- LayerNorm conditioning: Prevents residual magnitude decay across stages
- Finer granularity: More effective code usage without increasing codebook size
- Fixed statistics mode: Enables proper generation from prior-generated indices

Reference: https://github.com/zhuxiaoxuhit/robust_rfsq
Paper: "Improving Finite Scalar Quantization via Progressive Training"

Example:
    levels = [5, 5, 5, 5]  # 4 dimensions, 5 levels each
    num_stages = 2
    rfsq = RFSQ(levels, num_stages)
    # Total implicit codes: 625 x 625 = 390,625
"""

from typing import List, Tuple, Dict, Optional
import math

import numpy as np
import torch
import torch.nn as nn
from torch import Tensor

from .fsq import FSQ


class InvertibleLayerNorm(nn.Module):
    """
    LayerNorm that stores statistics for exact inverse transformation.

    This is critical for RFSQ: we normalize before quantization, then
    inverse-transform after to preserve the original scale of residuals.
    Without this, residual magnitudes decay exponentially across stages.

    For generation mode, fixed statistics (computed from training data) can be
    used instead of per-sample statistics. This enables proper decoding of
    indices generated by the prior model.

    Args:
        num_features: Number of channels/features (C dimension).
        eps: Small epsilon for numerical stability.

    Note:
        Unlike standard LayerNorm which normalizes over the last dimension,
        this normalizes over spatial dimensions (X, Y, Z) for 3D data.
    """

    def __init__(self, num_features: int, eps: float = 1e-5):
        super().__init__()
        self.num_features = num_features
        self.eps = eps

        # Learnable affine parameters (per-channel)
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))

        # Stored during forward for inverse (not persistent - don't save to checkpoint)
        self.register_buffer('stored_mean', None, persistent=False)
        self.register_buffer('stored_std', None, persistent=False)

        # Fixed statistics for generation mode (persistent - save to checkpoint)
        # Shape: [1, 1, 1, 1, num_features] for channels_last format
        self.register_buffer('fixed_mean', torch.zeros(1, 1, 1, 1, num_features))
        self.register_buffer('fixed_std', torch.ones(1, 1, 1, 1, num_features))
        self.use_fixed_stats = False

    def set_fixed_stats(self, mean: Tensor, std: Tensor) -> None:
        """Set fixed statistics for generation mode.

        Args:
            mean: Average mean from training data [1, 1, 1, 1, C]
            std: Average std from training data [1, 1, 1, 1, C]
        """
        self.fixed_mean.copy_(mean)
        self.fixed_std.copy_(std)

    def enable_fixed_stats(self, enable: bool = True) -> None:
        """Enable or disable fixed statistics mode for generation."""
        self.use_fixed_stats = enable

    def forward(self, x: Tensor) -> Tensor:
        """
        Normalize input and store statistics for inverse.

        Args:
            x: Input tensor of shape [B, C, X, Y, Z] or [B, X, Y, Z, C].
               Assumes C = num_features.

        Returns:
            Normalized tensor of same shape.
        """
        # Determine if channels are last or first
        channels_last = x.shape[-1] == self.num_features

        if channels_last:
            # [B, X, Y, Z, C] -> normalize over X, Y, Z (dims 1, 2, 3)
            self.stored_mean = x.mean(dim=(1, 2, 3), keepdim=True)
            self.stored_std = x.std(dim=(1, 2, 3), keepdim=True) + self.eps
            x_norm = (x - self.stored_mean) / self.stored_std
            # Apply affine: weight and bias are [C], broadcast to [..., C]
            return x_norm * self.weight + self.bias
        else:
            # [B, C, X, Y, Z] -> normalize over X, Y, Z (dims 2, 3, 4)
            self.stored_mean = x.mean(dim=(2, 3, 4), keepdim=True)
            self.stored_std = x.std(dim=(2, 3, 4), keepdim=True) + self.eps
            x_norm = (x - self.stored_mean) / self.stored_std
            # Apply affine: weight and bias are [C], need to reshape to [1, C, 1, 1, 1]
            return x_norm * self.weight.view(1, -1, 1, 1, 1) + self.bias.view(1, -1, 1, 1, 1)

    def inverse(self, x_norm: Tensor) -> Tensor:
        """
        Inverse transform: undo normalization using stored statistics.

        Args:
            x_norm: Normalized tensor from forward pass.

        Returns:
            Tensor in original scale.

        Raises:
            RuntimeError: If called before forward() (no stored statistics)
                         and not in fixed stats mode.
        """
        # Determine if channels are last or first
        channels_last = x_norm.shape[-1] == self.num_features

        if channels_last:
            # Undo affine
            x = (x_norm - self.bias) / self.weight

            # Undo normalization - use fixed stats if enabled
            if self.use_fixed_stats:
                return x * self.fixed_std + self.fixed_mean
            else:
                if self.stored_mean is None or self.stored_std is None:
                    raise RuntimeError("Must call forward() before inverse() or enable fixed stats")
                return x * self.stored_std + self.stored_mean
        else:
            # Undo affine
            x = (x_norm - self.bias.view(1, -1, 1, 1, 1)) / self.weight.view(1, -1, 1, 1, 1)

            # Undo normalization
            if self.use_fixed_stats:
                # Reshape fixed stats from [1,1,1,1,C] to [1,C,1,1,1]
                fixed_mean_cf = self.fixed_mean.permute(0, 4, 1, 2, 3)
                fixed_std_cf = self.fixed_std.permute(0, 4, 1, 2, 3)
                return x * fixed_std_cf + fixed_mean_cf
            else:
                if self.stored_mean is None or self.stored_std is None:
                    raise RuntimeError("Must call forward() before inverse() or enable fixed stats")
                return x * self.stored_std + self.stored_mean

    def inverse_with_fixed_stats(self, x_norm: Tensor) -> Tensor:
        """
        Inverse transform using fixed statistics (always, regardless of mode).

        Use this method when decoding indices from prior generation.

        Args:
            x_norm: Normalized tensor to inverse transform.

        Returns:
            Tensor in original scale using fixed statistics.
        """
        channels_last = x_norm.shape[-1] == self.num_features

        if channels_last:
            x = (x_norm - self.bias) / self.weight
            return x * self.fixed_std + self.fixed_mean
        else:
            x = (x_norm - self.bias.view(1, -1, 1, 1, 1)) / self.weight.view(1, -1, 1, 1, 1)
            fixed_mean_cf = self.fixed_mean.permute(0, 4, 1, 2, 3)
            fixed_std_cf = self.fixed_std.permute(0, 4, 1, 2, 3)
            return x * fixed_std_cf + fixed_mean_cf


class RFSQStage(nn.Module):
    """
    Single stage of Residual FSQ with LayerNorm conditioning.

    Each stage:
    1. Normalizes the residual (stores stats for inverse)
    2. Quantizes in normalized space using FSQ
    3. Inverse-transforms back to original scale
    4. Computes new residual for next stage

    Args:
        levels: List of quantization levels per dimension.
    """

    def __init__(self, levels: List[int]):
        super().__init__()
        self.levels = levels
        self.fsq = FSQ(levels)
        self.layernorm = InvertibleLayerNorm(len(levels))

    @property
    def codebook_size(self) -> int:
        """Number of implicit codes in this stage."""
        return self.fsq.codebook_size

    def forward(self, residual: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Quantize residual with LayerNorm conditioning.

        Args:
            residual: Input residual tensor [B, X, Y, Z, C] where C = len(levels).

        Returns:
            z_q: Quantized representation (in original scale).
            new_residual: residual - z_q (for next stage).
            indices: Integer indices from FSQ.
        """
        # 1. Normalize residual (prevents magnitude decay)
        z_norm = self.layernorm(residual)

        # 2. Quantize in normalized space
        z_q_norm, indices = self.fsq(z_norm)

        # 3. Inverse transform back to original scale
        z_q = self.layernorm.inverse(z_q_norm)

        # 4. Compute new residual for next stage
        new_residual = residual - z_q

        return z_q, new_residual, indices

    def indices_to_quantized_fixed(self, indices: Tensor) -> Tensor:
        """
        Convert indices to quantized values using fixed LayerNorm statistics.

        Use this for decoding indices generated by the prior model.

        Args:
            indices: Integer indices from prior generation.

        Returns:
            z_q: Quantized values in original scale (using fixed stats).
        """
        # Get normalized codes from FSQ
        z_q_norm = self.fsq.indices_to_codes(indices)

        # Apply inverse LayerNorm with fixed statistics
        z_q = self.layernorm.inverse_with_fixed_stats(z_q_norm)

        return z_q


class RFSQ(nn.Module):
    """
    Robust Residual FSQ with multiple stages.

    Applies residual quantization across multiple stages. Each stage quantizes
    the residual left by previous stages, progressively capturing finer details.

    Key insight: LayerNorm conditioning normalizes residuals before quantization,
    preventing the exponential decay of residual magnitudes that would otherwise
    cause later stages to become ineffective.

    Args:
        levels_per_stage: List of quantization levels per dimension.
                         E.g., [5, 5, 5, 5] gives 625 codes per stage.
        num_stages: Number of residual stages (default: 2).

    Example:
        >>> rfsq = RFSQ([5, 5, 5, 5], num_stages=2)
        >>> z = torch.randn(2, 8, 8, 8, 4)  # [B, X, Y, Z, C]
        >>> z_q, all_indices = rfsq(z)
        >>> len(all_indices)  # 2 stages
        2
    """

    def __init__(self, levels_per_stage: List[int], num_stages: int = 2):
        super().__init__()

        self.levels_per_stage = levels_per_stage
        self.num_stages = num_stages
        self.dim = len(levels_per_stage)

        # Create stages
        self.stages = nn.ModuleList([
            RFSQStage(levels_per_stage) for _ in range(num_stages)
        ])

        # Total implicit codebook size (product of all stages)
        codes_per_stage = int(np.prod(levels_per_stage))
        self.codebook_size = codes_per_stage ** num_stages
        self.codes_per_stage = codes_per_stage

    @property
    def num_codes(self) -> int:
        """Return total implicit codebook size."""
        return self.codebook_size

    def enable_fixed_stats(self, enable: bool = True) -> None:
        """Enable or disable fixed statistics mode for all stages."""
        for stage in self.stages:
            stage.layernorm.enable_fixed_stats(enable)

    def forward(self, z: Tensor) -> Tuple[Tensor, List[Tensor]]:
        """
        Multi-stage residual quantization.

        Args:
            z: Encoder output [B, X, Y, Z, C] where C = len(levels_per_stage).

        Returns:
            z_q: Quantized sum of all stages (same shape as input).
            all_indices: List of indices from each stage.
        """
        residual = z
        z_q_sum = torch.zeros_like(z)
        all_indices = []

        for stage in self.stages:
            z_q, residual, indices = stage(residual)
            z_q_sum = z_q_sum + z_q
            all_indices.append(indices)

        return z_q_sum, all_indices

    def forward_with_norms(self, z: Tensor) -> Tuple[Tensor, List[Tensor], List[float]]:
        """
        Forward pass that also returns residual norms for monitoring.

        Useful for verifying that LayerNorm prevents residual decay.

        Args:
            z: Encoder output [B, X, Y, Z, C].

        Returns:
            z_q: Quantized sum of all stages.
            all_indices: List of indices from each stage.
            residual_norms: List of residual norms before each stage.
        """
        residual = z
        z_q_sum = torch.zeros_like(z)
        all_indices = []
        residual_norms = []

        for stage in self.stages:
            # Record residual norm before this stage
            residual_norms.append(residual.norm().item())

            z_q, residual, indices = stage(residual)
            z_q_sum = z_q_sum + z_q
            all_indices.append(indices)

        # Also record final residual norm
        residual_norms.append(residual.norm().item())

        return z_q_sum, all_indices, residual_norms

    def get_codebook_usage(self, all_indices: List[Tensor]) -> Dict[str, float]:
        """
        Compute codebook usage statistics per stage.

        Args:
            all_indices: List of indices from forward pass.

        Returns:
            Dictionary with per-stage usage and perplexity.
        """
        metrics = {}

        for i, indices in enumerate(all_indices):
            usage, perplexity = self.stages[i].fsq.get_codebook_usage(indices)
            metrics[f'stage{i}_usage'] = usage
            metrics[f'stage{i}_perplexity'] = perplexity

        # Overall metrics
        metrics['avg_usage'] = sum(metrics[f'stage{i}_usage'] for i in range(self.num_stages)) / self.num_stages
        metrics['avg_perplexity'] = sum(metrics[f'stage{i}_perplexity'] for i in range(self.num_stages)) / self.num_stages

        return metrics

    def indices_to_codes(self, all_indices: List[Tensor]) -> Tensor:
        """
        Convert indices from all stages back to quantized vectors (NAIVE).

        WARNING: This method is approximate because it doesn't apply the
        LayerNorm inverse transform. Use indices_to_codes_fixed() for
        proper generation.

        Args:
            all_indices: List of indices from each stage.

        Returns:
            z_q: Sum of quantized vectors from all stages (in normalized space).
        """
        z_q_sum = None

        for i, indices in enumerate(all_indices):
            z_q_stage = self.stages[i].fsq.indices_to_codes(indices)
            if z_q_sum is None:
                z_q_sum = z_q_stage
            else:
                z_q_sum = z_q_sum + z_q_stage

        return z_q_sum

    def indices_to_codes_fixed(self, all_indices: List[Tensor]) -> Tensor:
        """
        Convert indices to quantized vectors using fixed LayerNorm statistics.

        Use this method when decoding indices generated by the prior model.
        Fixed statistics must be set beforehand using set_fixed_stats().

        Args:
            all_indices: List of indices from each stage [B, X, Y, Z].

        Returns:
            z_q: Sum of quantized vectors in original scale.
        """
        z_q_sum = None

        for i, indices in enumerate(all_indices):
            z_q_stage = self.stages[i].indices_to_quantized_fixed(indices)
            if z_q_sum is None:
                z_q_sum = z_q_stage
            else:
                z_q_sum = z_q_sum + z_q_stage

        return z_q_sum

    def collect_stats_from_batch(self, z: Tensor) -> Dict[str, Dict[str, Tensor]]:
        """
        Collect LayerNorm statistics from a batch of encoder outputs.

        Use this during a pass over the training data to collect statistics.

        Args:
            z: Encoder output [B, X, Y, Z, C].

        Returns:
            Dictionary with mean and std for each stage.
        """
        stats = {}
        residual = z

        for i, stage in enumerate(self.stages):
            # Run layernorm forward to store statistics
            z_norm = stage.layernorm(residual)
            z_q_norm, indices = stage.fsq(z_norm)
            z_q = stage.layernorm.inverse(z_q_norm)
            residual = residual - z_q

            # Collect the stored statistics
            stats[f'stage{i}'] = {
                'mean': stage.layernorm.stored_mean.clone(),
                'std': stage.layernorm.stored_std.clone(),
            }

        return stats

    def set_fixed_stats(self, stats: Dict[str, Dict[str, Tensor]]) -> None:
        """
        Set fixed statistics for all stages.

        Args:
            stats: Dictionary with 'stage0', 'stage1', etc. keys,
                   each containing 'mean' and 'std' tensors.
        """
        for i, stage in enumerate(self.stages):
            stage_stats = stats[f'stage{i}']
            stage.layernorm.set_fixed_stats(
                stage_stats['mean'],
                stage_stats['std']
            )


# Recommended RFSQ configurations
RFSQ_CONFIGS = {
    # v6 default: 2 stages, 625 codes each, ~390K total
    'v6_default': {
        'levels': [5, 5, 5, 5],
        'num_stages': 2,
    },

    # High capacity: 2 stages, 2401 codes each, ~5.8M total
    'high_capacity': {
        'levels': [7, 7, 7, 7],
        'num_stages': 2,
    },

    # 3-stage: more residual refinement
    'three_stage': {
        'levels': [5, 5, 5, 5],
        'num_stages': 3,
    },

    # Tiny: for testing
    'tiny': {
        'levels': [3, 3, 3, 3],
        'num_stages': 2,
    },
}


def get_rfsq_config(name: str) -> Dict:
    """Get a predefined RFSQ configuration by name."""
    if name not in RFSQ_CONFIGS:
        raise ValueError(f"Unknown RFSQ config: {name}. Available: {list(RFSQ_CONFIGS.keys())}")
    return RFSQ_CONFIGS[name]
